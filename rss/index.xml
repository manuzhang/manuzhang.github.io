<?xml version="1.0"?><rss version="2.0"><channel><title>ManuZhang's Blog</title><link>https://manuzhang.github.io</link><description>Static Blog generated in Scala</description><item><title>Note about Spark Python UDF </title><link>https://manuzhang.github.io/posts/2019-04-18-spark-python-udf/index.html</link><description>&lt;p&gt;The other day, my colleague was developing a PySpark(2.3.1) application which reads Chinese sentences from a Hive table, tokenizes them with a Python UDF and saves the first words into another table. The codes are roughly like this.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def tokenize(sentence):
    return tokenizer.tokenize(sentence)[0]

df = spark.sql(&amp;quot;select sentence from db.article&amp;quot;)
df = df.repartition(1000)

tokenize_udf = udf(tokenize, StringType())
df = df.withColumn('word', tokenize_udf('sentence'))
df.filter(df.word != '')

df.write.format('orc').mode('overwrite').saveAsTable('db.words')        
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Python UDF was time-consuming so my colleague tried increasing parallelism with &lt;code class='language-text'&gt;repartition&lt;/code&gt; from 99 to 1000. It didn't work as expected and the job still stuck in those 99 tasks.&lt;/p&gt;
&lt;p&gt;I looked into the physical plan and it turned out that the Python UDF (&lt;code class='language-text'&gt;BatchEvalPython&lt;/code&gt;) had been executed twice, once in a push down filter before repartition and once after repartition.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;== Physical Plan ==
Execute CreateDataSourceTableAsSelectCommand CreateDataSourceTableAsSelectCommand `db`.`words`, Overwrite, [word#19]
+- *(3) Project [sentence#13, pythonUDF0#26 AS word#19]
   +- BatchEvalPython [tokenize(sentence#13)], [sentence#13, pythonUDF0#26]
      +- Exchange RoundRobinPartitioning(1000)
         +- *(2) Project [sentence#13]
            +- *(2) Filter NOT (pythonUDF0#25 = )
               +- BatchEvalPython [tokenize(sentence#13)], [sentence#13, pythonUDF0#25]
                  +- *(1) Filter (isnotnull(sentence#13) &amp;amp;&amp;amp; NOT (sentence#13 = ))
                     +- HiveTableScan [sentence#13], HiveTableRelation `db`.`sentence`, org.apache.hadoop.hive.ql.io.orc.OrcSerde, [sentence#13]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I found &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-22541&quot;&gt;another issue when Python UDF is used in filter&lt;/a&gt; which led me to &lt;a href=&quot;https://github.com/apache/spark/blob/v2.3.1/python/pyspark/sql/functions.py#L2113&quot;&gt;an important note about Python UDF&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def udf(f=None, returnType=StringType()):
    &amp;quot;&amp;quot;&amp;quot;Creates a user defined function (UDF).
    .. note:: The user-defined functions are considered deterministic by default. Due to
        optimization, duplicate invocations may be eliminated or the function may even be invoked
        more times than it is present in the query. If your function is not deterministic, call
        `asNondeterministic` on the user defined function. E.g.:
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hence, it's possible &lt;strong&gt;Python UDF is invoked more times than it is present in the query&lt;/strong&gt;. There are more notes you should check out when your Spark application with Python UDF behaves strangely next time.&lt;/p&gt;
&lt;h3&gt;Solution&lt;/h3&gt;
&lt;p&gt;To increase parallelism of executing Python UDF, we can decrease the input split size as follows.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;spark.sparkContext._jsc.hadoopConfiguration().set(&amp;quot;mapred.max.split.size&amp;quot;, &amp;quot;33554432&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thanks to this StackOverflow answer on &lt;a href=&quot;https://stackoverflow.com/questions/28844631/how-to-set-hadoop-configuration-values-from-pyspark&quot;&gt;How to set Hadoop configuration values from PySpark&lt;/a&gt;.&lt;/p&gt;
</description><pubDate>Thu, 18 Apr 2019 00:00:00 GMT</pubDate></item><item><title>Fixing one Spark bug lead to another</title><link>https://manuzhang.github.io/posts/2019-03-24-create-spark-bug/index.html</link><description>&lt;p&gt;As we mentioned in the &lt;a href=&quot;https://manuzhang.github.io/2019/02/23/spark-app-npe.html&quot;&gt;last post&lt;/a&gt;, Spark had a bug checking whether an application has extended &lt;code class='language-text'&gt;scala.App&lt;/code&gt; (&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26977&quot;&gt;SPARK-26977&lt;/a&gt;). I went on to submit a &lt;a href=&quot;https://github.com/apache/spark/pull/23903&quot;&gt;pull request&lt;/a&gt; checking the existence of &lt;code class='language-text'&gt;childMainClass$&lt;/code&gt; on &lt;code class='language-text'&gt;spark-submit --class childMainClass&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;      Try {
        if (classOf[scala.App].isAssignableFrom(Utils.classForName(s&amp;quot;$childMainClass$$&amp;quot;))) {
          logWarning(&amp;quot;Subclasses of scala.App may not work correctly. &amp;quot; +
            &amp;quot;Use a main() method instead.&amp;quot;)
        }
      }
      // invoke main method of childMainClass
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This was a minor issue but since it's my first PR to Spark I was quite happy about it. I didn't realize then when fixing one minor Spark bug I created a major one.&lt;/p&gt;
&lt;p&gt;That's &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-27205&quot;&gt;SPARK-27205&lt;/a&gt;, where launching &lt;code class='language-text'&gt;spark-shell&lt;/code&gt; with &lt;code class='language-text'&gt;--packages&lt;/code&gt; option failed to load transitive dependencies on master.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;./bin/spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Weirdly, it would work after &lt;a href=&quot;https://github.com/apache/spark/pull/24147&quot;&gt;removing my changes&lt;/a&gt;. The two looked irrelevant so I dug into how &lt;code class='language-text'&gt;spark-shell&lt;/code&gt; load dependencies behind the scene.&lt;/p&gt;
&lt;p&gt;&lt;code class='language-text'&gt;spark-shell&lt;/code&gt; is actually a shorthand for &lt;code class='language-text'&gt;spark-submit --class org.apache.spark.repl.Main&lt;/code&gt; and &lt;a href=&quot;https://github.com/apache/spark/blob/master/repl/src/main/scala/org/apache/spark/repl/Main.scala&quot;&gt;repl.Main&lt;/a&gt; loads user specified jars or packages from &lt;code class='language-text'&gt;spark.repl.local.jars&lt;/code&gt; option of &lt;code class='language-text'&gt;SparkConf&lt;/code&gt;. The option is set with &lt;code class='language-text'&gt;--jars&lt;/code&gt; or &lt;code class='language-text'&gt;--packages&lt;/code&gt; from &lt;code class='language-text'&gt;SparkSubmit&lt;/code&gt;. I verified the option existed with user packages at &lt;code class='language-text'&gt;SparkSubmit&lt;/code&gt;'s side so the issue was that somehow &lt;code class='language-text'&gt;SparkConf&lt;/code&gt; didn't get to &lt;code class='language-text'&gt;repl.Main&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;It turns out that all options of &lt;code class='language-text'&gt;SparkConf&lt;/code&gt; are &lt;a href=&quot;https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/SparkApplication.scala#L47&quot;&gt;set to system properties right before invoking the main method&lt;/a&gt; of &lt;code class='language-text'&gt;repl.Main&lt;/code&gt; and loaded back into &lt;code class='language-text'&gt;SparkConf&lt;/code&gt; at initialization of &lt;code class='language-text'&gt;repl.Main&lt;/code&gt;. The latter has to &lt;strong&gt;happen after&lt;/strong&gt; the former while &amp;quot;my fix&amp;quot; broke it !&lt;/p&gt;
&lt;p&gt;Okay, I have been bitten twice by fields initialization of Scala Object.&lt;/p&gt;
</description><pubDate>Sun, 24 Mar 2019 00:00:00 GMT</pubDate></item><item><title>NPE from Spark App that extends scala.App</title><link>https://manuzhang.github.io/posts/2019-02-23-spark-app-npe/index.html</link><description>&lt;p&gt;This is the log of a bizarre &lt;code class='language-text'&gt;NullPointerException(NPE)&lt;/code&gt; from a Spark application that extends &lt;code class='language-text'&gt;scala.App&lt;/code&gt; and a minor bug of Spark hidden for years.&lt;/p&gt;
&lt;p&gt;Last week, I was developing the following Spark (2.3.1) application that pushes data from &lt;code class='language-text'&gt;Hive&lt;/code&gt; to a database.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;object Test extends App {
  val foo = args(0)
  val bar = args(1)
  
  val spark = SparkSession.builder
    .getOrCreate()
    
  spark.sql(&amp;quot;select * from test&amp;quot;)
    .foreachPartition { rowIter: Internal[Row] =&amp;gt;
       val client = new DbClient
       client.setFoo(foo) // throws NullPointerException
       client.setBar(bar)
       client.init
      
       client.put(rowIter.map { row =&amp;gt;
         row.getLong(0) -&amp;gt; row.getLong(1)
       }.toMap)
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The local variables &lt;code class='language-text'&gt;foo&lt;/code&gt; and &lt;code class='language-text'&gt;bar&lt;/code&gt; are used to configure the &lt;code class='language-text'&gt;DbClient&lt;/code&gt; on remote executors. I was expecting them to be shipped through &lt;a href=&quot;https://spark.apache.org/docs/latest/rdd-programming-guide.html#understanding-closures-&quot;&gt;closures&lt;/a&gt; but somehow they weren't.&lt;/p&gt;
&lt;p&gt;My colleague Vincent pointed me to &lt;code class='language-text'&gt;ClosureCleaner&lt;/code&gt; which &lt;code class='language-text'&gt;logDebug&lt;/code&gt; &lt;a href=&quot;https://github.com/apache/spark/blob/v2.3.1/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala#L221&quot;&gt;all fields, methods and classes&lt;/a&gt; in closures. I enabled driver's debug log and the absence of &lt;code class='language-text'&gt;foo&lt;/code&gt; and &lt;code class='language-text'&gt;bar&lt;/code&gt; assured me that they were not captured by closure.&lt;/p&gt;
&lt;p&gt;Then I looked into what happens in &lt;code class='language-text'&gt;foreachPartition&lt;/code&gt;. The byte code reminded me that &lt;code class='language-text'&gt;foo&lt;/code&gt; and &lt;code class='language-text'&gt;bar&lt;/code&gt; were static members of &lt;code class='language-text'&gt;Test$&lt;/code&gt; rather than local variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;  9: getstatic      #26                 // Field Test$.MODULE$:LTest$;
  12: invokevirtual #30                 // Method Test$.foo:()Ljava/lang/String;
  15: invokevirtual #34                 // Method DbClient.setFoo:(Ljava/lang/String;)V
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;It suddenly struck me that the initialization of the class (&lt;code class='language-text'&gt;Test$&lt;/code&gt;) that extends &lt;code class='language-text'&gt;scala.App&lt;/code&gt; is actually delayed to its &lt;code class='language-text'&gt;main()&lt;/code&gt; method&lt;/strong&gt;. That's why &lt;code class='language-text'&gt;foo&lt;/code&gt; and &lt;code class='language-text'&gt;bar&lt;/code&gt; were uninitialized as confirmed by &lt;a href=&quot;https://scala-lang.org/files/archive/api/2.11.12/#scala.App&quot;&gt;scala.App's doc&lt;/a&gt;,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It should be noted that this trait is implemented using the &lt;a href=&quot;https://scala-lang.org/files/archive/api/2.11.12/scala/DelayedInit.html&quot;&gt;DelayedInit&lt;/a&gt; functionality, which means that fields of the object will not have been initialized before the main method has been executed.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Meanwhile, I googled &amp;quot;Spark closure problems&amp;quot; which led me to an ancient Spark jira &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-4170&quot;&gt;Closure problems when running Scala app that &amp;quot;extends App&amp;quot;&lt;/a&gt;. There was a fix that would print a warning when an application extends &lt;code class='language-text'&gt;App&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;if (classOf[scala.App].isAssignableFrom(mainClass)) {
  printWarning(&amp;quot;Subclasses of scala.App may not work correctly. Use a main() method instead.&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why have I never been warned ? After some digging, I realized the above &amp;quot;if clause&amp;quot; would never be true. This is because Scala compiler generate two Java classes &lt;code class='language-text'&gt;Test&lt;/code&gt; and &lt;code class='language-text'&gt;Test$&lt;/code&gt; from &lt;code class='language-text'&gt;object Test&lt;/code&gt; and the &lt;code class='language-text'&gt;mainClass&lt;/code&gt; (&lt;code class='language-text'&gt;Test&lt;/code&gt;) I passed in is not that (&lt;code class='language-text'&gt;Test$&lt;/code&gt;) extends &lt;code class='language-text'&gt;scala.App&lt;/code&gt;. It's recorded in &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26977&quot;&gt;SPARK-26977&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The solution is not (&lt;strong&gt;never for Spark&lt;/strong&gt;) to extend &lt;code class='language-text'&gt;scala.App&lt;/code&gt; and put everything in Test's own main method. Then &lt;code class='language-text'&gt;foo&lt;/code&gt; and &lt;code class='language-text'&gt;bar&lt;/code&gt; are now initialized local variables and shipped remotely through closure.&lt;/p&gt;
&lt;h3&gt;Appendix&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;How to decompile the anonymous function passed to &lt;code class='language-text'&gt;foreachPartition&lt;/code&gt; ?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;javap -c Test\$\$anonfun\$1 
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How to enable driver's debug log ?&lt;/p&gt;
&lt;p&gt;Prepare a &lt;code class='language-text'&gt;log4j.properties&lt;/code&gt; file with&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;log4j.rootLogger=DEBUG
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and submit a Spark application with&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;--driver-java-options &amp;quot;-Dlog4j.configuration=file:/path/to/log4j.properties&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description><pubDate>Sat, 23 Feb 2019 00:00:00 GMT</pubDate></item><item><title>Gearpump Log #2</title><link>https://manuzhang.github.io/posts/2018-12-23-gearpump-log-2/index.html</link><description>&lt;p&gt;Each pull request on Gearpump will be checked for &lt;a href=&quot;https://codecov.io/gh/gearpump/gearpump&quot;&gt;code coverage&lt;/a&gt; thanks to &lt;a href=&quot;https://github.com/scoverage/sbt-scoverage&quot;&gt;sbt-scoverage&lt;/a&gt; plugin. &lt;code class='language-text'&gt;sbt coverage test&lt;/code&gt; had been added to &lt;code class='language-text'&gt;.travis.yml&lt;/code&gt; to trigger the check.&lt;/p&gt;
&lt;p&gt;The coverage suddenly dropped to 0 when I &lt;a href=&quot;https://github.com/gearpump/gearpump/pull/2106&quot;&gt;upgraded Gearpump's Scala version to 2.12&lt;/a&gt;. The &lt;a href=&quot;https://github.com/gearpump/gearpump/commit/78bed0b457ce7e7c4cd57f5ad6502a880f5f66f4#diff-a84f91f20f040218bccd09fed4761fb3&quot;&gt;suspicious change&lt;/a&gt; was upgrading sbt-scoverage from &lt;code class='language-text'&gt;1.2.0&lt;/code&gt; to &lt;code class='language-text'&gt;1.5.1&lt;/code&gt;. I thought the problem was related to the sbt version &lt;code class='language-text'&gt;0.13.16&lt;/code&gt; and &lt;a href=&quot;https://github.com/scoverage/sbt-scoverage/issues/270&quot;&gt;asked about it on sbt-scoverage's issues&lt;/a&gt;. Meanwhile, I went on to try my luck with sbt &lt;code class='language-text'&gt;1.2.7&lt;/code&gt; which turned a minor issue into significant work because all SBT plugins had to be upgraded to versions that work with SBT &lt;code class='language-text'&gt;1.x&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;One roadblock was we'd maintained forked versions of &lt;a href=&quot;https://github.com/sbt/sbt-assembly&quot;&gt;sbt-assembly&lt;/a&gt; and &lt;a href=&quot;https://github.com/xerial/sbt-pack&quot;&gt;sbt-pack&lt;/a&gt; from &lt;code class='language-text'&gt;0.13.x&lt;/code&gt; and it looked very hard to port those changes.&lt;/p&gt;
&lt;h3&gt;Upgrading sbt-assembly&lt;/h3&gt;
&lt;p&gt;Gearpump uses sbt-assembly to create fat jars with shaded dependencies for sub-modules. Tinkering with sbt-assembly has contributed to my blog series &lt;em&gt;Shade with SBT &lt;a href=&quot;https://manuzhang.github.io/2016/10/15/shading.html&quot;&gt;I&lt;/a&gt;, &lt;a href=&quot;https://manuzhang.github.io/2016/11/12/shading-2.html&quot;&gt;II&lt;/a&gt; and &lt;a href=&quot;https://manuzhang.github.io/2017/04/21/shading-3.html&quot;&gt;III&lt;/a&gt;&lt;/em&gt;. There was one more issue. &lt;a href=&quot;https://issues.apache.org/jira/browse/GEARPUMP-162&quot;&gt;Running gearpump examples in Intellij or SBT would fail&lt;/a&gt; with &lt;code class='language-text'&gt;ClassNotFoundException&lt;/code&gt; since they had &lt;strong&gt;provided&lt;/strong&gt; dependency on &lt;code class='language-text'&gt;gearpump-core&lt;/code&gt; and &lt;code class='language-text'&gt;gearpump-streaming&lt;/code&gt;. My solution then was changing the scope to &lt;strong&gt;compile&lt;/strong&gt; and &lt;a href=&quot;https://github.com/apache/incubator-gearpump/pull/200/files#diff-7f7e51fd877cefd5501e7567e56a9858R180&quot;&gt;manually filtering out everything other than examples' own source codes&lt;/a&gt; with the built-in &lt;code class='language-text'&gt;assemblyExcludedJars&lt;/code&gt; option. There was one limitation with the option that it only excluded jars. Hence, I went on to make a tweak &lt;a href=&quot;https://github.com/manuzhang/sbt-assembly/commit/344e092171c3e6f29f73b73df58884b239573915#diff-c670e50888639e8441bc5754a62689dfR182&quot;&gt;extending the exclusion to all files&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Apparently, it's not worth maintaining a tweak and porting it from one forked version to another. Therefore, I &lt;a href=&quot;https://github.com/gearpump/gearpump/commit/f6497e9d470dc8fbc70edb44cc68db1bb54ccdc8#diff-a84f91f20f040218bccd09fed4761fb3R3&quot;&gt;switched to the official sbt-assembly 0.14.9&lt;/a&gt; and used the &lt;code class='language-text'&gt;assemblyMergeStrategy&lt;/code&gt; option this time.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;assemblyMergeStrategy in assembly := {
x =&amp;gt;
  // core and streaming dependencies are not marked as provided
  // such that the examples can be run with sbt or Intellij
  // so they have to be excluded manually here
  if (x.contains(&amp;quot;examples&amp;quot;)) {
    val oldStrategy = (assemblyMergeStrategy in assembly).value
    oldStrategy(x)
  } else {
    MergeStrategy.discard
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It's working! &lt;a href=&quot;https://github.com/gearpump/gearpump/issues/2118&quot;&gt;At least 50%&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This brought in another issue that &lt;a href=&quot;https://github.com/gearpump/gearpump/issues/2120&quot;&gt;the assembled jar were not published as before&lt;/a&gt; with the following.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;addArtifact(Artifact(&amp;quot;gearpump-core&amp;quot;), sbtassembly.AssemblyKeys.assembly)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After taking a closer look, I found the previous way was fragile where the unassembled and assembled jars were created with the same name in different directories and the latter &lt;strong&gt;happened&lt;/strong&gt; to override the former when publishing. The &lt;a href=&quot;https://github.com/gearpump/gearpump/pull/2122/files&quot;&gt;fix&lt;/a&gt; was to append an &lt;code class='language-text'&gt;assembly&lt;/code&gt; to the assembled jar and alter the artifact setting &lt;a href=&quot;https://github.com/sbt/sbt-assembly#publishing-not-recommended&quot;&gt;as documented&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;artifact in (Compile, assembly) := {
  val art = (artifact in (Compile, assembly)).value
  art.withClassifier(Some(&amp;quot;assembly&amp;quot;))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By the way, it's not recommended to publish fat jars so I think this merits revisiting later.&lt;/p&gt;
&lt;h3&gt;Upgrading sbt-pack&lt;/h3&gt;
&lt;p&gt;&lt;code class='language-text'&gt;sbt-pack&lt;/code&gt; has served to create distributable Gearpump packages with launch scripts under &lt;code class='language-text'&gt;bin&lt;/code&gt; and system jars (output from sbt-assembly) under &lt;code class='language-text'&gt;lib&lt;/code&gt;. The &lt;code class='language-text'&gt;lib&lt;/code&gt; directory is by default put on all classpaths in the launch scripts. Not to conflict with users' dependencies, Gearpump tried to put system dependencies under subdirectories of &lt;code class='language-text'&gt;lib&lt;/code&gt;. My colleague Vincent &lt;a href=&quot;https://github.com/huafengw/sbt-pack/commit/88fd39e01a263e142c8668124beb88018b58ef69&quot;&gt;hacked into the packLibDir option&lt;/a&gt; and made the following possible.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;packLibDir := Map(	      
  &amp;quot;lib/hadoop&amp;quot; -&amp;gt; new ProjectsToPack(gearpumpHadoop.id).
      exclude(services.id, core.id),
  &amp;quot;lib/services&amp;quot; -&amp;gt; new ProjectsToPack(services.id).exclude(core.id)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Like sbt-assembly, I decided not to maintain the forked sbt-pack given the cost of porting. &lt;a href=&quot;https://github.com/gearpump/gearpump/commit/d2655ed5b2629ae7bb31f19caf595c2b46683588&quot;&gt;The solution was to pack system dependencies into other directories&lt;/a&gt; on the same level as &lt;code class='language-text'&gt;lib&lt;/code&gt; and I was able to upgrade sbt-pack to 0.11.&lt;/p&gt;
&lt;h3&gt;Other changes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/gearpump/gearpump/commit/801101732e315a63ba02af9d926f83df65cf5fb0&quot;&gt;sbt-unidoc has been upgraded to 0.4.2&lt;/a&gt; with &lt;a href=&quot;https://github.com/gearpump/gearpump/issues/2117&quot;&gt;unresolved Javadoc errors&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/gearpump/gearpump/commit/801101732e315a63ba02af9d926f83df65cf5fb0&quot;&gt;License headers have been updated removing ASF declarations&lt;/a&gt; with the help of &lt;a href=&quot;https://www.jetbrains.com/help/idea/copyright.html&quot;&gt;Intellij's updating copyright text button&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Build scripts have been moved from classes extending &lt;code class='language-text'&gt;sbt.Build&lt;/code&gt; in various &lt;code class='language-text'&gt;.scala&lt;/code&gt; files to &lt;code class='language-text'&gt;build.sbt&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Is the coverage issue solved ?&lt;/h3&gt;
&lt;p&gt;No! Unfortunately the coverage issue was not solved even when &lt;a href=&quot;https://github.com/gearpump/gearpump/commit/331f6e0f76dce8145a9fef286e82ef210861375b&quot;&gt;SBT had been upgraded 1.2.7&lt;/a&gt;. It's turned out that SBT version is not the scapegoat but that I have not read sbt-scoverage's README carefully enough. &lt;code class='language-text'&gt;sbt coverage&lt;/code&gt; only outputs the coverage data and &lt;code class='language-text'&gt;sbt coverageReport&lt;/code&gt; is needed to generate the report.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;sbt coverage test
sbt coverageReport
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that &lt;code class='language-text'&gt;coverageReport&lt;/code&gt; can't be put in the same command as &lt;code class='language-text'&gt;coverage&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;It has left me thinking whether my fight with SBT is worthwhile.&lt;/p&gt;
</description><pubDate>Sun, 23 Dec 2018 00:00:00 GMT</pubDate></item><item><title>Gearpump Log #1</title><link>https://manuzhang.github.io/posts/2018-12-02-gearpump-log-1/index.html</link><description>&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;[INFO] Gearpump retires from Apache Incubator 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After incubating for two and a half years at Apache Software Foundation (ASF), Gearpump recently &lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/incubator-general/201809.mbox/%3CCABT57mYLRAYvVS5N2GrD-07ddAdgkf42ibZmdXpeGQHAf%2BwcDg%40mail.gmail.com%3E&quot;&gt;retired from Apache incubator&lt;/a&gt; and &lt;a href=&quot;https://github.com/gearpump/gearpump&quot;&gt;returned to GitHub home&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Hi all,&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Gearpump has been an Apache incubator since 2016-03-08. Thank you all for help to incubate Gearpump and keep it move forward.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;The activity around Gearpump has, however, slowed down and the last release was more than one year ago. There is no sign of a growing community, which is a requirement to be a Apache TLP. The long period of release process has made things even harder for such a slim community.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;I don't see a future for Gearpump within Apache so it's better off to leave.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;What do you think ?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Thanks,&lt;br /&gt;
Manu Zhang&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;After had been the only one to develop Gearpump for a while, I finally started the discussion to retire it on dev list. We reached consensus and passed a formal vote. After another vote passed on general list, Gearpump officially retired. &lt;strong&gt;Note retirement doesn't mean the end of this project and Gearpump will continue on GitHub reserving all commits.&lt;/strong&gt; The &lt;a href=&quot;https://github.com/apache/incubator-gearpump&quot;&gt;Apache repo&lt;/a&gt; is archived.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;[INFO] Gearpump continues on GitHub
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A new journey has started. From now on, I'd like to log everything (commits, issues, thoughts, etc) of Gearpump . I regret not having done so earlier as I can hardly remember the early design decisions we made and why.&lt;/p&gt;
&lt;p&gt;The first thing I did is adding &lt;a href=&quot;https://github.com/fthomas/scala-steward&quot;&gt;scala-steward&lt;/a&gt; integration which helps to keep Gearpump's dependencies up-to-date. Scala-steward robot has opened 55 pull requests and I've merged 5 of them to update Akka, algebird, etc. It will periodically check for updates.&lt;/p&gt;
&lt;p&gt;Then I planed to decompose the huge codebase into various projects under &lt;a href=&quot;https://github.com/gearpump&quot;&gt;gearpump group&lt;/a&gt; since it would be too much for me to maintain and upgrade dependencies. For example, the &lt;code class='language-text'&gt;external-kafka&lt;/code&gt; module depends on Kafka 0.8.2.1 which has no scala 2.12 release. I drew a graph of &lt;a href=&quot;https://github.com/gearpump/gearpump/issues/2089#issuecomment-439678535&quot;&gt;inter-project dependency&lt;/a&gt; with &lt;a href=&quot;https://github.com/dwijnand/sbt-project-graph&quot;&gt;sbt-project-graph&lt;/a&gt; and ported the external modules to &lt;a href=&quot;https://github.com/gearpump/gearpump-externals&quot;&gt;gearpump-externals&lt;/a&gt; along with corresponding examples. Nonetheless, I quitted mid-way when I found it's too hard to separate out the remaining modules. Given everything is already archived at the Apache repo, I simply &lt;a href=&quot;https://github.com/gearpump/gearpump/commit/42b13c09b5e3192b0a3b760594e4ebe5b67bd68a&quot;&gt;removed non-core modules&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As Gearpump is no longer an Apache project, the package must be renamed from &lt;code class='language-text'&gt;org.apache.gearpump&lt;/code&gt; and ASF license headers removed. It's a &lt;a href=&quot;https://github.com/gearpump/gearpump/commit/25bb3e04a5feb6e4fe639a5eea78893822f365a7&quot;&gt;big change&lt;/a&gt; and renaming package with Intellij pushed my 2015 Mac Pro to the limit. A &lt;a href=&quot;https://github.com/gearpump/gearpump/commit/7c94cfdb7673e9b05cd836c983c3e2fddf1d59ad&quot;&gt;follow-up fix&lt;/a&gt; had to be made for what I had left out.&lt;/p&gt;
&lt;p&gt;Upgrading to Scala 2.12 had been on my schedule for a while but was blocked by an ancient version of &lt;a href=&quot;https://github.com/lihaoyi/upickle&quot;&gt;Upickle&lt;/a&gt;. Upickle is the json serialization library for communication between Gearpump's dashboard and backend. The usage of Upickle has changed a lot. It used to generate reader/writers for a case class automatically while they have to be &lt;a href=&quot;https://github.com/gearpump/gearpump/commit/6c9727df81089d78627dd00de7219db56b0dbcdb#diff-315a1559a02aa4b74e0c56c13c345485&quot;&gt;defined manually now&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/gearpump/gearpump/commit/78bed0b457ce7e7c4cd57f5ad6502a880f5f66f4&quot;&gt;Upgrading to Scala 2.12&lt;/a&gt; itself was not too much work. From the &lt;a href=&quot;https://www.scala-lang.org/news/2.12.0/&quot;&gt;release blog&lt;/a&gt; of Scala 2.12.0&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Although Scala 2.11 and 2.12 are mostly source compatible to facilitate cross-building, they are not binary compatible&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Some notable differences are&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;These &lt;code class='language-text'&gt;scalacOption&lt;/code&gt;s, &lt;code class='language-text'&gt;-Yinline-warnings&lt;/code&gt;, &lt;code class='language-text'&gt;-Yclosure-elim&lt;/code&gt; and &lt;code class='language-text'&gt;-Yinline&lt;/code&gt;, no longer exist. Here are some &lt;a href=&quot;https://tpolecat.github.io/2017/04/25/scalac-flags.html&quot;&gt;recommended flags for 2.12&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Type inference. &lt;code class='language-text'&gt;-Xsource:2.11&lt;/code&gt; can get you the old behavior and tell whether the failure is brought by changes in 2.12.&lt;/p&gt;
&lt;p&gt;a.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;trait BasicService {
  private val LOG: Logger = LogUtil.getLogger(getClass) 
}
// [error] overloaded method value getLogger with alternatives
// [T](clazz: Class[T]org.slf4j.Logger &amp;lt;and&amp;gt;
// [T](clazz: Class[T], context: String ...)org.slf4j.Logger

private val LOG: Logger = LogUtil.getLogger(classOf[BasicService]) //OK
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;b.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;class DummyRunner[T] extends FlatMapper[T, T](FlatMapFunction(Option(_)), &amp;quot;&amp;quot;)
// [error] missing parameter type for expanded function

class DummyRunner[T] extends FlatMapper[T,T](
    FlatMapFunction((t =&amp;gt; Option(t)): T =&amp;gt; TraversableOnce[T]), &amp;quot;&amp;quot;) //OK

&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With these major changes, I &lt;a href=&quot;https://github.com/gearpump/gearpump/commit/08e2ac4762aa9a73ea424f1b19484e209663c9aa&quot;&gt;bumped up the target version of next release&lt;/a&gt; from &lt;code class='language-text'&gt;0.8.5-SNAPSHOT&lt;/code&gt; to &lt;code class='language-text'&gt;0.9.0-SNAPSHOT&lt;/code&gt;. Next step would be more testing and make sure there is no regression with &lt;a href=&quot;https://github.com/apache/beam/tree/master/runners/gearpump&quot;&gt;Beam's Gearpump runner&lt;/a&gt;. If everything is OK, then &lt;code class='language-text'&gt;0.9.0&lt;/code&gt; is good to go.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;[INFO] 13 commits, 8 issues opened and 6 resolved after the return of Gearpump
&lt;/code&gt;&lt;/pre&gt;
</description><pubDate>Sun, 2 Dec 2018 00:00:00 GMT</pubDate></item><item><title>A whirlwind tour of Kafka Summit SF 2018</title><link>https://manuzhang.github.io/posts/2018-11-10-kafka-summit-sf-2018/index.html</link><description>&lt;p&gt;Kafka Summit San Francisco 2018 took place last month with +1200 attendees from +350 companies. &lt;a href=&quot;https://www.confluent.io/resources/kafka-summit-san-francisco-2018/&quot;&gt;All 60 talks with videos and slides side by side&lt;/a&gt; have been posted on Confluent's website and nicely organized in keynotes and four tracks (Pipeline, Streams, Internals and Business).&lt;/p&gt;
&lt;p&gt;I wasn't there but I've glanced through all the slides and lingered over those I found more interesting. I'd like to share my impressions and provide a whirlwind tour of the conference. Here's an &lt;a href=&quot;https://www.confluent.io/blog/kafka-summit-san-francisco-2018-roundup&quot;&gt;official wrap-up&lt;/a&gt;, by the way.&lt;/p&gt;
&lt;h3&gt;Is Kafka a Database&lt;/h3&gt;
&lt;p&gt;As you know, Kafka was born a message queue and has grown into a full-fledged streaming platform with &lt;a href=&quot;https://kafka.apache.org/documentation/#connect&quot;&gt;Kafka Connect&lt;/a&gt;, &lt;a href=&quot;https://kafka.apache.org/documentation/streams/&quot;&gt;Kafka Streams&lt;/a&gt; and &lt;a href=&quot;https://github.com/confluentinc/ksql&quot;&gt;KSQL&lt;/a&gt;. What else can Kafka be ? How about a database ! Martin Kleppmann argues that &lt;a href=&quot;https://www.confluent.io/kafka-summit-SF18/is-kafka-a-database&quot;&gt;Kafka is a database&lt;/a&gt; and achieves &lt;a href=&quot;https://en.wikipedia.org/wiki/ACID_(computer_science)&quot;&gt;ACID&lt;/a&gt; properties as in relational databases. This is mind-boggling since relation databases don't provide ACID at scale. Note that you can't set up a Kafka cluster and get ACID for free but need to carefully design your Kafka topics and streaming applications as demonstrated by the author. Still, I have some doubt over whether it can only ensure eventual consistency.&lt;/p&gt;
&lt;h3&gt;Kafka for IoT&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/MQTT&quot;&gt;MQTT&lt;/a&gt; is a lightweight messaging protocol, built for tens of thousands of client connections in unstable network but it lacks in buffering, stream processing, reprocessing and scalability. Those're what Kafka is good at and they make a perfect match to &lt;a href=&quot;https://www.confluent.io/kafka-summit-sf18/processing-iot-data-from-end-to-end&quot;&gt;process IoT data from end to end&lt;/a&gt; through &lt;a href=&quot;https://www.slideshare.net/ConfluentInc/processing-iot-data-from-end-to-end-with-mqtt-and-apache-kafka/27&quot;&gt;Kafka Connect&lt;/a&gt;, &lt;a href=&quot;https://www.slideshare.net/ConfluentInc/processing-iot-data-from-end-to-end-with-mqtt-and-apache-kafka/35&quot;&gt;MQTT Proxy&lt;/a&gt; or &lt;a href=&quot;https://www.slideshare.net/ConfluentInc/processing-iot-data-from-end-to-end-with-mqtt-and-apache-kafka/38&quot;&gt;REST Proxy&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Global Kafka&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://www.slideshare.net/ConfluentInc/data-streaming-ecosystem-management-at-bookingcom/26&quot;&gt;Booking manages a global Kafka cluster with brokers spread over three zones&lt;/a&gt;. They have containerized replicator, Kafka connect and Kafka monitor on Kubernetes. It' worth mention that they have run into issues with distributed mode of Kafka Connect so the standalone mode is used instead.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://www.slideshare.net/ConfluentInc/more-data-more-problems-scaling-kafkamirroring-pipelines-at-linkedin&quot;&gt;Linkedin has built its own Brooklin MirrorMaker (BMM)&lt;/a&gt; to mirror 100+ pipelines across 9 data centers with only 9 BMM clusters. In contrast, Kafka MirrorMaker (KMM) needs 100+ clusters. Moreover, KMM is difficult to operate, poor to isolate failure and unable to catch up with traffic while BMM has dynamic configuration, automatically pausing and resuming mirroring at partition level, and good throughput with almost linear scalability.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To overcome the explosion of number of KMMs, &lt;a href=&quot;https://www.slideshare.net/ConfluentInc/beyond-messaging-enterprisescale-multicloud-intelligent-routing/27&quot;&gt;Adobe introduces a routing KMM cluster&lt;/a&gt; to reduce the number of directly connected data center pipelines. They also have written &lt;a href=&quot;https://medium.com/adobetech/creating-the-adobe-experience-platform-pipeline-with-kafka-4f1057a11ef&quot;&gt;a nice summary of their Experience Platform Pipeline built on Kafka&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Streaming platform&lt;/h3&gt;
&lt;p&gt;When Kafka Streams first came out, I was wondering why I would need another Streaming platform given the place had already been crowded with &lt;a href=&quot;https://flink.apache.org/&quot;&gt;Flink&lt;/a&gt;, &lt;a href=&quot;https://spark.apache.org/streaming/&quot;&gt;Spark Streaming&lt;/a&gt;, &lt;a href=&quot;https://storm.apache.org/&quot;&gt;Storm&lt;/a&gt;, &lt;a href=&quot;https://github.com/gearpump/gearpump&quot;&gt;Gearpump&lt;/a&gt;, etc. Today it strikes me that why I would need another Streaming platform and all the workloads to set up a cluster and maintain when I can do streaming with my Kafka already there. Futhurmore, Confluent adds &lt;a href=&quot;https://github.com/confluentinc/ksql&quot;&gt;KSQL&lt;/a&gt;, a SQL interface, to relieve you of writing cumbersome codes.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Thus, maybe it's time to &lt;a href=&quot;https://www.slideshare.net/ConfluentInc/crossing-the-streams-rethinking-stream-processing-with-kstreams-and-ksql-120334214&quot;&gt;rethink stream processing with Kafka Streams and KSQL&lt;/a&gt;. There is an illustrative analogy between Kafka ecosystem and Unix Pipeline. &lt;img src=&quot;https://image.slidesharecdn.com/05viktorgamov-181022184511/95/crossing-the-streams-rethinking-stream-processing-with-kstreams-and-ksql-12-638.jpg?cb=1540233945&quot; alt=&quot;kafka_analogy&quot; /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stream joins are not your father's &lt;a href=&quot;https://www.w3schools.com/sql/sql_join.asp&quot;&gt;SQL joins&lt;/a&gt;. How joins are implemented in Kafka Streams and when to use them ? Read this &lt;a href=&quot;https://www.slideshare.net/ConfluentInc/zen-and-the-art-of-streaming-joinsthe-what-when-and-why&quot;&gt;zen and the art of streaming joins&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Companies like &lt;a href=&quot;https://www.slideshare.net/ConfluentInc/data-streaming-ecosystem-management-at-bookingcom/5&quot;&gt;Booking&lt;/a&gt; and &lt;a href=&quot;https://www.slideshare.net/ConfluentInc/realtime-dynamic-data-export-using-the-kafka-ecosystem/33&quot;&gt;Braze&lt;/a&gt; are building their streaming pipeline around Kafka Connect (data import/export) and Kafka Streams (data processing). &lt;img src=&quot;https://image.slidesharecdn.com/02alexmironov-181023055944/95/data-streaming-ecosystem-management-at-bookingcom-5-638.jpg?cb=1540274422&quot; alt=&quot;booking_arch&quot; /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Change Data Capture (CDC) is a way to make use of data and schema changes of your database. &lt;a href=&quot;https://www.slideshare.net/ConfluentInc/change-data-streaming-patterns-for-microservices-with-debezium&quot;&gt;Deberium is a CDC platform for various databases based on Kafka&lt;/a&gt; from Red Hat. &lt;a href=&quot;https://www.slideshare.net/ConfluentInc/pinterests-story-of-streaming-hundreds-of-terabytes-of-pins-from-mysql-to-s3hadoop-continuously&quot;&gt;Pinterest also shares their story of streaming hundreds of TBs of Pins from MySQL to S3&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Kafka on Cloud&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Intuit have deployed &lt;a href=&quot;https://www.slideshare.net/ConfluentInc/kafka-on-kubernetesfrom-evaluation-to-production-at-intuit&quot;&gt;Kafka on Kubernetes in production&lt;/a&gt; with proper configurations on load balancing, security, etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://www.slideshare.net/ConfluentInc/change-data-streaming-patterns-for-microservices-with-debezium/28&quot;&gt;Red Hat provide an enterprise distribution of Kafka on Kubernetes/OpenShift&lt;/a&gt; with an open-source upstream, &lt;a href=&quot;https://github.com/strimzi&quot;&gt;Strimzi&lt;/a&gt;. Typical Red Hat.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Google is not using Kafka internally but there is a demand for Kafka on Google Cloud Platform (GCP). It's interesting to learn &lt;a href=&quot;https://www.slideshare.net/ConfluentInc/putting-kafka-together-with-the-best-of-google-cloud-platform&quot;&gt;Google's perspective on Kafka and how they fit Kafka into GCP&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Confluent offers some official &lt;a href=&quot;https://www.slideshare.net/ConfluentInc/deploying-kafka-streams-applications-with-docker-and-kubernetes/33&quot;&gt;recommendations on deploying Kafka Streams with Docker and Kubernetes&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Internals&lt;/h3&gt;
&lt;p&gt;As a developer, I love nearly every deck of the Internals track.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Zalando shared their &lt;a href=&quot;https://www.slideshare.net/ConfluentInc/war-stories-diy-kafka-120353401&quot;&gt;War stories: DIY Kafka&lt;/a&gt;, especially lessons learned in backing up Zookeeper and Kafka&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Backups are only backups if you know how to restore them&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Another war story from a Kafka veteran, Todd Palino, on &lt;a href=&quot;https://www.slideshare.net/ConfluentInc/urp-excuse-you-the-three-metrics-you-have-to-know-120353592&quot;&gt;monitoring Linkedin's 5-trillion-message-per-day Kafka cluster&lt;/a&gt;. His lesson is not to monitor and alert on everything but to keep an eye on three key metrics, under-replicated partition, request handler and request timing. I totally agree with him that&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;sleep is best in life&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://www.slideshare.net/ConfluentInc/kafka-on-zfs-better-living-through-filesystems&quot;&gt;ZFS makes Kafka faster and cheaper&lt;/a&gt; through improving cache hit rates and making clever use of I/O devices.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Jason Gustafson walked you through &lt;a href=&quot;https://www.slideshare.net/ConfluentInc/hardening-kafka-replication&quot;&gt;failure scenarios of Kafka log replication and hardening it with TLA+ model checker&lt;/a&gt;. ZipRecruiter is also &lt;a href=&quot;https://www.slideshare.net/ConfluentInc/using-chaos-engineering-to-level-up-apache-kafka-skills&quot;&gt;using Chaos Engineering to level up Kafka skills&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kafka is famous for using &lt;a href=&quot;http://man7.org/linux/man-pages/man2/sendfile.2.html&quot;&gt;sendfile system call&lt;/a&gt; to accelerate data transfer. Do you know sendfile can be blocking ? I really enjoy how &lt;a href=&quot;https://www.slideshare.net/ConfluentInc/kafka-multitenancy160-billion-daily-messages-on-one-shared-cluster-at-line/&quot;&gt;Yuto from Line has approached the issue from hypothesis to solution&lt;/a&gt;. (More details at the amazing &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7504&quot;&gt;KAFKA-7504&lt;/a&gt;). His &lt;a href=&quot;https://www.slideshare.net/ConfluentInc/one-day-one-data-hub-100-billion-messages-kafka-at-line&quot;&gt;analysis of long GC pause harming broker performance caused by mmap&lt;/a&gt; from last year is a great computer system lesson as well.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Who has not secured Kafka ? If you don't know the answer, then Stephane Maarek from DataCumulus &lt;a href=&quot;https://www.slideshare.net/ConfluentInc/kafka-security-101-and-realworld-tips&quot;&gt;offers some introductions and real-world tips&lt;/a&gt;. For example, &lt;img src=&quot;https://image.slidesharecdn.com/02stephanemaarek-181023061947/95/kafka-security-101-and-realworld-tips-18-638.jpg?cb=1540275623&quot; alt=&quot;kafka_kerberos&quot; /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://www.slideshare.net/ConfluentInc/reliable-message-delivery-with-apache-kafka&quot;&gt;Uber keeps pushing for producer performance improvement without sacrificing at-least-once delivery&lt;/a&gt;. They shared about their best practices such as increasing replica fetch frequency and reducing thread contention and GC pauses.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Wrap-up&lt;/h3&gt;
&lt;p&gt;Kafka is definitely the backbone of companies' data architectures and serves as many as billions of messages per day. More and more streaming applications are built with Kafka Connect, Kafka Streams and KSQL. Meanwhile, quite a few companies are managing and mirroring Kafka at global scale. Finally, Kafka on Kubernetes is the new fashion.&lt;/p&gt;
</description><pubDate>Sat, 10 Nov 2018 00:00:00 GMT</pubDate></item><item><title>Upgrading to Spark 2.3.1</title><link>https://manuzhang.github.io/posts/2018-08-11-upgrade-spark-2.3/index.html</link><description>&lt;p&gt;Recently we upgraded our Spark cluster from 2.2.0 to 2.3.1. The transition has been smooth except that the Spark driver address has changed from IP to hostname.&lt;/p&gt;
&lt;p&gt;One thing I found immediately was that submitted application didn't run and I also failed to open application's Web UI page. I looked into the log and it said the driver couldn't be connected by its hostname, which is expected in our cluster environment. Back in Spark 2.2.0, executors found driver by its IP. So what has changed ? Google led me to &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-21642&quot;&gt;SPARK-21642&lt;/a&gt; whose change set driver address to FQDN from IP for SSL cases. This would break in environments where hostname could not be resolved. Luckily, a nice guy provided a solution in the comment&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I ran into the same problem. I simply put &lt;code class='language-text'&gt;export SPARK_LOCAL_HOSTNAME=$(ip route get 1 | awk '{print $NF;exit}')&lt;/code&gt;
into my launch script to solve the issue. This forces spark to use your IP as hostname. It should work on any debian based system.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We set &lt;code class='language-text'&gt;SPARK_LOCAL_HOSTNAME&lt;/code&gt; to IP in &lt;code class='language-text'&gt;spark-env.sh&lt;/code&gt; and it worked like a charm.&lt;/p&gt;
</description><pubDate>Sat, 11 Aug 2018 00:00:00 GMT</pubDate></item><item><title>The hidden cost of Spark withColumn</title><link>https://manuzhang.github.io/posts/2018-07-11-spark-catalyst-cost/index.html</link><description>&lt;p&gt;Recently, we've been working on machine learning pipeline with Spark, where &lt;a href=&quot;https://spark.apache.org/sql/&quot;&gt;Spark SQL &amp;amp; DataFrame&lt;/a&gt; is used for data preprocessing and &lt;a href=&quot;https://spark.apache.org/mllib/&quot;&gt;MLlib&lt;/a&gt; for training. In one use case, the data source is a very wide Hive table of ~1000 columns. The columns are stored in String so we need to cast them to Integer before they can be fed into model training.&lt;/p&gt;
&lt;p&gt;This was what I got initially with DataFrame Scala API (2.2.0).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;df.columns.foldLeft(df) { case (df, col) =&amp;gt;
  df.withColumn(col, df(col).cast(IntegerType))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since DataFrame is immutable, I created a new &lt;code class='language-text'&gt;DataFrame&lt;/code&gt; for each &lt;code class='language-text'&gt;Column&lt;/code&gt; casted using &lt;code class='language-text'&gt;withColumn&lt;/code&gt;. I didn't think that was be a big deal since at run time all columns would be casted in one shot thanks to &lt;a href=&quot;https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html&quot;&gt;Spark's Catalyst optimizer&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;At its core, Catalyst contains a general library for representing trees and applying rules to manipulate them. On top of this framework, we have built libraries specific to relational query processing (e.g., expressions, logical query plans), and several sets of rules that handle different phases of query execution: analysis, logical optimization, physical planning, and code generation to compile parts of queries to Java bytecode&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To my surprise, the job stuck in submission for minutes without outputting anything. Luckily, I have a nice colleague Vincent who saved my day with the following fix.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;df.select(df.columns.map { col =&amp;gt;
  df(col).cast(IntegerType)
}: _*)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;He suspected that it's expensive to call &lt;code class='language-text'&gt;withColumn&lt;/code&gt; for a thousand times. Hence, he dived into its implementation and found out the above in the private method &lt;code class='language-text'&gt;withColumns&lt;/code&gt; called by &lt;code class='language-text'&gt;withColumn&lt;/code&gt;. In his fast version, only one new &lt;code class='language-text'&gt;DataFrame&lt;/code&gt; was created.&lt;/p&gt;
&lt;p&gt;I wondered why there was a significant cost difference and looked further into it. After turning on the debug log, I saw a lot of &lt;code class='language-text'&gt;=== Result of Batch Resolution ===&lt;/code&gt;s in my slow version. It suddenly struck me that Catalyst's analysis might not be free. A thousand &lt;code class='language-text'&gt;withColumn&lt;/code&gt;s were actually a thousand times of analysis, which held true for all APIs on &lt;code class='language-text'&gt;DataFrame&lt;/code&gt;. On the other hand, analysis of transform on &lt;code class='language-text'&gt;Column&lt;/code&gt; was actually lazy.&lt;/p&gt;
&lt;p&gt;The log led me to &lt;code class='language-text'&gt;org.apache.spark.sql.catalyst.rules.RuleExecutor&lt;/code&gt; where I  spotted a &lt;code class='language-text'&gt;timeMap&lt;/code&gt; tracking time running specific rules. What's more exciting, the statistics was exposed through &lt;code class='language-text'&gt;RuleExecutor.dumpTimeSpent&lt;/code&gt; which I could add to compare the costs in two versions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;df.columns.foldLeft(df) { case (df, col) =&amp;gt;
  println(RuleExecutor.dumpTimeSpent())
  df.withColumn(col, df(col).cast(IntegerType))
}
println(RuleExecutor.dumpTimeSpent())

df.select(df.columns.map { col =&amp;gt;
  println(RuleExecutor.dumpTimeSpent())
  df(col).cast(IntegerType)
}: _*)
println(RuleExecutor.dumpTimeSpent())

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected, the time spent increased for each &lt;code class='language-text'&gt;DataFrame#withColumn&lt;/code&gt; while that stayed the same for &lt;code class='language-text'&gt;Column#cast&lt;/code&gt;. It would take about 100ms for one round of analysis. (I have a table of time spent for all 56 rules in 100 &lt;code class='language-text'&gt;withColumn&lt;/code&gt; calls in appendix if you are curious).&lt;/p&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;The hidden cost of &lt;code class='language-text'&gt;withColumn&lt;/code&gt; is Spark Catalyst's analysis time.&lt;/li&gt;
&lt;li&gt;The time spent in Catalyst analysis is usually negligible but it will become an issue when there is a large number of transforms on &lt;code class='language-text'&gt;DataFrame&lt;/code&gt;. It's not unusual for a model to have 1000 features which may require preprocessing.&lt;/li&gt;
&lt;li&gt;Don't create a new &lt;code class='language-text'&gt;DataFrame&lt;/code&gt; for each transform on &lt;code class='language-text'&gt;Column&lt;/code&gt;. Create one at last with &lt;code class='language-text'&gt;DataFrame#select&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Appendix&lt;/h3&gt;
&lt;table class=&quot;pure-table&quot;&gt;
&lt;thead&gt;
&lt;tr&gt;&lt;th&gt;Rule&lt;/th&gt;&lt;th&gt;Nano Time&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.Analyzer$FixNullability&lt;/td&gt;&lt;td&gt;489262230&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveGroupingAnalytics&lt;/td&gt;&lt;td&gt;243030776&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.TypeCoercion$PropagateTypes&lt;/td&gt;&lt;td&gt;143141555&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer&lt;/td&gt;&lt;td&gt;97690381&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.ResolveCreateNamedStruct&lt;/td&gt;&lt;td&gt;87845664&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveWindowFrame&lt;/td&gt;&lt;td&gt;85098172&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveWindowOrder&lt;/td&gt;&lt;td&gt;83967566&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.Analyzer$ExtractWindowExpressions&lt;/td&gt;&lt;td&gt;63928074&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences&lt;/td&gt;&lt;td&gt;56549170&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions&lt;/td&gt;&lt;td&gt;52411767&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.Analyzer$ExtractGenerator&lt;/td&gt;&lt;td&gt;24759815&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.ResolveTimeZone&lt;/td&gt;&lt;td&gt;24078761&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.Analyzer$ResolvePivot&lt;/td&gt;&lt;td&gt;23264984&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.ResolveInlineTables&lt;/td&gt;&lt;td&gt;22864548&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.execution.datasources.FindDataSourceTable&lt;/td&gt;&lt;td&gt;22127481&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.DecimalPrecision&lt;/td&gt;&lt;td&gt;20855512&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.TypeCoercion$ImplicitTypeCasts&lt;/td&gt;&lt;td&gt;19908820&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.TimeWindowing&lt;/td&gt;&lt;td&gt;17289560&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.TypeCoercion$DateTimeOperations&lt;/td&gt;&lt;td&gt;16691649&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.TypeCoercion$FunctionArgumentConversion&lt;/td&gt;&lt;td&gt;16645812&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.ResolveHints$ResolveBroadcastHints&lt;/td&gt;&lt;td&gt;16391773&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions&lt;/td&gt;&lt;td&gt;16094905&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.TypeCoercion$InConversion&lt;/td&gt;&lt;td&gt;15937875&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.TypeCoercion$PromoteStrings&lt;/td&gt;&lt;td&gt;15659420&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.TypeCoercion$IfCoercion&lt;/td&gt;&lt;td&gt;15131194&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.TypeCoercion$BooleanEquality&lt;/td&gt;&lt;td&gt;15120505&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.TypeCoercion$Division&lt;/td&gt;&lt;td&gt;14657587&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.execution.datasources.PreprocessTableCreation&lt;/td&gt;&lt;td&gt;12421808&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery&lt;/td&gt;&lt;td&gt;12330915&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.UpdateOuterReferences&lt;/td&gt;&lt;td&gt;11919954&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.TypeCoercion$CaseWhenCoercion&lt;/td&gt;&lt;td&gt;11807169&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.EliminateUnions&lt;/td&gt;&lt;td&gt;11761260&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.SubstituteUnresolvedOrdinals&lt;/td&gt;&lt;td&gt;11683297&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.ResolveHints$RemoveAllHints&lt;/td&gt;&lt;td&gt;11363987&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.execution.datasources.DataSourceAnalysis&lt;/td&gt;&lt;td&gt;11253060&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.Analyzer$HandleNullInputsForUDF&lt;/td&gt;&lt;td&gt;11075682&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.execution.datasources.PreprocessTableInsertion&lt;/td&gt;&lt;td&gt;11061610&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.Analyzer$GlobalAggregates&lt;/td&gt;&lt;td&gt;10708386&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.CleanupAliases&lt;/td&gt;&lt;td&gt;9447785&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAliases&lt;/td&gt;&lt;td&gt;4725210&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNewInstance&lt;/td&gt;&lt;td&gt;3634067&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.Analyzer$CTESubstitution&lt;/td&gt;&lt;td&gt;2359406&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveOrdinalInOrderByAndGroupBy&lt;/td&gt;&lt;td&gt;2191643&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.ResolveTableValuedFunctions&lt;/td&gt;&lt;td&gt;2160003&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations&lt;/td&gt;&lt;td&gt;2095181&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions&lt;/td&gt;&lt;td&gt;2029468&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.TypeCoercion$WidenSetOperationTypes&lt;/td&gt;&lt;td&gt;1999994&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.execution.datasources.ResolveSQLOnFile&lt;/td&gt;&lt;td&gt;1891759&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveMissingReferences&lt;/td&gt;&lt;td&gt;1864083&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNaturalAndUsingJoin&lt;/td&gt;&lt;td&gt;1856631&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggAliasInGroupBy&lt;/td&gt;&lt;td&gt;1740242&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveGenerate&lt;/td&gt;&lt;td&gt;1714332&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveUpCast&lt;/td&gt;&lt;td&gt;1686660&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.Analyzer$PullOutNondeterministic&lt;/td&gt;&lt;td&gt;1602061&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.Analyzer$WindowsSubstitution&lt;/td&gt;&lt;td&gt;1406648&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;org.apache.spark.sql.catalyst.analysis.AliasViewChild&lt;/td&gt;&lt;td&gt;1184166&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description><pubDate>Wed, 11 Jul 2018 00:00:00 GMT</pubDate></item><item><title>Quarterly Reading 0x13</title><link>https://manuzhang.github.io/posts/2017-07-29-quarterly-19/index.html</link><description>&lt;p&gt;I was writing this about two weeks ago, but the first section &lt;a href=&quot;http://manuzhang.github.io/2017/07/17/exactly-once-kafka.html&quot;&gt;Exactly-once in Kafka&lt;/a&gt; ended up as a full post. The topic has been keeping its momentum this week,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://the-paper-trail.org/blog/exactly-not-atomic-broadcast-still-impossible-kafka/&quot;&gt;Exactly-once or not, atomic broadcast is still impossible in Kafka – or anywhere&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://data.alishoker.com/2017/07/notes-on-exactly-once-semantics-in.html&quot;&gt;Notes on Exactly-Once semantics in light of Kafka's 0.11 release&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://softwaremill.com/what-kafka-exactly-once-really-means/&quot;&gt;What does Kafka's exactly-once processing really mean?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;but I will refrain myself from talking more since there is a lot to catch up.&lt;/p&gt;
&lt;h3&gt;Releases&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Apache Beam &lt;a href=&quot;https://beam.apache.org/blog/2017/05/17/beam-first-stable-release.html&quot;&gt;published first stable release 2.0.0&lt;/a&gt;. They went up directly from 0.6.0 to join &lt;a href=&quot;https://cloud.google.com/blog/big-data/2017/06/cloud-dataflow-20-sdk-goes-ga&quot;&gt;Cloud Dataflow 2.0 SDK&lt;/a&gt; which is based on Beam 2.0.0.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Apache Storm community &lt;a href=&quot;http://storm.apache.org/2017/07/28/storm104-released.html&quot;&gt;released 1.0.4&lt;/a&gt;, a maintenance release along the 1.0.x line. Note Storm also has a &lt;a href=&quot;http://storm.apache.org/2017/03/29/storm110-released.html&quot;&gt;1.1.x line with Streaming SQL and Kafka 0.10 support&lt;/a&gt;. I guess they try to stick to &lt;a href=&quot;http://semver.org/&quot;&gt;Semantic Versioning&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;MAJOR version when you make incompatible API changes,&lt;/li&gt;
&lt;li&gt;MINOR version when you add functionality in a backwards-compatible manner, and&lt;/li&gt;
&lt;li&gt;PATCH version when you make backwards-compatible bug fixes.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;The downside is now two minor versions have to be maintained for bug fixes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Apache Flink has made the fourth major release &lt;a href=&quot;http://flink.apache.org/news/2017/06/01/release-1.3.0.html&quot;&gt;1.3.0&lt;/a&gt; shortly followed by bug-fix release &lt;a href=&quot;http://flink.apache.org/news/2017/06/23/release-1.3.1.html&quot;&gt;1.3.1&lt;/a&gt;. Some notable new features are incremental checkpointing for RocksDB, side outputs, support for retractions in Table API/SQL. This is also a bug-fix version, &lt;a href=&quot;http://flink.apache.org/news/2017/04/26/release-1.2.1.html&quot;&gt;1.2.1&lt;/a&gt; for 1.2 series. In the future,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Users can expect Flink releases now in a 4 month cycle. At the beginning of the 1.3 release cycle, the community decided to follow a strict &lt;a href=&quot;https://cwiki.apache.org/confluence/display/FLINK/Time-based+releases&quot;&gt;time-based release model&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Our Gearpump has also made a &lt;a href=&quot;https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319920&amp;amp;version=12340225&quot;&gt;bug-fix release, 0.8.4&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Google Cloud Big Data&lt;/h3&gt;
&lt;p&gt;Google Cloud Big Data blog has a series of &lt;strong&gt;After Lambda: Exactly-once processing in Cloud Dataflow&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://cloud.google.com/blog/big-data/2017/05/after-lambda-exactly-once-processing-in-google-cloud-dataflow-part-1&quot;&gt;Part 1 Learning the meaning of &amp;quot;Exactly-once&amp;quot; in Cloud Dataflow, its importance and implementation in Shuffle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://cloud.google.com/blog/big-data/2017/05/after-lambda-exactly-once-processing-in-cloud-dataflow-part-2-ensuring-low-latency&quot;&gt;Part 2 Ensuring low latency with graph optimizations, bloom filters and garbage collection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://cloud.google.com/blog/big-data/2017/07/after-lambda-exactly-once-processing-in-cloud-dataflow-part-3-sources-and-sinks&quot;&gt;Part 3 Implementing sources and sinks for Exactly-once support&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;They also have announced&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://cloud.google.com/blog/big-data/2017/06/introducing-cloud-dataflow-shuffle-for-up-to-5x-performance-improvement-in-data-analytic-pipelines&quot;&gt;Service-based shuffling which brings up to 5x performance improvements&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://cloud.google.com/blog/big-data/2017/07/cloud-dataproc-is-now-even-faster-and-easier-to-use-for-running-apache-spark-and-apache-hadoop&quot;&gt;Cloud Dataproc 1.2&lt;/a&gt;, faster and easier to run Apache Spark (2.2.0) and Apache Hadoop (2.8.0).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Java land&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;With &lt;a href=&quot;https://www.jcp.org/en/jsr/results?id=6016&quot;&gt;Jigsaw finally completed&lt;/a&gt;, Java 9 is around the corner (Sep. 21st). Azul blog has made a &lt;a href=&quot;https://www.azul.com/jdk-9-pitfalls-for-the-unwary/&quot;&gt;summary of required changes when upgrading to Java 9&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Wondering how to take advantage of Single Instruction, Multiple Data(SIMD) in Java ? &lt;a href=&quot;http://prestodb.rocks/code/simd/&quot;&gt;There is no way to use SIMD intrinsics in Java directly, as of Java 8&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Brendan Gregg has made a &lt;a href=&quot;http://www.brendangregg.com/blog/2017-06-30/package-flame-graph.html&quot;&gt;Java Package Flame Graph&lt;/a&gt; to visualize which package has taken up most CPU time.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kotlin is trending as &lt;a href=&quot;https://developer.android.com/kotlin/index.html&quot;&gt;Android makes it an official language&lt;/a&gt;. Why not Scala which also has &lt;a href=&quot;http://scala-android.org/&quot;&gt;Android support&lt;/a&gt; ? &lt;a href=&quot;https://www.reactivesystems.eu/2017/05/21/java-scala-kotlin-skiing.html&quot;&gt;This is how a Scala developer takes it by drawing an analogy to skiing&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That's it but I still have more than 50 articles in my Pocket(read-it-later App). &amp;quot;read-it-later&amp;quot; is actually becoming &amp;quot;never read&amp;quot;. That has made me to start a new experiment, &lt;a href=&quot;https://github.com/manuzhang/read-it-now&quot;&gt;read-it-now&lt;/a&gt;, to improve my reading quality.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Rather than saving an article to Pocket to read it later, I’d like to read it now, take notes and put down my thoughts in a GitHub issue&lt;/p&gt;
&lt;/blockquote&gt;
</description><pubDate>Sat, 29 Jul 2017 00:00:00 GMT</pubDate></item><item><title>Exactly-once in Kafka</title><link>https://manuzhang.github.io/posts/2017-07-17-exactly-once-kafka/index.html</link><description>&lt;p&gt;&lt;a href=&quot;https://www.confluent.io/blog/exactly-once-semantics-are-possible-heres-how-apache-kafka-does-it/&quot;&gt;Exactly-once finally landed in Kafka 0.11&lt;/a&gt; with idempotent producer per partition and atomic writes across multiple partitions. Furthermore,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Building on idempotency and atomicity, exactly-once stream processing is now possible through the Streams API in Apache Kafka&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We also get to know Kafka team has gone through a &lt;a href=&quot;https://goo.gl/fnycgk&quot;&gt;meticulous design&lt;/a&gt;(&amp;gt; 60 pages, man), iterative development process and extensive tests to ensure correctness and low performance overhead of Exactly-once guarantee.&lt;/p&gt;
&lt;p&gt;Lastly, it's warned that Exactly-once is not Magical Pixie Dust you can sprinkle on your application.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Exactly-once processing is an end-to-end guarantee&lt;/strong&gt; and the application has to be designed to not violate the property as well. If you are using the consumer API, this means ensuring that you commit changes to your application state concordant with your offsets as described here.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This has stirred up heated discussions (on &lt;a href=&quot;https://www.reddit.com/r/programming/comments/6kh65f/exactlyonce_semantics_is_possible_heres_how/&quot;&gt;Reddit&lt;/a&gt; and &lt;a href=&quot;https://news.ycombinator.com/item?id=14670801&quot;&gt;HackerNews&lt;/a&gt;) where some people are skeptical whether Exactly-once is (mathematically) possible linking to consensus problems like &lt;a href=&quot;https://en.wikipedia.org/wiki/Two_Generals%27_Problem&quot;&gt;Two General Problem&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Several distributed systems veterans (&lt;a href=&quot;https://twitter.com/StephanEwen/status/882298788827869184&quot;&gt;credit to Stephan Ewen&lt;/a&gt;) have weighed in to sort out the confusion for us.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Kafka co-creator, Jay Kreps &lt;a href=&quot;https://news.ycombinator.com/item?id=14671305&quot;&gt;responded on HackerNews&lt;/a&gt; and &lt;a href=&quot;https://medium.com/@jaykreps/exactly-once-support-in-apache-kafka-55e1fdd0a35f&quot;&gt;explained in depth&lt;/a&gt; how Exactly-once is possible and how Kafka has supported it. (I'd like to get back to some of Jay's words later)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Zookeeper and Bookeeper PMC, Flavio Junqueira argued that there is &lt;a href=&quot;https://fpj.me/2017/07/04/no-consensus-in-exactly-once/&quot;&gt;No consensus in exactly-once&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I have argued here that exactly-once delivery and consensus are not equivalent, and in fact, suggested that it is a weaker problem compared to consensus primarily because it does not require order of delivery.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;and what Exactly-once means in practice.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Exactly-once intuitively means that something happens once and only once. In the context of some current systems, however, the term implies that something that happens multiple times is &lt;strong&gt;effective only once&lt;/strong&gt;. Multiple applications of a transformation or message delivery only affect the state of a given application or system once.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In &lt;a href=&quot;http://bravenewgeek.com/you-cannot-have-exactly-once-delivery-redux/&quot;&gt;You Cannot Have Exactly-Once Delivery Redux&lt;/a&gt;, Tyler Treat reminded us of the difference between &amp;quot;Exactly-once delivery&amp;quot; and &amp;quot;Exactly-once processing&amp;quot;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Delivery” is a transport semantic. “Processing” is an application semantic.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;and the latter is possible in a closed system which Kafka provides.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;To achieve &lt;strong&gt;exactly-once processing semantics&lt;/strong&gt;, we must have a closed system with end-to-end support for modeling input, output, and processor state as a single, atomic operation. Kafka supports this by providing a new transaction API and idempotent producers. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I also looked into the fault tolerance semantics in &lt;a href=&quot;https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/connectors/guarantees.html&quot;&gt;Flink Streaming&lt;/a&gt; and &lt;a href=&quot;http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#fault-tolerance-semantics&quot;&gt;Spark Structured Streaming&lt;/a&gt;, both of which requires coordination of source, sink and the checkpointing/snapshoting mechanism of the system.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Flink Streaming (v1.3)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Flink can guarantee &lt;strong&gt;exactly-once state updates&lt;/strong&gt; to user-defined state only when the source participates in the snapshotting mechanism.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;To guarantee &lt;strong&gt;end-to-end exactly-once record delivery&lt;/strong&gt; (in addition to exactly-once state semantics), the data sink needs to take part in the checkpointing mechanism.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Spark Structured Streaming (v2.2.0)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The engine uses checkpointing and write ahead logs to record the offset range of the data being processed in each trigger. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Together, using replayable sources and idempotent sinks, Structured Streaming can ensure &lt;strong&gt;end-to-end exactly-once semantics&lt;/strong&gt; under any failure&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Exactly-once delivery, Exactly-once processing, Exactly-once semantics or Effectively once, we have seen all usage of these words to mean the same thing. What on earth is Exactly-once ? Jay Kreps said it best,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;the real guarantee people want is the &lt;strong&gt;end-to-end correct processing&lt;/strong&gt; of messages in the presence of failure without having to think hard about the integration with their app.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let's use &amp;quot;end-to-end correct processing&amp;quot; from now on. I like it more what Jay concluded his article with&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Rather than giving up and punting all the hard problems onto the poor person implementing the application we should strive to understand how we redefine the problem space to build correct, fast, and most of all &lt;strong&gt;usable system primitives&lt;/strong&gt; they can build on&lt;/p&gt;
&lt;/blockquote&gt;
</description><pubDate>Mon, 17 Jul 2017 00:00:00 GMT</pubDate></item><item><title>Instrument NameNode metrics with Hadoop Metrics2</title><link>https://manuzhang.github.io/posts/2017-07-10-hadoop-metrics2/index.html</link><description>&lt;p&gt;Recently at work, we need to collect the metrics of users accessing HDFS, e.g. how many times a user has read a file, to help users adjust their storage policies (For more on background, please refer to &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-7343&quot;&gt;HDFS-7343&lt;/a&gt; and &lt;a href=&quot;https://github.com/Intel-bigdata/SSM&quot;&gt;Smart Storage Management&lt;/a&gt;). Unfortunately, that is not available in the &lt;a href=&quot;https://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/Metrics.html&quot;&gt;existing (Hadoop 2.7.3) metrics&lt;/a&gt; which means we have to hack NameNode (who knows it when users access HDFS files) ourselves. Fortunately, we don't have to start from scratch since Hadoop already provides a pluggable metrics framework, &lt;a href=&quot;https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/metrics2/package-summary.html&quot;&gt;Hadoop Metrics2&lt;/a&gt;,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The framework provides a variety of ways to implement metrics instrumentation easily via the simple MetricsSource interface or the even simpler and more concise and declarative metrics annotations. The consumers of metrics just need to implement the simple MetricsSink interface. Producers register the metrics sources with a metrics system, while consumers register the sinks. A default metrics system is provided to marshal metrics from sources to sinks based on (per source/sink) configuration options...&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Specifically for our requirement, we need to&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Implement a &lt;code class='language-text'&gt;MetricsSource&lt;/code&gt; inside NameNode that record users accessing file.&lt;/li&gt;
&lt;li&gt;Implement a &lt;code class='language-text'&gt;MetricsSink&lt;/code&gt; that write the metrics somewhere (e.g. HDFS).&lt;/li&gt;
&lt;li&gt;Finally, hook them together.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The default metrics system is a singleton so that we have to add our &lt;code class='language-text'&gt;MetricsSource&lt;/code&gt; into the existing &lt;code class='language-text'&gt;NameNodeMetrics&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here is the big picture of the metrics flow. The blue parts are already provided while the red parts should be implemented.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://goo.gl/v9MuvS&quot; alt=&quot;hadoop_metrics2&quot; /&gt;{:height=&amp;quot;200px&amp;quot; width=&amp;quot;650px&amp;quot;}&lt;/p&gt;
&lt;h2&gt;MetricsSource&lt;/h2&gt;
&lt;p&gt;As said in the doc, there are two ways to write a &lt;code class='language-text'&gt;MetricsSource&lt;/code&gt;. The simpler and more limited one is &lt;code class='language-text'&gt;@Metrics&lt;/code&gt; annotation.&lt;/p&gt;
&lt;h3&gt;&lt;code class='language-text'&gt;@Metrics&lt;/code&gt; annotation&lt;/h3&gt;
&lt;p&gt;For example, the &lt;a href=&quot;https://github.com/apache/hadoop/blob/branch-2.7.3/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeMetrics.java&quot;&gt;NameNodeMetrics&lt;/a&gt;, where&lt;code class='language-text'&gt;@Metric&lt;/code&gt; is used to indict a metrics source.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;@Metrics(name=&amp;quot;NameNodeActivity&amp;quot;, about=&amp;quot;NameNode metrics&amp;quot;, context=&amp;quot;dfs&amp;quot;)
public class NameNodeMetrics {
  ...
  @Metric MutableCounterLong createFileOps;
  @Metric MutableCounterLong filesCreated;
  @Metric MutableCounterLong filesAppended;
  
  FileAccessMetrics fileAccessMetrics;
  ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The limitation is the class must have at least one &lt;code class='language-text'&gt;@Metric&lt;/code&gt; field, whose class has to extend &lt;code class='language-text'&gt;MutableMetric&lt;/code&gt;.  Other than that, we are free to add any &lt;code class='language-text'&gt;MetricsSource&lt;/code&gt;, for instance, the &lt;code class='language-text'&gt;FileAccessMetrics&lt;/code&gt; we are going to implement.&lt;/p&gt;
&lt;h3&gt;Implementing &lt;code class='language-text'&gt;MetricsSource&lt;/code&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;public class FileAccessMetrics implements MetricsSource {
  public static final String NAME = &amp;quot;FileAccessMetrics&amp;quot;;
  public static final String DESC = &amp;quot;FileAccessMetrics&amp;quot;;
  public static final String CONTEXT_VALUE =&amp;quot;file_access&amp;quot;;

  private List&amp;lt;Info&amp;gt; infos = new LinkedList&amp;lt;&amp;gt;();

  public static FileAccessMetrics create(MetricsSystem ms) {
    return ms.register(NAME, DESC, new FileAccessMetrics());
 }

  public synchronized void addMetrics(String path, String user, long time) {
    infos.add(new Info(path, user, time));
  }

  @Override
  public void getMetrics(MetricsCollector collector, boolean all) {
    for (Info info: infos) {
      MetricsRecordBuilder rb = collector.addRecord(info).setContext(CONTEXT_VALUE);
      rb.addGauge(info, 1);
    }
    infos.clear();
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Distilling it,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code class='language-text'&gt;NameNodeMetrics&lt;/code&gt; will &lt;code class='language-text'&gt;create&lt;/code&gt; this &lt;code class='language-text'&gt;FileAccessMetrics&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Whenever &lt;code class='language-text'&gt;DFSClient&lt;/code&gt; opens a read call, &lt;code class='language-text'&gt;NameNode&lt;/code&gt; will &lt;code class='language-text'&gt;addMetrics(Info(path, user, time)&lt;/code&gt; to the list.&lt;/li&gt;
&lt;li&gt;The &lt;code class='language-text'&gt;MetricsSystem&lt;/code&gt; will periodically &lt;code class='language-text'&gt;getMetrics&lt;/code&gt; from the list and put onto its internal queue.&lt;/li&gt;
&lt;li&gt;The &lt;code class='language-text'&gt;MetricsRecordBuilder&lt;/code&gt; expect a numerical value so we do a small trick by storing the &lt;code class='language-text'&gt;Info&lt;/code&gt; into the record name and setting the value to &lt;code class='language-text'&gt;1&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The &lt;code class='language-text'&gt;CONTEXT_VALUE&lt;/code&gt; will be used later to identify the record for write.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;MetricsSink&lt;/h2&gt;
&lt;p&gt;Periodically, &lt;code class='language-text'&gt;MetricsSystem&lt;/code&gt; will poll its internal queue
and &lt;code class='language-text'&gt;putMetrics&lt;/code&gt; to &lt;code class='language-text'&gt;MetricsSink&lt;/code&gt;, which can write it out as  follows.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;public class FileAccessMetrics implements MetricsSource {
  ...
  public static class Writer implements MetricsSink, Closeable {
    ...
    @Override
    public void putMetrics(MetricsRecord record) {
      ...
      for (AbstractMetric metric : record.metrics()) {
        currentOutStream.print(metric.name() + &amp;quot;:&amp;quot; + metric.value());
      }
      ...
    }
    ...
  } 
  ...
}  
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that Hadoop 3.x already packs a bunch of useful sinks&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RollingFileSystemSink (Hadoop FileSystem)&lt;/li&gt;
&lt;li&gt;KafkaSink&lt;/li&gt;
&lt;li&gt;GraphiteSink&lt;/li&gt;
&lt;li&gt;StatsDSink&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Now, hook them together&lt;/h2&gt;
&lt;p&gt;Put the following configurations into either &lt;code class='language-text'&gt;hadoop-metrics2-namenode.properties&lt;/code&gt; or &lt;code class='language-text'&gt;hadoop-metrics2.properties&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;namenode.sink.file_access.class=org.apache.hadoop.hdfs.server.namenode.metrics.FileAccessMetrics$Writer
namenode.sink.file_access.context=file_access
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code class='language-text'&gt;namenode&lt;/code&gt; is the prefix with which &lt;code class='language-text'&gt;NameNodeMetrics&lt;/code&gt; initialize the metrics system.&lt;/li&gt;
&lt;li&gt;The metrics system will firstly try to load &lt;code class='language-text'&gt;hadoop-metrics2-[prefix].properties&lt;/code&gt; and fall back to &lt;code class='language-text'&gt;hadoop-metrics2.properties&lt;/code&gt; if not found.&lt;/li&gt;
&lt;li&gt;The context value is exactly &lt;code class='language-text'&gt;FileAccessMetrics$CONTEXT_VALUE&lt;/code&gt; such that &lt;code class='language-text'&gt;MetricsSystem&lt;/code&gt; are able to filter out other &lt;code class='language-text'&gt;NameNodeMetrics&lt;/code&gt; sources and only send &lt;code class='language-text'&gt;FileAccessMetrics&lt;/code&gt; to our sink.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;This post describes the architecture and usage of Hadoop metrics2 through an example to instrument user accessing HDFS files. However, I cannot cover all the features since I haven't tried them out myself so please refer to the &lt;a href=&quot;http://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/metrics2/package-summary.html&quot;&gt;official documentation&lt;/a&gt;.&lt;/p&gt;
</description><pubDate>Mon, 10 Jul 2017 00:00:00 GMT</pubDate></item><item><title>Quarterly Reading 0x12</title><link>https://manuzhang.github.io/posts/2017-05-01-quarterly-18/index.html</link><description>&lt;p&gt;The weekly reading has been held off since Feb 27th, bypassing Biweekly, Monthly and finally to Quarterly. My Pocket list has been piling up (125 read-it-later) and I can no longer pretend the world doesn't change. It is revolving super fast. Unlike previous readings where we dived into technique details, we will look more from outside this time.&lt;/p&gt;
&lt;h3&gt;Acquisition&lt;/h3&gt;
&lt;p&gt;While the big fish looks to fill in the missing part of its software stacks, the small fish seeks for sustainably growth and profit.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.mozilla.org/blog/2017/02/27/mozilla-acquires-pocket/&quot;&gt;Mozilla Acquires Pocket&lt;/a&gt;. Pocket started out as a Firefox add-on and finally has become part of it. Let's see how the integration could go deeper after the acquisition.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://cloudplatform.googleblog.com/2017/03/welcome-Kaggle-to-Google-Cloud.html&quot;&gt;Kaggle joins Google Cloud&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Founded in 2010, Kaggle is home to the world's largest community of data scientists and machine learning enthusiasts.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This could make deployment of machine learning algorithms much easier.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://blog.gitter.im/2017/03/15/gitter-gitlab-acquisition/&quot;&gt;Gitter has been acquired by GitLab&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Next piece of wow: we will be open sourcing all of the Gitter.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Gitter will be like GitLab, which allows you to setup your own GitHub.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Business story&lt;/h3&gt;
&lt;p&gt;Another topic I rarely mention but is worth bringing up.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://producthabits.com/why-trello-failed-to-build-a-1-billion-business/&quot;&gt;Why Trello Failed to Build a $1 Billion+ Business&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Actually Trello has more than 19M users and &lt;a href=&quot;https://techcrunch.com/2017/01/09/atlassian-acquires-trello/&quot;&gt;was sold to Atlassian for $425M&lt;/a&gt;, but it fell short in monetization and its main features were easily copy-catted.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://stratechery.com/2017/intel-mobileye-and-smiling-curves/&quot;&gt;Intel, Mobileye, and Smiling Curves&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A mind-blown post on &lt;a href=&quot;https://www.wsj.com/articles/intel-to-buy-mobileye-for-15-3-billion-1489404970&quot;&gt;Intel's purchase of Mobileye&lt;/a&gt;. The &lt;a href=&quot;https://en.wikipedia.org/wiki/Smiling_curve&quot;&gt;smiling curve&lt;/a&gt; is introduced to explain the value added by different players in the industry of PCs, Phones, Servers and future Cars, and thus the context of this purchase.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Releases&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319920&amp;amp;version=12338681&quot;&gt;Apache Gearpump(incubating)&lt;/a&gt; just released 0.8.3 with support for window DSL and finite stream.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12339256&quot;&gt;Apache Beam 0.6.0&lt;/a&gt; is available featuring, &lt;a href=&quot;https://beam.apache.org/blog/2017/03/16/python-sdk-release.html&quot;&gt;Python SDK&lt;/a&gt;, &lt;a href=&quot;https://www.slideshare.net/FlinkForward/flink-forward-sf-2017-kenneth-knowles-back-to-sessions-overview&quot;&gt;State API and Timer API support&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://storm.apache.org/2017/03/29/storm110-released.html&quot;&gt;Storm reached a major of milestone, releasing 1.1.0&lt;/a&gt;. Streaming SQL, PMML support, Kafka 0.10 / Druid / OpenTSDB and AWS Kinesis and HDFS support are all remarkable.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://flink.apache.org/news/2017/03/23/release-1.1.5.html&quot;&gt;Flink rolled out 1.1.5, a bugfix version of 1.1.0 series&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Road Ahead&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://www.bradfordcross.com/blog/2017/3/3/five-ai-startup-predictions-for-2017&quot;&gt;Five AI Startup Predictions for 2017&lt;/a&gt; by Bradford Cross, who has been working with AI for nearly 20 years, and building silicon valley AI startups for nearly 10 (which leads to the fifth point).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Bots go bust.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;the current mania around ‘bots’ defined as conversational interfaces over voice and chat will begin its collapse in 2017&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Deep learning goes commodity&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I am suggesting that deep learning will become more commodity among machine learning people this year, but i am not suggesting that machine learning itself will become commodity.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;AI is Cleantech 2.0 for VCs&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;the batch that are diving in at the top of this pre-mania are making the same mistake that cleantechs did -- they are diving into AI instead of diving into a customer need.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MLaaS dies a second death&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;the people that know what they’re doing just use open source, and the people that don’t will not get anything to work, ever, even with APIs&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Full stack vertical AI startups actually work&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Vertical AI startups solve full-stack industry problems that require subject matter expertise, unique data, and a product that uses AI to deliver its core value proposition.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description><pubDate>Mon, 1 May 2017 00:00:00 GMT</pubDate></item><item><title>Flink Forward SF 2017 Readings</title><link>https://manuzhang.github.io/posts/2017-04-27-flink-forward-sf-2017/index.html</link><description>&lt;p&gt;&lt;a href=&quot;http://sf.flink-forward.org/&quot;&gt;Flink Forward&lt;/a&gt;, &amp;quot;The premier conference on Apache Flink®&amp;quot;, just took place in San Francisco. All the &lt;a href=&quot;https://www.slideshare.net/FlinkForward&quot;&gt;slides&lt;/a&gt; and &lt;a href=&quot;https://www.youtube.com/playlist?list=PLDX4T_cnKjD2UC6wJr_wRbIvtlMtkc-n2&quot;&gt;videos&lt;/a&gt; are available now. The conference was both abundant in practical experiences and technical details. After going through all the slides, I'd like to share some interesting contents that you can't find on &lt;a href=&quot;http://flink.apache.org/&quot;&gt;http://flink.apache.org/&lt;/a&gt;. (I wasn't there and neither have I watched all the videos so take my readings with a grain of salt)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Update: data Artisans has published an official &lt;a href=&quot;https://data-artisans.com/blog/stream-processing-flink-forward-sf-2017-recap&quot;&gt;recap: On the State of Stream Processing with Apache Flink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;TensorFlow with Flink&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;-&amp;quot;What is so hot ?&amp;quot;&lt;br /&gt;
-&amp;quot;Deep learning&amp;quot;&lt;/p&gt;
&lt;p&gt;-&amp;quot;What is so hot in deep learning ?&amp;quot;&lt;br /&gt;
-&amp;quot;TensorFlow&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://www.slideshare.net/FlinkForward/flink-forward-sf-2017-eron-wright-introducing-flink-tensorflow&quot;&gt;TensorFlow &amp;amp; Apache Flink&lt;/a&gt; immediately caught my eye. The basic idea is &amp;quot;TF Graph as a Flink map function&amp;quot; for inference after preprocessing data to off-heap tensor. Online learning is a future direction and the project is &lt;a href=&quot;https://github.com/cookieai/flink-tensorflow/&quot;&gt;open sourced on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Deep Learning with Flink&lt;/h3&gt;
&lt;p&gt;Lightbend's Dean Wampler &lt;a href=&quot;https://www.slideshare.net/FlinkForward/flink-forward-sf-2017-dean-wampler-streaming-deep-learning-scenarios-with-flink&quot;&gt;discussed about how to do Deep Learning with Flink generally&lt;/a&gt;, from the challenges in (mini-batch / distributed / online) training and inference to practical approaches, leveraging such Flink features as side inputs and async I/O. He also recommended &amp;quot;Do&amp;quot;s and &amp;quot;Don't&amp;quot;s for the road ahead. One interesting &amp;quot;Don't&amp;quot; is &lt;a href=&quot;https://en.wikipedia.org/wiki/Predictive_Model_Markup_Language&quot;&gt;PMML&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;PMML - doesn't work well enough. Not really that useful ?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;PMML - not useful ?&lt;/h3&gt;
&lt;p&gt;At least &lt;a href=&quot;https://en.wikipedia.org/wiki/Predictive_Model_Markup_Language&quot;&gt;ING uses PMML models to bridge the offline training with Spark and online scoring with Flink streaming&lt;/a&gt;. The striking part is how they've decoupled &amp;quot;What&amp;quot; (DAG) and &amp;quot;How&amp;quot; (behavior of each node on the DAG) in the scoring application. Model definition is added at runtime through a broadcast stream without downtime. The same applies for data persist and feature extraction.&lt;/p&gt;
&lt;p&gt;By the way, &lt;a href=&quot;http://storm.apache.org/2017/03/29/storm110-released.html&quot;&gt;Apache Storm has added PMML support in 1.1.0&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;It's the data&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;-&amp;quot;Which machine learning (ML) framework is the best ?&amp;quot;
-&amp;quot;All of them&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That's Ted Dunning's answer after learning that his customers typically use 12 ML packages and the smallest number is 5. That's why he didn't talk about Flink ML in &lt;a href=&quot;https://www.slideshare.net/FlinkForward/flink-forward-sf-2017-ted-dunning-nonflink-machine-learning-on-flink&quot;&gt;Machine Learning on Flink&lt;/a&gt;. Even  ML is not the key here.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;90%+ of effort is logistics, not learning.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is the data. Record raw data, use streams to keep data around, and measure and understand (with meta-data) everything. Another thing is to make deployment easier with containerization.&lt;/p&gt;
&lt;p&gt;One more lesson for me is there is no such thing as one model.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You will have dozens of models, likely hundreds to thousands.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Blink Improvements&lt;/h3&gt;
&lt;p&gt;Blink is Alibaba's fork of Flink. For large scale streaming (&amp;gt; 1000 nodes) at Alibaba, Blink added a bunch of &lt;a href=&quot;https://www.slideshare.net/FlinkForward/flink-forward-sf-2017-feng-wang-zhijiang-wang-runtime-improvements-in-blink-for-large-scale-streaming-at-alibaba&quot;&gt;runtime improvements&lt;/a&gt;. (the right side of &amp;quot;=&amp;gt;&amp;quot; is the problem to solve)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Native integration with resource management (YARN) =&amp;gt; single JobManager for all tasks&lt;/li&gt;
&lt;li&gt;Incremental checkpoint =&amp;gt; large state&lt;/li&gt;
&lt;li&gt;Asynchronous operator =&amp;gt; blocking I/O&lt;/li&gt;
&lt;li&gt;Fine-grained recovery from task failures =&amp;gt; application restart on one task failure&lt;/li&gt;
&lt;li&gt;Allocation reuse for task recovery =&amp;gt; expensive to restore from HDFS&lt;/li&gt;
&lt;li&gt;Non-disruptive JobManager failures via reconciliation =&amp;gt; tasks restart on JobManager failure&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What's cool is the improvements are being contributed back to the community.&lt;/p&gt;
&lt;h3&gt;SQL as building block&lt;/h3&gt;
&lt;p&gt;Uber shared the &lt;a href=&quot;https://www.slideshare.net/FlinkForward/flink-forward-sf-2017-chinmay-soman-real-time-analytics-in-the-real-world-challenges-and-lessons-at-uber&quot;&gt;evolution of their business needs and their system evolved accordingly&lt;/a&gt; from event processing, to OLAP, to Streaming SQL on Flink.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;70-80% of jobs can be implemented via SQL.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For more on Flink's SQL API, check out &lt;a href=&quot;https://www.slideshare.net/FlinkForward/flink-forward-sf-2017-timo-walther-table-sql-api-unified-apis-for-batch-and-stream-processing&quot;&gt;Table &amp;amp; SQL API – unified APIs for batch and stream processing&lt;/a&gt; and &lt;a href=&quot;https://www.slideshare.net/FlinkForward/flink-forward-sf-2017-shaoxuan-wangxiaowei-jiang-blinks-improvements-to-flink-sql-and-tableapi&quot;&gt;Blink's Improvements to Flink SQL And TableAPI&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;To Beam or not to Beam&lt;/h3&gt;
&lt;p&gt;This is one of Uber's future discussions.   From the &lt;a href=&quot;https://beam.apache.org/&quot;&gt;official site&lt;/a&gt;,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Apache Beam provides an advanced unified programming model, allowing you to implement batch and streaming data processing jobs that can run on any execution engine.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Beam has recently added &lt;a href=&quot;https://www.slideshare.net/FlinkForward/flink-forward-sf-2017-kenneth-knowles-back-to-sessions-overview&quot;&gt;State and Timer support to unlock new use cases&lt;/a&gt; which are portable across runners (e.g. Flink).&lt;/p&gt;
&lt;h3&gt;What is Streaming ?&lt;/h3&gt;
&lt;p&gt;I'd like to wrap up with Stephen Ewen's &lt;a href=&quot;https://www.slideshare.net/FlinkForward/flink-forward-sf-2017-stephan-ewen-convergence-of-realtime-analytics-and-datadriven-applications&quot;&gt;answer and high level view of Streaming&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;2016 was the year when streaming technologies became mainstream&lt;/p&gt;
&lt;p&gt;2017 is the year to realize the full spectrum of streaming applications&lt;/p&gt;
&lt;/blockquote&gt;
</description><pubDate>Thu, 27 Apr 2017 00:00:00 GMT</pubDate></item><item><title>Shade with SBT III</title><link>https://manuzhang.github.io/posts/2017-04-21-shading-3/index.html</link><description>&lt;p&gt;In the last part of &lt;a href=&quot;http://manuzhang.github.io/2016/11/12/shading-2.html&quot;&gt;Shade with SBT II&lt;/a&gt;,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;One one more thing is &lt;code class='language-text'&gt;java.lang.VerifyError&lt;/code&gt; exceptions are thrown by JVM after the change. The work-around is adding &lt;code class='language-text'&gt;-noverify&lt;/code&gt; in the JVM options. I haven't yet found the root cause and the issue is logged at &lt;a href=&quot;https://issues.apache.org/jira/browse/GEARPUMP-236&quot;&gt;GEARPUMP-236&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;I have a feeling that there will be a third episode of our story with sbt shade.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now, here is the third episode. (The issue was resolved two months ago but my procrastination has been getting worse these days)&lt;/p&gt;
&lt;p&gt;My colleague &lt;a href=&quot;https://github.com/karol-brejna-i&quot;&gt;Karol&lt;/a&gt; found that the &lt;code class='language-text'&gt;java.lang.VerifyError&lt;/code&gt; went away when forcing usage of asm 5.1. Thanks to his thorough investigation, I can just post his &lt;a href=&quot;https://issues.apache.org/jira/browse/GEARPUMP-236?focusedCommentId=15866352&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15866352&quot;&gt;findings&lt;/a&gt; here.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As stated here: &lt;a href=&quot;https://github.com/sbt/sbt-assembly/issues/205#issuecomment-278287602&quot;&gt;https://github.com/sbt/sbt-assembly/issues/205#issuecomment-278287602&lt;/a&gt; there is probably a problem with shading.&lt;/p&gt;
&lt;p&gt;In our tests the plugin regenerated the classes that shouldn't be impacted by shading. Moreover, the outcome of the operation made the classes not validated by JVM.&lt;/p&gt;
&lt;p&gt;sbt-assembly we use (0.14.3) depends on jarjar(1.6.2)
jarjar seems to be based on org.ow2.asm 5.0.4 (&lt;a href=&quot;https://github.com/pantsbuild/jarjar/blob/master/lib/BUILD&quot;&gt;https://github.com/pantsbuild/jarjar/blob/master/lib/BUILD&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;It seems like the bug is related to asm lib.
The experiments show that after upgrading asm to 5.1.x&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This was also &lt;a href=&quot;https://github.com/sbt/sbt-assembly/issues/205#issuecomment-279967557&quot;&gt;confirmed by another user of sbt-assembly&lt;/a&gt; and fixed in &lt;a href=&quot;https://github.com/sbt/sbt-assembly/releases/tag/v0.14.4&quot;&gt;sbt-assembly 0.14.4&lt;/a&gt;, and followed by asm upgrade in other downstream projects.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/pantsbuild/jarjar/issues/26&quot;&gt;Update ASM dependency pantsbuild/jarjar#26&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/apache/beam/issues/2014&quot;&gt;BEAM-1492 Upgrade bytebuddy to 1.6.8 to jump past asm 5.0&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ok, should this be the final episode ? I'm afraid not. Remember there is still an open question, &lt;a href=&quot;http://stackoverflow.com/questions/40526420/sbt-published-maven-file-missing-artifacts-with-multiple-scopes&quot;&gt;sbt published maven file missing artifacts with multiple scopes&lt;/a&gt;.&lt;/p&gt;
</description><pubDate>Fri, 21 Apr 2017 00:00:00 GMT</pubDate></item><item><title>Weekly Reading 0x11</title><link>https://manuzhang.github.io/posts/2017-02-27-weekly-17/index.html</link><description>&lt;p&gt;Last week, we mentioned &lt;a href=&quot;https://cloudplatform.googleblog.com/2017/02/introducing-Cloud-Spanner-a-global-database-service-for-mission-critical-applications.html&quot;&gt;announcement of Google Cloud Spanner&lt;/a&gt;. There is an &amp;quot;open source version&amp;quot;, &lt;a href=&quot;https://github.com/cockroachdb/cockroach&quot;&gt;CockroachDB&lt;/a&gt;, built by three ex-Googlers who also founded a company around it. We would read about how &lt;a href=&quot;https://www.nextplatform.com/2017/02/22/google-spanner-inspires-cockroachdb-outrun/&quot;&gt;Google Spanner Inspires CockroachDB To Outrun It&lt;/a&gt; this week.&lt;/p&gt;
&lt;p&gt;Without Google's private network and atomic clock, CockroachDB provides serializer isolation, weaker than Spanner's linearizable isolation. CockRoachDB is based on &lt;a href=&quot;http://rocksdb.org/&quot;&gt;RocksDB&lt;/a&gt; using &lt;a href=&quot;https://github.com/coreos/etcd&quot;&gt;etcd&lt;/a&gt;(&lt;a href=&quot;https://raft.github.io/raft.pdf&quot;&gt;Raft&lt;/a&gt;) for consensus. It has compatible interface with Postgres, supporting most SQL 92, some SQL 2011 and Spanner functions. They've employed &lt;a href=&quot;http://jepsen.io/analyses/cockroachdb-beta-20160829&quot;&gt;jepsen for consistency analysis&lt;/a&gt; and found bugs leading to serializability violations. The software is still in beta at the time of writing. Let's see how it will turn out.&lt;/p&gt;
&lt;p&gt;Now, what else ?&lt;/p&gt;
&lt;h3&gt;Releases&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://blogs.apache.org/samza/entry/announcing-the-release-of-apache&quot;&gt;Apache Samza 0.12.0 has been released&lt;/a&gt; with the support of both unbounded and bounded sources, or &amp;quot;Convergence of Batch and Real-time processing&amp;quot; as said.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://archive.apache.org/dist/kafka/0.10.2.0/RELEASE_NOTES.html&quot;&gt;Apache Kafka has made a feature release 0.10.2.0&lt;/a&gt; which includes the completion of 15 KIPs, over 200 bug fixes and improvements. Among them are supports for session windows and global table in Kafka Streams.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Docker&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We &lt;a href=&quot;http://manuzhang.github.io/2016/01/28/unikernel.html&quot;&gt;talked about Unikernel&lt;/a&gt; one year ago, which can only be run on Xen hypervisor then. Now &lt;a href=&quot;http://www.eweek.com/cloud/mirageos-unikernel-effort-moves-forward.html&quot;&gt;the Unikernel operating system MirageOS supports the KVM hypervisor&lt;/a&gt;. MirageOS components have been used to &lt;a href=&quot;http://www.eweek.com/virtualization/docker-goes-native-for-windows-and-mac.html&quot;&gt;enable native Docker on macOS and Windows&lt;/a&gt; instead of running on VirtualBox.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Google&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Sharing our experiences with Virtual Reality(VR) can't be difficult, and that's where &lt;a href=&quot;https://www.youtube.com/watch?v=lAFpA1AGs4E&quot;&gt;Mixed Reality&lt;/a&gt;(MR) comes in. With MR, we can see the virtual environment of a VR user except the facial expressions blocked by the headset. Now Google researchers have been &lt;a href=&quot;https://research.googleblog.com/2017/02/headset-removal-for-virtual-and-mixed.html&quot;&gt;working on a solution to &amp;quot;remove&amp;quot; the headset&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Preprocessing is required to apply machine learning to real world datasets, which can be done &lt;em&gt;separately&lt;/em&gt; through a large scale data processing framework such as &lt;a href=&quot;http://spark.apache.org/&quot;&gt;Apache Spark&lt;/a&gt; and &lt;a href=&quot;http://flink.apache.org/&quot;&gt;Apache Flink&lt;/a&gt;. Now Google &lt;a href=&quot;https://research.googleblog.com/2017/02/preprocessing-for-machine-learning-with.html&quot;&gt;announced tf.Transform to make it part of a TensorFlow graph&lt;/a&gt;. That is achieved through &lt;a href=&quot;https://beam.apache.org/&quot;&gt;Apache Beam&lt;/a&gt;  whose applications can be run on Spark, Flink and Google's own &lt;a href=&quot;https://cloud.google.com/dataflow&quot;&gt;Cloud Dataflow&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A piece of &amp;quot;old&amp;quot; news from Google, &lt;a href=&quot;https://research.googleblog.com/2017/02/announcing-tensorflow-fold-deep.html&quot;&gt;Announcing TensorFlow Fold: Deep Learning With Dynamic Computation Graphs&lt;/a&gt;. TensorFlow Fold is to address challenge that data of varying size and structure don't batch together. You may read their &lt;a href=&quot;https://arxiv.org/abs/1702.02181&quot;&gt;paper&lt;/a&gt; for more details.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Security&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Google &lt;a href=&quot;https://security.googleblog.com/2017/02/announcing-first-sha1-collision.html&quot;&gt;has found the first SHA1 collision&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The attacker could then use this collision to deceive systems that rely on hashes into accepting a malicious file in place of its benign counterpart. For example, two insurance contracts with drastically different terms.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here is [an interesting site showing lifetimes of cryptographic hash functions](cryptographic hash functions).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The other breaking news is &lt;a href=&quot;https://medium.com/@octal/cloudbleed-how-to-deal-with-it-150e907fd165#.qn9jsos51&quot;&gt;CloudBleed&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Essentially, web requests to Cloudflare-backed sites received answers which included random information from other Cloudflare-backed sites!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here is &lt;a href=&quot;https://github.com/pirate/sites-using-cloudflare&quot;&gt;a list of possibly affected domains&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dropbox have &lt;a href=&quot;https://blogs.dropbox.com/tech/2017/02/meet-securitybot-open-sourcing-automated-security-at-scale/&quot;&gt;open sourced Securitybot&lt;/a&gt;, to &amp;quot;automatically confirm and aggregate suspicious behavior with employees on a distributed scale&amp;quot;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That's all for the week. Super happy to keep up the momentum.&lt;/p&gt;
</description><pubDate>Mon, 27 Feb 2017 00:00:00 GMT</pubDate></item><item><title>Biweekly Reading 0x10</title><link>https://manuzhang.github.io/posts/2017-02-20-biweekly-16/index.html</link><description>&lt;p&gt;It's 2017 and I'm still struggling with my procrastination. The &lt;a href=&quot;http://manuzhang.github.io/2016/12/11/triweekly-15.html&quot;&gt;last reading&lt;/a&gt; was more than a month ago and articles have been piling up in my Pocket list. Not to overburden and scare off myself from this routine, I'd like to just do a biweekly reading to start again. Another reason is the value of news decay quickly as the value of data. I'll start with open source releases.&lt;/p&gt;
&lt;h3&gt;Releases&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://cloudplatform.googleblog.com/2017/02/introducing-Cloud-Spanner-a-global-database-service-for-mission-critical-applications.html&quot;&gt;Google announced the public beta for Cloud Spanner: a global database service for mission-critical applications&lt;/a&gt;. People have been discussing whether it beats the CAP theorem. Eric Brewer, the author of CAP and VP of Google Cloud &lt;a href=&quot;https://cloudplatform.googleblog.com/2017/02/inside-Cloud-Spanner-and-the-CAP-Theorem.html&quot;&gt;shared his idea&lt;/a&gt;. Spanner is technically a CP system while providing more than five 9s of available in practice. It is achieved through Google's private network.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Google controls the entire network and thus can ensure redundancy of hardware and paths, and can also control upgrades and operations in general.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://flink.apache.org/news/2017/02/06/release-1.2.0.html&quot;&gt;Apache Flink made 1.2.0 major release&lt;/a&gt; with a bunch of new features&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;supports changing the parallelism of a streaming job by restoring it from a savepoint with a different parallelism.&lt;/li&gt;
&lt;li&gt;redistribution of kafka partitions and offsets among consumers&lt;/li&gt;
&lt;li&gt;async I/O operator&lt;/li&gt;
&lt;li&gt;queryable state&lt;/li&gt;
&lt;li&gt;allows user to restart from a 1.1.4 savepoint&lt;/li&gt;
&lt;li&gt;enhanced Table API &amp;amp; SQL (e.g. window aggregations over streaming tables)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://storm.apache.org/2017/02/14/storm103-released.html&quot;&gt;Apache Storm released 1.0.3&lt;/a&gt; as a maintenance release, to be followed by 1.1.0 soon.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://akka.io/news/2017/02/10/akka-2.4.17-released.html&quot;&gt;Akka 2.4.17 released with security patch&lt;/a&gt; for a potential security issue with Java deserialization.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://databricks.com/blog/2017/02/09/intels-bigdl-databricks.html&quot;&gt;Databricks blog gives highlights and tutorial&lt;/a&gt; of Intel's recently released &lt;a href=&quot;https://github.com/intel-analytics/BigDL&quot;&gt;BigDL&lt;/a&gt; project. BigDL will sit on the same level as Structured Streaming, MLLib and GraphX in the Spark Stack.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2017/02/accelerating-apache-spark-mllib-with-intel-math-kernel-library-intel-mkl/&quot;&gt;Intel and Cloudera have collaborated to speed up Spark’s ML algorithms, via integration with Intel’s Math Kernel Library&lt;/a&gt;. Benchmark results show performance boost against JVM based execution and &lt;a href=&quot;http://www.openblas.net/&quot;&gt;OpenBLAS&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Conferences&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://code.facebook.com/posts/1692857177682119/machine-learning-scale-2017-recap&quot;&gt;Recap of Machine Learning @Scale 2017&lt;/a&gt; is posted on &lt;a href=&quot;https://code.facebook.com/&quot;&gt;Facebook Code blog&lt;/a&gt;. For those not familiar with this conference like me&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Machine Learning @Scale is an invitation-only technical conference for data scientists, engineers and researchers working on large-scale applied machine learning solutions&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://databricks.com/blog/2017/02/09/spark-summit-east-2017-another-record-setting-spark-summit.html&quot;&gt;Keynotes and highlights&lt;/a&gt; from Databricks' speakers at &lt;a href=&quot;https://spark-summit.org/east-2017/schedule/&quot;&gt;Spark Summit East 2017&lt;/a&gt; (with videos and slides) are available on their official blog. For users blocked from YouTube, check out &lt;a href=&quot;https://pan.baidu.com/s/1jHD7yey&quot;&gt;this link&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Industry&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://www.businesswire.com/news/home/20170201005431/en/Confluent-Grows-Subscriptions-700-Percent-2016-Businesses&quot;&gt;Confluent Grows Subscriptions by Over 700 Percent in 2016 as Businesses Seize the Power of Real-Time Data&lt;/a&gt;. Confluent Enterprise product is built around Apache Kafka™, including &lt;a href=&quot;https://www.confluent.io/product/control-center/&quot;&gt;Control Center&lt;/a&gt; to manage Kafka at scale, &lt;a href=&quot;https://www.confluent.io/product/connectors/&quot;&gt;Kafka Connector API&lt;/a&gt; to connect with other systems and &lt;a href=&quot;https://www.confluent.io/product/kafka-streams/&quot;&gt;Kafka Streams API&lt;/a&gt; for lightweight stream processing. Kafka in Big Data is similar to pipe &lt;code class='language-text'&gt;|&lt;/code&gt; in Unix.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://hortonworks.com/press-releases/hortonworks-reports-record-2016-revenue-184-5-million-fourth-quarter-revenue-52-0-million/&quot;&gt;Hortonworks reports record 2016 revenue of 184.5 million and 52.0 fourth quarter revenue&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.evernote.com/blog/2017/02/08/evernote-reaches-the-cloud&quot;&gt;Evernote migrates to Google Gloud Platform&lt;/a&gt; with 5 billion notes and 5 billion attachments, or over 3 petabytes data.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Programming&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://www.lihaoyi.com/post/WhatsFunctionalProgrammingAllAbout.html&quot;&gt;What's Functional Programming All About?&lt;/a&gt; Li haoyi gives his answer as&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The core of Functional Programming is thinking about data-flow rather than control-flow&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;He has written a lot of worth-reading &lt;a href=&quot;http://www.lihaoyi.com/&quot;&gt;articles&lt;/a&gt; as well as high-profile &lt;a href=&quot;https://github.com/lihaoyi&quot;&gt;tools&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Streaming&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Stateful processing has been added to Apache Beam, the unified model of batch and streaming. &lt;a href=&quot;https://beam.apache.org/blog/2017/02/13/stateful-processing.html&quot;&gt;Here is a nice guide to walk you through the new feature&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That's all for the first reading in 2017.   I'd like to &lt;strong&gt;write more posts&lt;/strong&gt; this year, and &lt;strong&gt;beyond weekly reading&lt;/strong&gt;. Let's see if I can win the battle against procrastination.&lt;/p&gt;
</description><pubDate>Mon, 20 Feb 2017 00:00:00 GMT</pubDate></item><item><title>Triweekly Reading 0xF</title><link>https://manuzhang.github.io/posts/2016-12-11-triweekly-15/index.html</link><description>&lt;p&gt;Starting out this week, I'd like to share something different. It's &lt;a href=&quot;http://manuginobili.com/my-showdown-with-mj/&quot;&gt;My Showdown with MJ&lt;/a&gt; by my hero Manu Ginobili recollecting his only game with Michael Jordan, his childhood hero. I believe it's called inheritance. I've always been fascinated by Manu's passion, his magic and how he saved his team and fans from despair. I even named myself &amp;quot;Manu&amp;quot;!&lt;/p&gt;
&lt;p&gt;Let's get back to technical session.&lt;/p&gt;
&lt;h3&gt;Streaming&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;data Artisans guys &lt;a href=&quot;http://data-artisans.com/stream-processing-myths-debunked/&quot;&gt;debunked six stream processing myths&lt;/a&gt;. These are long-time misconceptions on streaming people get from early versions of streaming systems.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There's no streaming without batch&lt;/li&gt;
&lt;li&gt;Latency and Throughput: Choose One&lt;/li&gt;
&lt;li&gt;Micro-batching means better throughput&lt;/li&gt;
&lt;li&gt;Exactly once? Completely impossible&lt;/li&gt;
&lt;li&gt;Streaming only applies to &amp;quot;real-time&amp;quot;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ever heard of &lt;a href=&quot;http://www.reactive-streams.org/&quot;&gt;Reactive Streams&lt;/a&gt; but wondering what it actually does? Let's get into &lt;a href=&quot;https://medium.com/@kvnwbbr/a-journey-into-reactive-streams-5ee2a9cd7e29#.n8y4v6em0&quot;&gt;A Journey into Reactive Streams&lt;/a&gt;. Reactive Streams is a specification to solve stream processing problems and its implementations will be able to communicate via the Reactive Streams protocol.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Scala&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Li haoyi &lt;a href=&quot;http://www.lihaoyi.com/post/OldDesignPatternsinScala.html&quot;&gt;explored how old design patterns apply apply to Scala&lt;/a&gt;. I like his definition of &lt;a href=&quot;https://en.wikipedia.org/wiki/Software_design_pattern&quot;&gt;Design Patterns&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A Design Pattern is something you do over and over when building software, but isn't concrete enough to be made into a helper method, class or other abstraction.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In short, Scala has built-in support for some Java design patterns in the language to make your programming easier and less error-prone.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Not all people buy in Scala or functional programming. &lt;a href=&quot;http://flyingfrogblog.blogspot.jp/2016/05/disadvantages-of-purely-functional.html&quot;&gt;Disadvantages of purely functional programming&lt;/a&gt; threw cold water on it. Given my short experience with Scala, a impurely functional programming, it's true that some data structures are not efficient and sometimes &lt;a href=&quot;https://issues.apache.org/jira/browse/GEARPUMP-249&quot;&gt;I got bitten by its laziness&lt;/a&gt; but I will never go back to Java. In Java, after working out a solution, I spend extra time to figure out &lt;strong&gt;how to implement it&lt;/strong&gt; while I can easily &lt;strong&gt;express it&lt;/strong&gt; in Scala. I also like  &lt;strong&gt;immutable&lt;/strong&gt; which reduces errors in my codes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/lampepfl/dotty/pull/1758&quot;&gt;Functions with more than 22 parameters are now automatically converted to functions taking a single object array parameter&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Akka is &lt;a href=&quot;http://blog.akka.io/artery/2016/12/05/aeron-in-artery&quot;&gt;using Aeron as the underlying transport in the new remoting implementation (codenamed Artery) for Actor messages&lt;/a&gt;. It is based on UDP, providing the same guarantees as TCP but more efficient.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Artery is designed from the ground up to support high-throughput in the magnitude of 1 million messages per second and low-latency in the magnitude of 100 microseconds.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Java&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Plumber share their experiences on &lt;a href=&quot;https://plumbr.eu/blog/java/staying-on-top-of-the-jvm-crashes&quot;&gt;the reasons why JVM crashes and where to look for evidence&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Takipi &lt;a href=&quot;http://blog.takipi.com/is-standard-java-logging-dead-log4j-vs-log4j2-vs-logback-vs-java-util-logging&quot;&gt;compare popular Java logging frameworks&lt;/a&gt; from the logging statements used by GitHub's top Java projects.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Programming&lt;/h3&gt;
&lt;p&gt;From Martin Fowler's Bliki (Blog + Wiki)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://martinfowler.com/bliki/FunctionLength.html&quot;&gt;How long should a function be?&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The argument that makes most sense to me, however, is the separation between intention and implementation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://martinfowler.com/bliki/HiddenPrecision.html&quot;&gt;Hidden precision can lead to some subtle bugs&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The more general conclusion is that floating point is tricksy when it comes to comparisons.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://martinfowler.com/bliki/AliasingBug.html&quot;&gt;Aliasing bug occurs when the same memory location is accessed through more than one reference&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;So if we want changes to be shared, we need to handle that as the exception rather than the rule.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://martinfowler.com/bliki/ValueObject.html&quot;&gt;Value Objects should be immutable to avoid aliasing bugs&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Objects that are equal due to the value of their properties are called value objects.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Others&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Google &lt;a href=&quot;https://research.googleblog.com/2016/12/open-sourcing-embedding-projector-tool.html&quot;&gt;open source the Embedding Projector for visualizing high dimensional data&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Do you know &lt;a href=&quot;http://chris.beams.io/posts/git-commit/&quot;&gt;how to write a git commit message&lt;/a&gt;? The author gives seven rules of a great a git commit message.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Apache HBase is built on HDFS. Theoretically, it could be ported to other Hadoop File Systems or general distributed file systems. &lt;a href=&quot;https://aws.amazon.com/cn/blogs/big-data/low-latency-access-on-trillions-of-records-finras-architecture-using-apache-hbase-on-amazon-emr-with-amazon-s3/&quot;&gt;FINRA migrated to HBase on EMR using S3 for storage and have lowered their costs by 60%&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://highscalability.com/blog/2016/11/28/how-to-make-your-database-200x-faster-without-having-to-pay.html&quot;&gt;How To Make Your Database 200x Faster Without Having To Pay More?&lt;/a&gt;. The key is returning 99.9%-accurate answers for low-priority queries, and 100% for the rest, or Approximate Query Processing. Some proposals are &lt;a href=&quot;http://blinkdb.org/&quot;&gt;BlinkDB&lt;/a&gt; and &lt;a href=&quot;http://www.snappydata.io/&quot;&gt;SnappyData&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description><pubDate>Sun, 11 Dec 2016 00:00:00 GMT</pubDate></item><item><title>Weekly Reading 0xE</title><link>https://manuzhang.github.io/posts/2016-11-20-weekly-14/index.html</link><description>&lt;p&gt;Last week, &lt;a href=&quot;http://events.linuxfoundation.org/events/apache-big-data-europe&quot;&gt;Apache Big Data Europe&lt;/a&gt; was held in Seville, Spain from Nov.14th to Nov.16th. The conference was full of great contents from Apache Big Data projects. My colleagues Karol and Huafeng  co-presented &lt;a href=&quot;http://events.linuxfoundation.org/sites/events/files/slides/Apache%20Gearpump%20-%2016-9.pdf&quot;&gt;Apache Gearpump Next-Gen Streaming Engine&lt;/a&gt; with the use case of Gearpump-on-&lt;a href=&quot;http://trustedanalytics.org/&quot;&gt;TAP&lt;/a&gt; and latest performance data. Huafeng also &lt;a href=&quot;http://events.linuxfoundation.org/sites/events/files/slides/Streaming%20Report.pdf]&quot;&gt;gave a report comparing modern stream processing engines on functionalities and performances&lt;/a&gt;.  I will write more about my takes here after going through the slides.&lt;/p&gt;
&lt;h3&gt;Releases&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://spark.apache.org/releases/spark-release-2-0-2.html&quot;&gt;Apache Spark 2.0.2&lt;/a&gt; has been released containing stability fixes. All 2.0.x users are strongly recommended to upgrade. Kafka 0.10 and runtime metrics have been added for Structured Streaming.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Kafka&lt;/h3&gt;
&lt;p&gt;Kafka made up a large part of my readings last week.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Anil Kumar from WalmartLabs wrote about how Kafka has decentralized autonomous services and enabled agile development in &lt;a href=&quot;https://medium.com/walmartlabs/apache-kafka-for-item-setup-3fe8f4ba5967#.8bj3haxas&quot;&gt;Apache Kafka for Item Setup&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Confluent &lt;a href=&quot;https://www.confluent.io/blog/unifying-stream-processing-and-interactive-queries-in-apache-kafka/&quot;&gt;announced Interactive Queries for Kafka Streams&lt;/a&gt;. The queryable states are stored in embedded databases like &lt;a href=&quot;http://rocksdb.org/&quot;&gt;RocksDB&lt;/a&gt;. Under the hood, each Kafka Streams instance exposes its metadata which a developer could obtain for a given store name and key through Interactive Query APIs. There is no built-in RPC layer for distributed querying but Confluent provides a [reference REST-based implementation](REST-based implementation).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Confluent also &lt;a href=&quot;https://www.confluent.io/blog/confluent-contributions-to-the-apache-kafka-client-ecosystem&quot;&gt;walked through their contributions to Kafka Client Ecosystem&lt;/a&gt;. Besides the Java client, they have focused on making high quality C client and wrap C client for clients in other languages.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kafka Streams is a lightweight library and has no built-in scheduler or cluster support. How can it be scaled ? &lt;a href=&quot;http://aseigneurin.github.io/2016/10/07/kafka-streams-scaling-up-or-down.html&quot;&gt;Kafka Streams - Scaling up or down&lt;/a&gt; explains it with a simple example.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The scaling of Kafka Streams is built on Kafka's consumer group and its rebalancing feature. Here is article to hep you &lt;a href=&quot;https://dzone.com/articles/understanding-kafka-consumer-groups-and-consumer-l&quot;&gt;understand Kafka consumer groups&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Deep Learning and AI&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;What Artificial Intelligence Can and Can’t Do Right Now ? Andrew Ng gives his &lt;a href=&quot;https://hbr.org/2016/11/what-artificial-intelligence-can-and-cant-do-right-now&quot;&gt;answers&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;What AI can do ?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If a typical person can do a mental task with less than one second of thought, we can probably automate it using AI either now or in the near future.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What AI can't do ?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;AI work requires carefully choosing A and B and providing the necessary data to help the AI figure out the A→B relationship.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A is the input and B is the response.  The necessary data means a huge amount of data which Andrew calls Achilles' heel in today's supervised learning software.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Algorithmia shared about their &lt;a href=&quot;http://blog.algorithmia.com/deploying-deep-learning-cloud-services/&quot;&gt;lessons learned in deploying deep learning at scale&lt;/a&gt;. They find the cloud is in its infancy to deploy models into production.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That's all for this week. Happy Reading !&lt;/p&gt;
</description><pubDate>Sun, 20 Nov 2016 00:00:00 GMT</pubDate></item><item><title>Biweekly Reading 0xD</title><link>https://manuzhang.github.io/posts/2016-11-13-biweekly-13/index.html</link><description>&lt;p&gt;The biggest news for me in the past two weeks is the &lt;a href=&quot;https://en.wikipedia.org/wiki/United_States_presidential_election,_2016&quot;&gt;US Presidential Election&lt;/a&gt;. No, it's the release of &lt;a href=&quot;http://www.lightbend.com/blog/scala-2-12-released&quot;&gt;Scala 2.12&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The Scala 2.12 compiler has been completely overhauled to fully leverage the new VM features available in Java 8&lt;br /&gt;
...&lt;br /&gt;
Code compiled on 2.12 requires a Java 8 runtime&lt;br /&gt;
...&lt;br /&gt;
Scala 2.12 maintains source compatibility with 2.11, cross-building for both 2.11 and 2.12 is a one-line change to most sbt-based projects&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The latest news is that Scala 2.12 would require jdk8u111 owing to a &lt;a href=&quot;https://issues.scala-lang.org/browse/SI-9828&quot;&gt;JIT bug&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Also in the news are the results for &lt;a href=&quot;http://sortbenchmark.org/&quot;&gt;Sort Benchmark Competition&lt;/a&gt; this year. Tencent won GraySort (44.8 TB/min) and MinuteSort (37TB in 60 seconds) with OpenPOWER systems while Alibaba lead CloudSort ($1.44/TB) with Haswell servers.&lt;/p&gt;
&lt;p&gt;More on the hardware side, North Carolina researchers and Intel proposed to &lt;a href=&quot;https://www.extremetech.com/computing/238586-using-hardware-queues-break-multi-core-bottleneck&quot;&gt;use hardware queues to break the multi-core CPU bottleneck&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now some nice readings for the past weeks.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Adam Warski &lt;a href=&quot;https://softwaremill.com/windowing-in-big-data-streams-spark-flink-kafka-akka/&quot;&gt;looks at Windowing in Big Data Streams&lt;/a&gt; and how they've been supported in Spark Streaming, Flink, Kafka Streams and Akka Streams (Remember that Adam &lt;a href=&quot;https://softwaremill.com/windowing-data-in-akka-streams/&quot;&gt;manually implemented windowing&lt;/a&gt; for Akka).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Jendrik Poloczek &lt;a href=&quot;https://www.madewithtea.com/released-mocked-streams-for-apache-kafka.html&quot;&gt;released Mocked Streams to unit test Kafka Streams topologoies&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Previously, we had an unsmooth experience through our journey of &lt;a href=&quot;http://manuzhang.github.io/2016/10/15/shading.html&quot;&gt;Shade with SBT&lt;/a&gt;. Someone even said &lt;a href=&quot;https://www.reddit.com/r/scala/comments/5a6muj/sbt_makes_me_want_to_give_up_scala/&quot;&gt;SBT makes me want to give up Scala&lt;/a&gt; on Reddit. In the discussion, however, I found a nice article from Lightbend developer, James Roper, introducing &lt;a href=&quot;http://manuzhang.github.io/2016/10/15/shading.html&quot;&gt;sbt, A task engine&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Do you know how much L3 cache read usually costs ? &lt;a href=&quot;http://ithare.com/infographics-operation-costs-in-cpu-clock-cycles/&quot;&gt;An infographics estimates costs of certain operations in CPU clocks cycles&lt;/a&gt; will help you to answer this question. This reminds me of &lt;a href=&quot;https://people.eecs.berkeley.edu/~rcs/research/interactive_latency.html&quot;&gt;Latency Numbers Every Programmer Should Know&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bloomberg Beta has published &lt;a href=&quot;https://www.oreilly.com/ideas/the-current-state-of-machine-intelligence-3-0&quot;&gt;the current state of machine intelligence 3.0&lt;/a&gt; with a crowded image including Technology Stack, Enterprise and Industries, and Autonomous Systems.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are far more interesting materials from&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.lightbend.com/blog/lightbend-tech-digest-october-2016?utm_content=buffer4d3e6&amp;amp;utm_medium=social&amp;amp;utm_source=twitter.com&amp;amp;utm_campaign=buffer&quot;&gt;Lightbend Tech Digest - October 2016&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.confluent.io/blog/bloglog-compaction-highlights-in-the-apache-kafka-and-stream-processing-community-november-2016/&quot;&gt;Log Compaction - Highlights in the Apache Kafka and Stream Processing Community - November 2016&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description><pubDate>Sun, 13 Nov 2016 00:00:00 GMT</pubDate></item><item><title>Shade with SBT II</title><link>https://manuzhang.github.io/posts/2016-11-12-shading-2/index.html</link><description>&lt;p&gt;Previously, we talked about &lt;a href=&quot;http://manuzhang.github.io/2016/10/15/shading.html&quot;&gt;how we shaded Gearpump dependencies with SBT&lt;/a&gt;. One thing I didn't see or mention at that time is that the compilation of depending projects could fail since the shading of depended libraries happen simultaneously. I tried enforcing shade first with tricks like SBT's &lt;a href=&quot;http://www.scala-sbt.org/0.13/docs/Howto-Sequential-Task.html&quot;&gt;sequential task&lt;/a&gt; but that didn't work (At least, I don't know how to). Finally, I went to &lt;a href=&quot;http://stackoverflow.com/questions/40397065/how-to-shade-before-compile-with-sbt&quot;&gt;Stackoverflow&lt;/a&gt; for last resort and &lt;a href=&quot;http://stackoverflow.com/users/1870803/yuval-itzchakov&quot;&gt;Yuval Itzchakov&lt;/a&gt; saved my day with his answer that I should shade the depending project along with its dependencies.&lt;/p&gt;
&lt;p&gt;Remember the shaded libraries were depended as &lt;code class='language-text'&gt;unmanagedJars&lt;/code&gt; and we had to manually add the dependencies to the published pom of the depending project. Now the problem is kind of the opposite. The dependencies are already assembled and published as a uber jar with the depending codes. Now that being &lt;em&gt;managed dependencies&lt;/em&gt; they also end up in the published pom. Hence, we need to manually remove the dependencies in &lt;code class='language-text'&gt;pomPostProcess&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;One more subtlety here is &lt;code class='language-text'&gt;gearpump-streaming&lt;/code&gt; depends on &lt;code class='language-text'&gt;gearpump-core&lt;/code&gt; with both &lt;code class='language-text'&gt;test&lt;/code&gt; and &lt;code class='language-text'&gt;provided&lt;/code&gt; dependencies. The published pom, however, only contains the &lt;code class='language-text'&gt;test&lt;/code&gt; dependency. I haven't found a cleaner way than again &lt;em&gt;manually&lt;/em&gt; added it. I also posted &lt;a href=&quot;http://stackoverflow.com/questions/40526420/sbt-published-maven-file-missing-artifacts-with-multiple-scopes&quot;&gt;a question to Stackoverflow&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;One one more thing is &lt;code class='language-text'&gt;java.lang.VerifyError&lt;/code&gt; exceptions are thrown by JVM after the change. The work-around is adding &lt;code class='language-text'&gt;-noverify&lt;/code&gt; in the JVM options. I haven't yet found the root cause and the issue is logged at &lt;a href=&quot;https://issues.apache.org/jira/browse/GEARPUMP-236&quot;&gt;GEARPUMP-236&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I have a feeling that there will be a third episode of our story with sbt shade.&lt;/p&gt;
</description><pubDate>Sat, 12 Nov 2016 00:00:00 GMT</pubDate></item><item><title>Biweekly Reading 0xC</title><link>https://manuzhang.github.io/posts/2016-10-30-biweekly-12/index.html</link><description>&lt;p&gt;Last week, I made use of my three-hour shuttle time watching &lt;a href=&quot;https://www.youtube.com/channel/UChUrUs_xAW2YiSV7iBWkzhw&quot;&gt;Reactive Summit 2016&lt;/a&gt;. Since YouTube wouldn't allow me to cache videos so I downloaded them to my laptop with a &lt;a href=&quot;https://www.kissyoutube.com/watch?v=DRK7WYNh6AA&quot;&gt;&amp;quot;kiss&amp;quot;&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Reactive Summit&lt;/h3&gt;
&lt;h4&gt;Distributed stream processing with Apache Kafka&lt;/h4&gt;
&lt;p&gt;My first pick is Jay Kreps' keynote on &lt;a href=&quot;https://reactivesummit2016.sched.org/event/8B6K/distributed-stream-processing-with-apache-kafka&quot;&gt;Distributed stream processing with Apache Kafka&lt;/a&gt;. His articles have never failed me and this talk is no exception. He firstly argued there is an intersection between streaming processing and micro-services, which his talk is about. Then he categorized computer programming into 3 paradigms based on input / output.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Request / Response. One Input and one output, and only process future input.&lt;/li&gt;
&lt;li&gt;Batch. A batch of inputs and outputs, and only process past input.&lt;/li&gt;
&lt;li&gt;Stream Processing. &lt;strong&gt;Generalization of 1 and 2&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I couldn't agree more that &lt;strong&gt;stream processing isn't necessarily transient, approximate and lossy&lt;/strong&gt;. After introducing the challenges in stream processing and micro-services, he spent rest of the time on how to solve the hard parts with Kafka, Kafka Streams, Kafka Connect and Confluent platform.&lt;/p&gt;
&lt;p&gt;I really enjoy Jay's keynote, especially the first half.&lt;/p&gt;
&lt;h4&gt;bla bla microservices bla bla: Director’s Cut&lt;/h4&gt;
&lt;p&gt;Akka inventor Jonas Boner &lt;a href=&quot;https://reactivesummit2016.sched.org/event/7geD/bla-bla-microservices-bla-bla-directors-cut&quot;&gt;distilled into micro-services and how to build distributed systems on it&lt;/a&gt;. My most experiences with micro-services come from &lt;a href=&quot;http://akka.io/&quot;&gt;Akka&lt;/a&gt; and &lt;a href=&quot;http://gearpump.apache.org&quot;&gt;Gearpump&lt;/a&gt;, where different system roles (actors) work asynchronously and communicate only through messages. I have no idea how micro-services work as a backend across groups in a big company. Hence, Jonas' talk, full of good quotes, looks more like a bird view to me. Nevertheless, I do remember he said&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;There is no such thing as a &amp;quot;stateless&amp;quot; architecture. It's just someone else's problem.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I believe state management and &amp;quot;stateful&amp;quot; API is a must-have for a stream processing system.&lt;/p&gt;
&lt;h4&gt;Scala and the JVM as a Big Data Platform - Lessons from Apache Spark&lt;/h4&gt;
&lt;p&gt;Dean Wampler &lt;a href=&quot;https://reactivesummit2016.sched.org/event/7emh/scala-and-the-jvm-as-a-big-data-platform-lessons-from-apache-spark&quot;&gt;shared how easy it is to develop distributed applications with Spark&lt;/a&gt;. Meanwhile, the JVM has significant GC problems and Spark is fixing it with project &amp;quot;Tungsten&amp;quot;. What caught my eyes is the OOM issue caused by copy of 2.2GB array. The codes below, although written in Scala REPL, will be compiled into a JVM class. Then the instance, closure over &lt;code class='language-text'&gt;b&lt;/code&gt;,  will be serialized and shipped to remote cluster with all of its fields including the 2.2GB array.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; val N = 1100 * 1000 * 1000
scala&amp;gt; val array = Array.fill[Short](N)(0)
scala&amp;gt; val b = sc.broadcast(array)
scala&amp;gt; sc.parallelize(0 until 100000).
     | map(i =&amp;gt; b.value(i))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The solution is to either mark the array as &lt;code class='language-text'&gt;@transient&lt;/code&gt; or put it into a singleton, a companion object. Please check the slides for more details.&lt;/p&gt;
&lt;h4&gt;The Zen of Erlang&lt;/h4&gt;
&lt;p&gt;Akka brings Erlang's actor model to JVM. &lt;a href=&quot;https://reactivesummit2016.sched.org/event/7emY/the-zen-of-erlang&quot;&gt;The Zen of Erlang&lt;/a&gt;, by Fred Hebert, is almost the design patterns of Akka except for &lt;strong&gt;preemptive scheduling&lt;/strong&gt;. We can't do blocking stuff in actor if a shared-thread-pool dispatcher is used. Otherwise, the whole system will be blocked.&lt;/p&gt;
&lt;p&gt;Built on Akka, Gearpump employs Akka's supervision tree for fault tolerance. The supervisor will restart an application on task failures. Why restarting works ? According to Fred, there are such thing as transient bugs which are hard to find in development but happen in production all the time. Restarting heals it.&lt;/p&gt;
&lt;p&gt;That's my impression on Reactive Summit so far. I'll share more as I read on. Let's turn our attention towards the Big Data community.&lt;/p&gt;
&lt;h3&gt;Releases&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://www.confluent.io/blog/announcing-apache-kafka-0-10-1-0/&quot;&gt;Apache Kafka 0.10.1.0 released&lt;/a&gt; including completion of 15 KIPs, over 200 bug fixes and improvements. One notable feature is &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-67%3A+Queryable+state+for+Kafka+Streams&quot;&gt;queryable states&lt;/a&gt; for Kafka Streams.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://notes.implicit.ly/post/152428448989/sbt-013-01313&quot;&gt;sbt 0.13.13 is available&lt;/a&gt; deprecating old sbt 0.12 DSL, &lt;code class='language-text'&gt;&amp;lt;&amp;lt;=&lt;/code&gt;, &lt;code class='language-text'&gt;&amp;lt;+=&lt;/code&gt;, &lt;code class='language-text'&gt;&amp;lt;++=&lt;/code&gt; and tuple enrichments.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Big Data Systems&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://data-artisans.com/blink-flink-alibaba-search/&quot;&gt;Blink: How Alibaba Uses Apache Flink&lt;/a&gt; introduces Flink's usage at Alibaba search and how it is adapted to meet their unique requirements. The good news is Alibaba are contributing the improvements back to community and transiting to vanilla Flink.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://code.facebook.com/posts/319004238457019/a-comparison-of-state-of-the-art-graph-processing-systems/&quot;&gt;Facebook did a comparison between Giraph and GraphX&lt;/a&gt;. The key takeaway is Giraph has larger scale and better performance while GraphX is easier to program.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://databricks.com/blog/2016/10/27/gpu-acceleration-in-databricks.html&quot;&gt;Databricks is adding support for Spark clusters with GPU to accelerate deep learning workloads&lt;/a&gt;. Spark already has integrations with &lt;a href=&quot;https://github.com/databricks/tensorframes&quot;&gt;Tensorflow&lt;/a&gt; and &lt;a href=&quot;http://caffe.berkeleyvision.org/&quot;&gt;Caffe&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Software&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.acolyer.org/2016/10/24/whats-wrong-with-git-a-conceptual-design-analysis/&quot;&gt;What's wrong with Git ? A conceptual design analysis&lt;/a&gt;. Ever confused with Git's staging concept ? Felt cumbersome to stash or commit unfinished changes before switching branches. The authors show how the conceptual model of Git lead to those difficulties and simplify it in &lt;a href=&quot;http://gitless.com/&quot;&gt;Gitless&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How to code with good taste ? &lt;a href=&quot;https://medium.com/@bartobri/applying-the-linus-tarvolds-good-taste-coding-requirement-99749f37684a#.leqrzi260&quot;&gt;Applying the Linus Torvalds “Good Taste” Coding Requirement&lt;/a&gt; looks at Linus' taste and conclude&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What I think Linus meant, and what developers who have “good taste” do differently, is that they take the time to conceptualize what they are building before they start.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Think about it.&lt;/p&gt;
</description><pubDate>Sun, 30 Oct 2016 00:00:00 GMT</pubDate></item><item><title>Monthly Reading 0xB</title><link>https://manuzhang.github.io/posts/2016-10-16-monthly-11/index.html</link><description>&lt;p&gt;It seems everyone in Big Data is doing Weekly / Bi-weekly / Monthly review. Here is my subscription list&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.hadoopweekly.com/&quot;&gt;Hadoop Weekly&lt;/a&gt; by &lt;a href=&quot;https://twitter.com/joecrobak&quot;&gt;Joe Crobak&lt;/a&gt; (Weekly)&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.cakesolutions.net/teamblogs/this-week-in-scala-10/10/2016&quot;&gt;This week in #Scala&lt;/a&gt; by CakeSolutions (Weekly)&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.confluent.io/blog/log-compaction-highlights-apache-kafka-stream-processing-community-september-2016/&quot;&gt;Log Compaction - Highlights in the Apache Kafka and Stream Processing Community&lt;/a&gt; by Confluent (Monthly)&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://databricks.com/blog/2016/10/04/databricks-bi-weekly-apache-spark-digest-10316.html&quot;&gt;Databricks Bi-Weekly Apache Spark Digest&lt;/a&gt; by Databricks (Bi-Weekly)&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.lightbend.com/blog/lightbend-tech-digest-september-2016&quot;&gt;Lightbend Tech Digest&lt;/a&gt; by Lightbend (Monthly)&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://data-artisans.com/sept-2016-in-review/&quot;&gt;Montly Review&lt;/a&gt; by dataArtisans (Monthly)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now I'm writing up a review based on reviews.&lt;/p&gt;
&lt;h3&gt;Releases&lt;/h3&gt;
&lt;p&gt;A bunch of maintenace releases from the open source community.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://storm.apache.org/2016/09/14/storm0102-released.html&quot;&gt;Apache Storm made a maintenance release 0.10.2 for 0.10&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://flink.apache.org/news/2016/10/12/release-1.1.3.html&quot;&gt;Apache Flink released 1.1.3&lt;/a&gt;, bugfix release for 1.1 series. RocksDB state backend users are recommended to use the &amp;quot;fully async&amp;quot; mode such that upgrade to 1.2 will be easier.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://spark.apache.org/releases/spark-release-2-0-1.html&quot;&gt;Apache Spark also made a maintenance release 2.0.1&lt;/a&gt; containing 300 stability and bug fixes.&lt;/li&gt;
&lt;li&gt;Apache Calcite released &lt;a href=&quot;https://calcite.apache.org/docs/history.html#v1-10-0&quot;&gt;1.10.0&lt;/a&gt; and &lt;a href=&quot;https://calcite.apache.org/docs/history.html#v1-9-0&quot;&gt;1.9.0&lt;/a&gt;, and mainly improved Druid Adapter.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Conferences&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://flink-forward.org/thank-you/#more-1037&quot;&gt;Flink Forward 2016&lt;/a&gt; was held from Sep.12th to Sep.14th with 350 attendees and over 40 speakers. Videos and slides for all &lt;a href=&quot;http://flink-forward.org/program/sessions/&quot;&gt;sessions&lt;/a&gt; are available now. After a quick glance through, I come up with this list of mainly technical talks which nicely capture the current status and next steps of Flink.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://flink-forward.org/kb_sessions/connecting-apache-flink-with-the-world-reviewing-the-streaming-connectors/&quot;&gt;Connecting Apache Flink to the World: Reviewing the streaming connectors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://flink-forward.org/kb_sessions/declarative-stream-processing-with-streamsql-and-cep/&quot;&gt;Declarative stream processing with StreamSQL and CEP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://flink-forward.org/kb_sessions/dynamic-scaling-how-apache-flink-adapts-to-changing-workloads/&quot;&gt;Dynamic scaling: How Apache Flink adapts to changing workloads&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://flink-forward.org/kb_sessions/running-apache-flink-everywhere-standalone-yarn-mesos-docker-kubernetes-etc/&quot;&gt;Running Apache Flink everywhere: Standalone, Yarn, Mesos, Docker, Kubernetes, etc.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://flink-forward.org/kb_sessions/scaling-stream-processing-with-apache-flink-to-very-large-state/&quot;&gt;Scaling Stream Processing with Apache Flink to very large State&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://flink-forward.org/kb_sessions/streaming-ml-with-flink/&quot;&gt;Streaming ML with Flink
&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://flink-forward.org/kb_sessions/taking-a-look-under-the-hood-of-apache-flinks-relational-apis/&quot;&gt;Taking a look under the hood of Apache Flink’s relational APIs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://flink-forward.org/kb_sessions/the-future-of-apache-flinktm/&quot;&gt;The Future of Apache Flink&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://www.reactivesummit.org/&quot;&gt;Reactive Summit 2016&lt;/a&gt;, the first ever Reactive Summit, took place Oct.4 - Oct.5 in Austin. All talks are now &lt;a href=&quot;https://www.youtube.com/channel/UChUrUs_xAW2YiSV7iBWkzhw/videos&quot;&gt;available&lt;/a&gt;. I haven't gone through the topics but &lt;a href=&quot;http://sched.co/7jeY&quot;&gt;Implementing an akka-streams materializer for big data&lt;/a&gt; by our &lt;a href=&quot;https://twitter.com/kkasravi&quot;&gt;Kam&lt;/a&gt; is definitely worth checking out.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Articles&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://www.infoworld.com/article/3128344/analytics/7-big-data-tools-to-ditch-in-2017.html&quot;&gt;7-big-data-tools-to-ditch-in-2017&lt;/a&gt;. Both MapReduce and its streaming counterpart Storm make the list. Wait, are you sure to ditch Java ?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://medium.baqend.com/real-time-stream-processors-a-survey-and-decision-guidance-6d248f692056#.ws1mi2eb9&quot;&gt;Scalable Stream Processing: A Survey of Storm, Samza, Spark and Flink&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;give an overview over the state of the art of stream processors for low-latency Big Data analytics and conduct a qualitative comparison of the most popular contenders&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;4-min read on &lt;a href=&quot;https://medium.com/the-hoard/how-kafkas-storage-internals-work-3a29b02e026#.gwuov9szg&quot;&gt;How Kafka’s Storage Internals Work&lt;/a&gt;. A big fan of such short informative stories.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Jay Kreps has a new masterpiece, &lt;a href=&quot;http://www.confluent.io/blog/sharing-is-caring-multi-tenancy-in-distributed-data-systems&quot;&gt;Sharing is Caring: Multi-tenancy in Distributed Data Systems&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You see hundreds of blog posts on benchmarking infrastructure systems—showing millions of requests per second on vast clusters—but far fewer about the work of scaling a system to hundreds or thousands of engineers and use cases. It’s just a lot harder to quantify multi-tenancy than it is to quantify scalability.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Can't agree more ! A multi-tenancy benchmark, anyone ? I'll leave you here to think more about this topic.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description><pubDate>Sun, 16 Oct 2016 00:00:00 GMT</pubDate></item><item><title>Shade with SBT</title><link>https://manuzhang.github.io/posts/2016-10-15-shading/index.html</link><description>&lt;p&gt;What will we learn in this post ?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is / Why shade ?&lt;/li&gt;
&lt;li&gt;How to shade with &lt;code class='language-text'&gt;sbt-assembly&lt;/code&gt; ?&lt;/li&gt;
&lt;li&gt;How to add shaded library to published maven &lt;code class='language-text'&gt;pom&lt;/code&gt; ?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&quot;http://www.scala-sbt.org/&quot;&gt;SBT&lt;/a&gt; (Simple Build Tool) is a build tool for Scala and Java. We write build definition in Scala and it seems we can do anything with the flexibility of the language.  It also means sometimes we have to do it ourselves. Another downside is SBT has scarce documentation on its &lt;a href=&quot;https://github.com/sbt/sbt/blob/1.0.x/main/src/main/scala/sbt/Keys.scala&quot;&gt;configuration keys&lt;/a&gt; so we have to dig into its hidden capabilities.&lt;/p&gt;
&lt;p&gt;As a Scala project, &lt;a href=&quot;http://gearpump.apache.org&quot;&gt;Apache Gearpump (incubating)&lt;/a&gt; has been using SBT from the beginning. In this post, I'll share our experience working on the shaded libraries / dependencies of Gearpump.&lt;/p&gt;
&lt;h3&gt;What is / Why shade ?&lt;/h3&gt;
&lt;p&gt;Gearpump uses popular libraries like &lt;a href=&quot;https://github.com/google/guava&quot;&gt;guava&lt;/a&gt; and puts them on classpath on launch. Guava is popular so it may happen to be depended on guava by a user application but unfortunately of a &lt;strong&gt;different&lt;/strong&gt; version. At runtime, two versions of guava will be on the classpath but the system guava could be loaded rather than the users' and unexpectedly breaks the application. That's where shade comes in. It allows us, for example, to &lt;strong&gt;rename&lt;/strong&gt; the system guava classes from &lt;code class='language-text'&gt;com.google.*&lt;/code&gt; to &lt;code class='language-text'&gt;org.apache.gearpump.google.*&lt;/code&gt;.  While user codes will import &lt;code class='language-text'&gt;com.google.*&lt;/code&gt;, Gearpump system code will have &lt;code class='language-text'&gt;org.apache.gearpump.google.*&lt;/code&gt; such that they will never be accidentally loaded into a user application.&lt;/p&gt;
&lt;h3&gt;How to shade in SBT ?&lt;/h3&gt;
&lt;p&gt;There is no available shade plugin for SBT like &lt;a href=&quot;http://maven.apache.org/plugins/maven-shade-plugin/&quot;&gt;Maven Shade Plugin&lt;/a&gt;. In the old time, we shade system libraries with Maven and host them at &lt;a href=&quot;https://github.com/gearpump/gearpump-shaded-repo&quot;&gt;gearpump-shaded-repo&lt;/a&gt; as external dependencies. When Gearpump was moving to Apache, it's required they should be hosted at Gearpump's Apache repo. To the rescue, the &lt;a href=&quot;https://github.com/sbt/sbt-assembly&quot;&gt;sbt-assembly&lt;/a&gt; plugin has supported shade since its 0.14.0 release. sbt-assembly is to create a fat JAR of a project with all of its dependencies (akin to &lt;a href=&quot;http://maven.apache.org/plugins/maven-assembly-plugin/&quot;&gt;Maven Assembly Plugin&lt;/a&gt;). In Gearpump, we usually use it to assemble example projects. It can be used like &lt;a href=&quot;http://maven.apache.org/plugins/maven-shade-plugin/&quot;&gt;Maven Shade Plugin&lt;/a&gt; as well.&lt;/p&gt;
&lt;p&gt;We create empty SBT shaded projects with the corresponding original libraries as dependencies and define &lt;code class='language-text'&gt;ShadeRule&lt;/code&gt; (check out more &lt;a href=&quot;https://github.com/sbt/sbt-assembly#shading&quot;&gt;shading rules&lt;/a&gt;) inside &lt;code class='language-text'&gt;assemblyShadeRules&lt;/code&gt;, e.g.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val shaded = Project(
  id = &amp;quot;gearpump-shaded&amp;quot;,
  base = file(&amp;quot;shaded&amp;quot;)
  ).aggregate(shaded_akka_kryo, shaded_gs_collections, shaded_guava, shaded_metrics_graphite)
  
lazy val shaded_guava = Project(
  id = &amp;quot;gearpump-shaded-guava&amp;quot;,
  base = file(&amp;quot;shaded/guava&amp;quot;),
  settings = shadeAssemblySettings ++ addArtifact(Artifact(&amp;quot;gearpump-shaded-guava&amp;quot;), sbtassembly.AssemblyKeys.assembly) ++
  Seq(
    assemblyShadeRules in assembly := Seq(
      ShadeRule.rename(&amp;quot;com.google.**&amp;quot; -&amp;gt; &amp;quot;org.apache.gearpump.google.@1&amp;quot;).inAll
    )
  ) ++
  Seq(
    libraryDependencies ++= Seq(
      &amp;quot;com.google.guava&amp;quot; % &amp;quot;guava&amp;quot; % guavaVersion
    )
  )
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note we declare this project to publish with &lt;code class='language-text'&gt;addArtifact(Artifact(&amp;quot;gearpump-shaded-akka-kryo&amp;quot;), sbtassembly.AssemblyKeys.assembly)&lt;/code&gt; (Please refer to &lt;a href=&quot;http://www.scala-sbt.org/1.0/docs/Artifacts.html#Defining+custom+artifacts&quot;&gt;Defining custom artifacts&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;With &lt;code class='language-text'&gt;shadeAssemblySettings&lt;/code&gt; we add more assembly options. The &lt;code class='language-text'&gt;assemblyJarName&lt;/code&gt; and &lt;code class='language-text'&gt;target in assembly&lt;/code&gt; is crucial for other projects to pick up the dependency.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val shadeAssemblySettings = Build.commonSettings ++ Seq(
  scalaVersion := Build.scalaVersionNumber,
  test in assembly := {},
  assemblyOption in assembly ~= {
    _.copy(includeScala = false)
  },
  assemblyJarName in assembly := {
    s&amp;quot;${name.value}_$scalaVersionMajor-${version.value}.jar&amp;quot;
  },
  target in assembly := baseDirectory.value.getParentFile / &amp;quot;target&amp;quot; / scalaVersionMajor
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember it's the assembled shaded jar that will be depended, inter-project source dependency like the following won't work.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;lazy val core = Project(
  id = &amp;quot;gearpump-core&amp;quot;,
  base = file(&amp;quot;core&amp;quot;),
  settings = commonSettings ++ javadocSettings ++ coreDependencies
).dependsOn(shaded_guava)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;SBT provides an &lt;code class='language-text'&gt;unmanagedJars&lt;/code&gt; setting such that we can add an arbitrary jar to the dependencies. The jar path must match that defined for shaded project.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;lazy val core = Project(
  id = &amp;quot;gearpump-core&amp;quot;,
  base = file(&amp;quot;core&amp;quot;),
  settings = commonSettings ++ javadocSettings ++ coreDependencies ++ Seq(
    unmanagedJars in compile ++= Seq(
      shaded.base / &amp;quot;target&amp;quot; / scalaVersionMajor / s&amp;quot;gearpump-shaded-guava_$scalaVersionMajor-$gearpumpVersion.jar&amp;quot;
    )
  )
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we can compile with &lt;code class='language-text'&gt;sbt assembly&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;How to add shaded library to published Maven &lt;code class='language-text'&gt;pom&lt;/code&gt; ?&lt;/h3&gt;
&lt;p&gt;The &lt;code class='language-text'&gt;unmanagedJar&lt;/code&gt; approach doesn't add the shaded project to the published Maven pom of &lt;code class='language-text'&gt;gearpump-core&lt;/code&gt; project. Although being transitive dependencies,  they have to be explicitly added to the build file, which means more cumbersome for users. Are we at a dead end ?&lt;/p&gt;
&lt;p&gt;Not yet. It's time to dig into SBT configuration keys! The nugget I found is &lt;code class='language-text'&gt;pomPostProcess&lt;/code&gt; which &amp;quot;Transforms the generated POM&amp;quot;. We traverse the XML tree and append the shaded dependency to the &lt;code class='language-text'&gt;&amp;lt;dependencies&amp;gt;&amp;lt;/dependencies&amp;gt;&lt;/code&gt; tags.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;lazy val core = Project(
  id = &amp;quot;gearpump-core&amp;quot;,
  base = file(&amp;quot;core&amp;quot;),
  settings = commonSettings ++ javadocSettings ++ coreDependencies ++ Seq(
    pomPostProcess := {
        (node: xml.Node) =&amp;gt; addShadedDeps(List(
          &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;{organization.value}&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;{shaded_guava.id}&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;{version.value}&amp;lt;/version&amp;gt;
          &amp;lt;/dependency&amp;gt;
        ), node)
    },
  
    unmanagedJars in compile ++= Seq(
      shaded.base / &amp;quot;target&amp;quot; / scalaVersionMajor / s&amp;quot;gearpump-shaded-guava_$scalaVersionMajor-$gearpumpVersion.jar&amp;quot;
    )
  )
)
  
private def addShadedDeps(deps: Seq[xml.Node], node: xml.Node): xml.Node = {
  node match {
    case elem: xml.Elem =&amp;gt;
      val child = if (elem.label == &amp;quot;dependencies&amp;quot;) {
        elem.child ++ deps
      } else {
        elem.child.map(addShadedDeps(deps, _))
      }
      xml.Elem(elem.prefix, elem.label, elem.attributes, elem.scope, false, child: _*)
    case _ =&amp;gt;
      node
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;There is not too much one can find when googling for the above problems. I hope this post can become a handy guide for SBT users. Please check out Gearpump build files for a complete example.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/apache/incubator-gearpump/blob/cbf59fb77834914116657a135b30899b91a7408d/project/BuildShaded.scala&quot;&gt;BuildShaded.scala&lt;/a&gt; includes shaded projects.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/apache/incubator-gearpump/blob/cbf59fb77834914116657a135b30899b91a7408d/project/Build.scala#L184&quot;&gt;Build.scala&lt;/a&gt; includes &lt;code class='language-text'&gt;gearpump-core&lt;/code&gt; and &lt;code class='language-text'&gt;gearpump-streaming&lt;/code&gt; that depends on shaded projects.&lt;/li&gt;
&lt;/ul&gt;
</description><pubDate>Sat, 15 Oct 2016 00:00:00 GMT</pubDate></item><item><title>Rebuild Gearpump documentation with MkDocs</title><link>https://manuzhang.github.io/posts/2016-09-25-mkdocs/index.html</link><description>&lt;p&gt;&lt;em&gt;(Disclamer: I am a committer of Apache Gearpump(incubating))&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;What's wrong with Jekyll&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://gearpump.apache.org/&quot;&gt;Apache Gearpump(incubating)&lt;/a&gt; website is currently built with Jekyll, the most popular &lt;a href=&quot;https://staticsitegenerators.net/&quot;&gt;static site generator&lt;/a&gt;. Most of the contents are written in &lt;a href=&quot;https://daringfireball.net/projects/markdown/&quot;&gt;Markdown&lt;/a&gt; while Jekyll allows you to mix in HTMLs. With the default &amp;quot;Bootstrap&amp;quot; style theme, the website has a two-level top navigation bar. That is good for a general blog but not for documentations because the items are hidden in the pull-down menu and there is no table of contents for each page. First-time visitors won't know what to look for or which menu to click on.&lt;/p&gt;
&lt;p&gt;I prefer the &amp;quot;ReadTheDocs&amp;quot; style like &lt;a href=&quot;http://docs.confluent.io&quot;&gt;Confluent's Doc site&lt;/a&gt; where all the items are shown on the sidebar. If you click on one item, it will be collapsed into a multi-level table of contents and it's easy to go back and forth. I've been searching for such themes in Jekyll but the closest I've found is &lt;a href=&quot;http://idratherbewriting.com/documentation-theme-jekyll/mydoc_sidebar_navigation.html&quot;&gt;Jekyll Doc Theme&lt;/a&gt;. However, the table of contents of a page lies at the head of contents which I think is not convenient for navigation. Confluent's Doc site is built with &lt;a href=&quot;http://www.sphinx-doc.org/en/stable/&quot;&gt;Sphinx&lt;/a&gt; and the sources are written in &lt;a href=&quot;http://docutils.sourceforge.net/rst.html&quot;&gt;reStructuredText&lt;/a&gt;. I didn't want to raise the bar for contributors by learning another markup language so I started to look for alternative generators that support Markdown.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.mkdocs.org/&quot;&gt;MkDocs&lt;/a&gt; is by far the best answer with both Markdown sources and built-in &amp;quot;ReadTheDocs&amp;quot; theme. Then I started to rebuild documentations with MkDocs. It's not a smooth transition and I'd like to record the roadblocks.&lt;/p&gt;
&lt;h2&gt;Roadblocks&lt;/h2&gt;
&lt;h3&gt;Global Variables&lt;/h3&gt;
&lt;p&gt;We may have version numbers in the documentation here and there. For example,&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;### To run WordCount example
bin/gear app -jar examples/wordcount-2.11-0.8.1-assembly.jar org.apache.gearpump.streaming.examples.wordcount.WordCount
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are two version numbers here, scala version(2.11) and gearpump version(0.8.1). We don't want to hardcode the numbers which would mean updating all the numbers whenever a new version of Gearpump is released. We want variables. In Jekyll, we can define version numbers as &lt;a href=&quot;https://jekyllrb.com/docs/variables/&quot;&gt;variables&lt;/a&gt; in configuration files. Unfortunately, MkDocs hasn't supported variables for Markdown source yet and the work is still &lt;a href=&quot;https://github.com/mkdocs/mkdocs/issues/304&quot;&gt;under discussion&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In the discussion thread, someone shared a work-around where markdown sources are pre-processed with a template system like &lt;a href=&quot;http://mustache.github.io/&quot;&gt;mustache&lt;/a&gt; and variables will be replaced with corresponding values. The previous example is now written as&lt;/p&gt;
&lt;p&gt;{% raw %}&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;### To run WordCount example
bin/gear app -jar examples/wordcount-{{SCALA_BINARY_VERSION}}-{{GEARPUMP_VERSION}}-assembly.jar org.apache.gearpump.streaming.examples.wordcount.WordCount
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;{% endraw %}&lt;/p&gt;
&lt;p&gt;and the variables are defined in a yaml file&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;---
GEARPUMP_VERSION: &amp;quot;0.8.2-SNAPSHOT&amp;quot;
SCALA_BINARY_VERSION: &amp;quot;2.11&amp;quot;
SCALA_VERSION: &amp;quot;2.11.8&amp;quot;
---
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Also, I added scripts to copy documentations to a temporary directory and traverse down the directory pre-processing source with mustache.&lt;/p&gt;
&lt;p&gt;Finally, update &lt;code class='language-text'&gt;docs_dir&lt;/code&gt; configuration from the default &lt;code class='language-text'&gt;docs&lt;/code&gt; to &lt;code class='language-text'&gt;tmp&lt;/code&gt; in &lt;code class='language-text'&gt;mkdocs.yml&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;docs_dir: tmp
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;My first thought was to pre-process the HTMLs in &lt;code class='language-text'&gt;site&lt;/code&gt; but they got overwritten when I run &lt;code class='language-text'&gt;mkdocs serve&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;Code Blocks&lt;/h3&gt;
&lt;p&gt;Existing code examples are written in fenced code blocks from &lt;a href=&quot;https://help.github.com/articles/basic-writing-and-formatting-syntax/&quot;&gt;GitHub Flavored Markdown&lt;/a&gt;. MkDocs uses &lt;a href=&quot;https://pythonhosted.org/Markdown/&quot;&gt;Python Markdown&lt;/a&gt; library to translate Markdown files into HTML. Python-Markdown has &lt;code class='language-text'&gt;fenced_code&lt;/code&gt; extensions but there is a warning that&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Fenced Code Blocks are only supported at the document root level. Therefore, they cannot be nested inside lists or blockquotes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A downside of markdown is many implementations do not enforce markdown syntax, for example, &amp;quot;each subsequent paragraph in a list item must be indented by either 4 spaces or one tab&amp;quot;. This is enforced in Python-Markdown.&lt;/p&gt;
&lt;p&gt;For code blocks inside list, we indent by 8 spaces or two tabs and use &lt;code class='language-text'&gt;codehilite&lt;/code&gt; extensions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;1. item1 

		:::scala
		# Code goes here ...

2. item2
3. item3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is a &lt;a href=&quot;https://github.com/waylan/Python-Markdown/issues/471&quot;&gt;bug&lt;/a&gt; when using both &lt;code class='language-text'&gt;fenced_code&lt;/code&gt; and &lt;code class='language-text'&gt;codehilite&lt;/code&gt; extensions. Meanwhile I think it's better to stick to one style so I replaced all fenced code blocks with &lt;code class='language-text'&gt;codehilite&lt;/code&gt; syntax.&lt;/p&gt;
&lt;p&gt;Remember to add the markdown extension in &lt;code class='language-text'&gt;mkdocs.yml&lt;/code&gt;,&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-yml&quot;&gt;markdown_extensions:
    - codehilite:
        use_pygments: False
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I switched off Pygments highlights since it caused a line of code to overflow if it is too long.&lt;/p&gt;
&lt;h3&gt;Unsolved issues&lt;/h3&gt;
&lt;p&gt;With the above roadblocks moved aside, the new documentation is good to go although some minor issues remain unsolved yet.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/mkdocs/mkdocs/issues/834&quot;&gt;Markdown tables expand beyond window with no scrollbar&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;tabs to switch between different language implementations of an example. (e.g. &lt;a href=&quot;http://spark.apache.org/docs/latest/quick-start.html#self-contained-applications&quot;&gt;self-contained-applications&lt;/a&gt; in Spark docs)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Here is a comparison of documentations before and after the change.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/gr4nW03d8caedR9N4y1dYoGTy25YSU1vZHzjFEj0SRzHy0eoQmoelKFwDXEuwibnwf0s39vwOR5QuB_W55rWHXjGeqTWGUuiHbPP05on9fR9ECOliDdBtGYXdOEdstYvhlphLUwZ-uXDsyK1jmBrZcBrkmcKcwFToVlodi6RmZ2iXnXJhflAddknVtseCOFlR42krWfq0WWJZGcwPag9B45vBDEiRoJuWfCvK_DB3hP9WXPiv2HdvDjqq91kqft3BXKz6-fBSIBHp2q6R_zzLv_RZahKmmI70JVUlwQdpi7qDt3VjEnwj--K_JvwdNMarLsAjb-9N3AIihfLUXzSU61_vCKBn21JrnTcEMpVKiiJstxDGxbMs1pZpKUutxUpM3HIq1dMtlKnGtRjkTQObEetsxFz3NdAPBIEe5yDr2gyC1rz_JUqCS3J0KO2_hiZmR4P5ephNEAlYXkciCoRq061EcOuTmnndo-AJwMHy82_GN28Diey8rArUbQu4-9GED6mUfwx8LJ2glIqfOkcnSzItV2UGSMsMthanY7bVZ894e-rMW51QjSTTyir4eEMN0guZRlsyTRdRncS9IEa2d8SnIpOfATOVMOeuDtyK6NcYy88=w1336-h759-no&quot; alt=&quot;before&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/YkGzEayBztA5Ie922eYwutYdjQOUVQhS0gFa-TiBhsbV46sqr9I72yIdIX_qw75vFSP4Ex7798cJLsge1LQaxtU5k6RShuLDECuXnQ7FtAS2zW3-vt3gC2ZgMrNfJqg4Y_JBuQOWA4xidM9ps4XTI7C0780NCs129m2qplKC1LaF0EaOf_f2w10O6G89eArRCksJ2wqSifysdjbqUUQzPtDAr_X3WAdFM7YQFDucYczEK8p-IFSgEiRurMxpT0CWhkGZf7VmNRel9cG_puiMtoDEksTYRUxeZnNXMWXpqMsTcTfVe3DpMvR2ugrhk1Aic8fMT7rbFhFMdcFT4sqb6G4sw2lAIQiDf4kMLuMUtLVDS-LGvu6GWo3PdPOEej-lLQuZ-RbuTt40qJAqbmIUJ88SwR7vnYfGggqeFKAv3HHqSY1nsUoP27O_U_pVQJqFTynhNRctDxuUtrO2EWWjArXT_3NcJ0tim_8NT2S7lwbozlk4KId3caf29zncZXDUN7wMCkz8GblhcID5JTqq8PxENDpkIadTY2-x-IJ97nDnT2cBeDQtsNVxuCz2uUUISlbvFtqKutWoP89dRRpSACvt4ju3kv6NLNbAqY8QxxajoJg_=w1351-h759-no&quot; alt=&quot;after&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The details are at &lt;a href=&quot;https://github.com/apache/incubator-gearpump/pull/88&quot;&gt;https://github.com/apache/incubator-gearpump/pull/88&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This seems a lot of work but I believe it's worth transiting to a doc style that helps users to learn Gearpump more easily.&lt;/p&gt;
</description><pubDate>Sun, 25 Sep 2016 00:00:00 GMT</pubDate></item><item><title>Biweekly Reading 0xA</title><link>https://manuzhang.github.io/posts/2016-09-04-biweekly-10/index.html</link><description>&lt;p&gt;In the &lt;a href=&quot;http://manuzhang.github.io/2016/08/20/monthly-9.html&quot;&gt;last reading&lt;/a&gt;, I missed an important bug fix in Apache Storm 1.0.2, &lt;a href=&quot;https://issues.apache.org/jira/browse/STORM-1728&quot;&gt;STORM-1728: TransactionalTridentKafkaSpout error&lt;/a&gt;, which I ran into myself writing a TridentKafka pipeline. You can only use &lt;code class='language-text'&gt;OpaqueTridentKafkaSpout&lt;/code&gt; in 1.0.1 or write your own. Okay, let's see what's new.&lt;/p&gt;
&lt;h3&gt;Streaming&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Kafka Streams' fault tolerance mechanism resumes processing from where it left off. There are cases, however, users want to reprocess data from scratch for testing or addressing bugs. It's no easy work since you need to clean up committed offsets and states manually. Luckily, Kafka provided an application reset tool. &lt;a href=&quot;http://www.confluent.io/blog/data-reprocessing-with-kafka-streams-resetting-a-streams-application/&quot;&gt;Data Reprocessing with Kafka Streams: Resetting a Streams Application&lt;/a&gt; has a thorough introduction on what's happening behind the scenes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stephan Ewen, CTO of data Artisans and Neha Narkhede, CTO of Confluent jointly posted &lt;a href=&quot;http://data-artisans.com/apache-flink-apache-kafka-streams/&quot;&gt;Apache Flink and Apache Kafka Streams&lt;/a&gt;. Better together ? No, a comparison and guideline for users.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In summary, while there certainly is an overlap between Kafka Streams and Flink, they live in different parts of a company, largely due to differences in their architecture and thus we see them as complementary systems.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I see them make an alliance. Do you ?&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Akka&lt;/h3&gt;
&lt;p&gt;Lightbend blog already has &lt;a href=&quot;http://www.lightbend.com/blog/lightbend-tech-digest-august-2016&quot;&gt;a good summary on Akka updates for August&lt;/a&gt;. I'd like to highlight those interested me most.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Akka team &lt;a href=&quot;http://blog.akka.io/integrations/2016/08/23/intro-alpakka?_ga=1.60942572.1941686431.1458395042&quot;&gt;introduced Alpakka&lt;/a&gt;, Akka Streams Integration project, to build an ecosystem around connectors, similar to Apache Camel.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Being a streaming library, Akka Streams is not able to window data based on time yet. Adam Warski from SoftwareMill wrote about &lt;a href=&quot;https://softwaremill.com/windowing-data-in-akka-streams/&quot;&gt;how to implement windowing data in Akka Streams&lt;/a&gt;. I also like his introduction to windowing.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://highscalability.com/blog/2016/8/15/how-paypal-scaled-to-billions-of-transactions-daily-using-ju.html&quot;&gt;PayPal Scaled To Billions Of Transactions Daily Using Just 8VMs&lt;/a&gt; with Akka and Scala.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are far more interesting stuff so check out the original post.&lt;/p&gt;
&lt;h3&gt;JVM&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;How do you compare Strings in Java ?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It compares strings by the first differing character, falling back to the length difference when they are identical up to the end of the shorter string&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Have you heard of a second implementation ? If not, you may like &lt;a href=&quot;http://jcdav.is/2016/09/01/How-the-JVM-compares-your-strings/&quot;&gt;How the JVM compares your strings using the craziest x86 instruction you've never heard of&lt;/a&gt;. Note from the comments that&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Just because there is an instruction for it doesn't mean it's faster than some other simpler approach&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Others&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Linkedin shared their &lt;a href=&quot;https://engineering.linkedin.com/blog/2016/08/serving-ads-beyond-linkedin-via-real-time-bidding&quot;&gt;real-time bidding pipeline&lt;/a&gt; which backed their sponsored content. Nice introduction to Real-Time Bidding (RTB).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It seems there are for more interesting things than you are able to work on, so &lt;a href=&quot;http://jvns.ca/blog/2016/08/16/how-do-you-work-on-something-important/&quot;&gt;how do you decide&lt;/a&gt; ?&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That's all for the last two weeks. I have weekly, monthly, and now biweekly. What's next ?&lt;/p&gt;
</description><pubDate>Sun, 4 Sep 2016 00:00:00 GMT</pubDate></item><item><title>Monthly Reading 0x9</title><link>https://manuzhang.github.io/posts/2016-08-20-monthly-9/index.html</link><description>&lt;p&gt;It's almost another month. In the early August, I attended &lt;a href=&quot;http://strata.oreilly.com.cn/hadoop-big-data-cn&quot;&gt;Strata + Hadoop World Beijing&lt;/a&gt;, the first ever Strata in China, and shared about &lt;a href=&quot;http://www.slideshare.net/manuzhang/apache-gearpump-nextgen-streaming-engine&quot;&gt;Apache Gearpump next-gen streaming engine&lt;/a&gt;. Following that, Apache Gearpump (incubating) made its first Apache release. Meanwhile, Apache Spark, Apache Storm and Akka also released new versions.&lt;/p&gt;
&lt;h3&gt;Release&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://databricks.com/blog/2016/07/26/introducing-apache-spark-2-0.html&quot;&gt;Databricks has introduced Apache Spark 2.0&lt;/a&gt;. Most eye-catching new features are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;expanded SQL Support (including all 99 TPC-DS query)&lt;/li&gt;
&lt;li&gt;unified DataFrames and Datasets API in Scala/Java&lt;/li&gt;
&lt;li&gt;A single entry point &lt;code class='language-text'&gt;SparkSession&lt;/code&gt; subsuming both &lt;code class='language-text'&gt;SQLContext&lt;/code&gt; and &lt;code class='language-text'&gt;HiveContext&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;whole-stage code generation that makes Spark 10X faster&lt;/li&gt;
&lt;li&gt;Structured Streaming (Learn more about the feature at &lt;a href=&quot;https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html&quot;&gt;Structured Streaming in Apache Spark&lt;/a&gt; and &lt;a href=&quot;https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.html&quot;&gt;Continuous Applications: Evolving Streaming in Apache Spark 2.0&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://storm.apache.org/2016/08/10/storm102-released.html&quot;&gt;Apache Storm 1.0.2 has released&lt;/a&gt; fixing bugs, improving performance, stability and fault tolerance. One thing to note is &lt;a href=&quot;https://issues.apache.org/jira/browse/STORM-1949&quot;&gt;Backpressure can cause spout to stop emitting and stall topology&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/STORM-1956&quot;&gt;is disabled by default&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://akka.io/news/2016/08/19/akka-2.4.9-released.html&quot;&gt;Akka 2.4.9 is available&lt;/a&gt; with improved performance on Akka HTTP and Akka Streams. Akka HTTP performance is on-par or better than Spray.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Streaming&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Yelp has a series covering its real-time streaming data infrastructure.
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://engineeringblog.yelp.com/2016/07/billions-of-messages-a-day-yelps-real-time-data-pipeline.html&quot;&gt;Billions of Messages a Day - Yelp's Real-time Data Pipeline&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://engineeringblog.yelp.com/2016/08/streaming-mysql-tables-in-real-time-to-kafka.html&quot;&gt;Streaming MySQL tables in real-time to Kafka&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://engineeringblog.yelp.com/2016/08/more-than-just-a-schema-store.html&quot;&gt;More Than Just a Schema Store&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://developers.linecorp.com/blog/?p=3960&quot;&gt;LINE is applying Kafka Streams for internal message delivery pipeline&lt;/a&gt;. If you are looking for a thin streaming layer (without execution framework, resource scheduler, etc) that works best with Kafka, Kafka Streams could be the right choice.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blogs.aws.amazon.com/bigdata/post/Tx2D4GLDJXPKHOY/Writing-SQL-on-Streaming-Data-with-Amazon-Kinesis-Analytics-Part-1&quot;&gt;Writing SQL on Streaming Data with Amazon Kinesis Analytics – Part 1&lt;/a&gt; provides an overview of streaming analytics on AWS.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Scala&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://www.lightbend.com/company/news/after-a-quiet-2015-martin-odersky-outlined-significant-plans-for-scala-at-scala-days-new-york&quot;&gt;Martin Odersky outlined significant plans for Scala after a quiet 2015&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Scala Center&lt;/li&gt;
&lt;li&gt;Scala 2.12&lt;/li&gt;
&lt;li&gt;Plans to rethink the Scala libraries&lt;/li&gt;
&lt;li&gt;Development of new target platforms&lt;/li&gt;
&lt;li&gt;DOT and Dotty&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A related news is &lt;a href=&quot;https://www.lightbend.com/company/news/after-a-quiet-2015-martin-odersky-outlined-significant-plans-for-scala-at-scala-days-new-york&quot;&gt;Samsung SAMI team enabled live IoT data pipeling with Akka and reactive architecture&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Speaking of reactive, Akka Streams is an implementation of &lt;a href=&quot;http://www.reactive-streams.org/&quot;&gt;Reactive Streams&lt;/a&gt; where back pressure is the key. &lt;a href=&quot;http://chariotsolutions.com/blog/post/simply-explained-akka-streams-backpressure/&quot;&gt;Simply explained: Akka Streams Backpressure&lt;/a&gt; walks through an example to illustrate what back pressure is.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you are building applications on Akka or with &lt;a href=&quot;https://github.com/typesafehub/config&quot;&gt;Typesafe Config&lt;/a&gt;, &lt;a href=&quot;http://www.janvsmachine.net/2016/07/effective-typesafe-config.html&quot;&gt;Effective Typesafe Config&lt;/a&gt; is a handy guide.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Java&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Oracle Java Architect, &lt;a href=&quot;https://www.infoq.com/presentations/lessons-java-evolution&quot;&gt;Brian Goetz&lt;/a&gt; looks at lessons of Java evolution and where Java platform is headed.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For Java applications to work, dependencies must be present on the classpath. We usually build fat jars with all dependencies using maven-assembly-plugin or maven-shade-plugin. Deploying a fat every time could be slow and inefficient while &lt;a href=&quot;http://product.hubspot.com/blog/the-fault-in-our-jars-why-we-stopped-building-fat-jars&quot;&gt;a better way is to upload dependencies to a distributed file system for once individually&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That's it for last month. Wish I could return to my weekly timeline.&lt;/p&gt;
</description><pubDate>Sat, 20 Aug 2016 00:00:00 GMT</pubDate></item><item><title>Streaming at Hadoop Summit 2016</title><link>https://manuzhang.github.io/posts/2016-07-26-hadoop-summit/index.html</link><description>&lt;p&gt;This blog has been biased towards &lt;em&gt;Streaming&lt;/em&gt; although I meant to write all things &lt;em&gt;Big Data&lt;/em&gt;. This time I will look back at Streaming sessions of Hadoop Summit 2016 is held in San Jose from June 28 to June 30.  I didn't go to the event so all my take is based on  videos/slides of the &lt;em&gt;IoT and Streaming&lt;/em&gt; track. You may checkout Zhang Zhe's &lt;a href=&quot;http://zhe-thoughts.github.io/2016/07/11/Hadoop-Summit/&quot;&gt;Notes from Hadoop Summit 2016&lt;/a&gt; for HDFS news and &lt;a href=&quot;http://hortonworks.com/blog/hadoop-summit-2016-growth-accelerates/&quot;&gt;Hadoop Summit 2016: The Growth Accelerates - Hortonworks&lt;/a&gt; from business's perspectives.&lt;/p&gt;
&lt;h3&gt;IoT Streaming&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/HadoopSummit/connected-vehicle-data-platform&quot;&gt;Connected Vehicle Data Platform&lt;/a&gt; introduced Ford Motor's data platform with interesting data.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A single Vehicle can generate 25GB of Controller Area Network (CAN) in a hour.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/HadoopSummit/building-a-smarter-home-with-apache-nifi-and-spark&quot;&gt;Building a Smarter Home with Apache NiFi and Spark&lt;/a&gt; builds around a mixed environment of edge, data center and cloud where &lt;strong&gt;Apache Nifi&lt;/strong&gt; plays as a connector all over places.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Streaming Engines&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;While &lt;strong&gt;Flink&lt;/strong&gt; author Kostas Tzoumas talked more about use cases at &lt;a href=&quot;http://www.slideshare.net/KostasTzoumas/streaming-in-the-wild-with-apache-flink-63790942&quot;&gt;Streaming in the Wild with Apache Flink&lt;/a&gt;, Stephan Ewen dived deeper into technique details especially state management at &lt;a href=&quot;http://www.slideshare.net/HadoopSummit/the-stream-processor-as-a-database-apache-flink&quot;&gt;The Stream Processor as a Database Apache Flink&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/HadoopSummit/next-gen-big-data-analytics-with-apache-apex&quot;&gt;Next Gen Big Data Analytics with Apache Apex&lt;/a&gt; gave a general introduction to stream processing engine, &lt;strong&gt;Apache Apex&lt;/strong&gt;, from DataTorrent.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Storm&lt;/strong&gt; PMC Chair Taylor Goetz talked about &lt;a href=&quot;http://www.slideshare.net/HadoopSummit/the-future-of-apache-storm-63920895&quot;&gt;The Future of Apache Storm&lt;/a&gt;. Although about &lt;em&gt;future&lt;/em&gt;, I found it the best documentation to learn about current status of Storm (1.0.1) so far. A major feature in the future is &lt;a href=&quot;http://www.slideshare.net/HadoopSummit/resource-aware-scheduling-in-apache-spark&quot;&gt;Resource Aware Scheduling&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Guozhang Wang talked about how &lt;strong&gt;Kafka Streams&lt;/strong&gt; dealt with &lt;em&gt;streaming processing hard parts&lt;/em&gt; in &lt;a href=&quot;http://www.slideshare.net/HadoopSummit/stream-processing-made-simple-with-kafka&quot;&gt;Stream Processing made simple with Kafka&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Apache Calcite&lt;/strong&gt; author Julian Hyde introduced &lt;a href=&quot;http://www.slideshare.net/HadoopSummit/streaming-sql-63920557&quot;&gt;Streaming SQL&lt;/a&gt;, which is being integrated into Storm, Flink and Samza as SQL over streaming solution.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;With various batch and streaming engines, there is one to unify all, &lt;a href=&quot;http://www.slideshare.net/HadoopSummit/apache-beam-a-unified-model-for-batch-and-stream-processing-data&quot;&gt;&lt;strong&gt;Apache Beam&lt;/strong&gt;: A unified model for batch and stream processing data&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Streaming platforms&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Streaming at Symantec has been used to process log and metrics data. They shared about their pipeline (Storm + ELK) and how they handled the influx issue in &lt;a href=&quot;http://www.slideshare.net/HadoopSummit/in-flux-limiting-for-a-multitenant-logging-service&quot;&gt;In Flux Limiting for a multi-tenant logging service&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Another sharing from Symantec which took close look at their &lt;a href=&quot;http://www.slideshare.net/HadoopSummit/end-to-end-processing-of-37-million-telemetry-events-per-second-using-lambda-architecture&quot;&gt;End to End Processing of 3.7 Million Telemetry Events per Second using Lambda Architecture
&lt;/a&gt;. The deck is full of practical contents like tuning parameters and benchmark results on Kafka, Storm(Trident), etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/HadoopSummit/lambdaless-stream-processing-scale-in-linkedin&quot;&gt;Lambda-less Stream Processing @Scale in LinkedIn&lt;/a&gt; went through how hard problems (&lt;em&gt;accurracy&lt;/em&gt; and &lt;em&gt;reprocessing&lt;/em&gt;) in stream processing had been solved without lambda architecture in Linkedin. The solution is based on &lt;strong&gt;Apache Samza&lt;/strong&gt; and influenced by Google's &lt;a href=&quot;http://research.google.com/pubs/pub41378.html&quot;&gt;Millwheel&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;Almost all Apache Streaming Engines made their presence at Hadoop Summit although it is biased towards Apache Nifi for obvious reasons. One can get a great bird view of existing streaming problems and solutions going through the contents.&lt;/p&gt;
</description><pubDate>Tue, 26 Jul 2016 00:00:00 GMT</pubDate></item><item><title>Monthly Reading 0x8</title><link>https://manuzhang.github.io/posts/2016-07-24-monthly-8/index.html</link><description>&lt;p&gt;Okay, the &lt;em&gt;Weekly Reading&lt;/em&gt; has finally embarrassingly turned into &lt;em&gt;Monthly Reading&lt;/em&gt;. I leave the id auto-incremented to &lt;em&gt;0x8&lt;/em&gt;.&lt;/p&gt;
&lt;h3&gt;Streaming&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We introduced Kafka Streams about &lt;a href=&quot;http://manuzhang.github.io/2016/05/29/weekly-6.html&quot;&gt;2 months ago&lt;/a&gt; but haven't talked about &lt;a href=&quot;https://softwaremill.com/kafka-streams-how-does-it-fit-stream-landscape/&quot;&gt;how does it fit the stream processing landscape?&lt;/a&gt;. My two cents is that you get full functionality of stream processing without adding/maintaining another system if Kafka is already used. It makes much sense since the premise holds in many cases. What's different on Kafka Streams ? Michael Noll from &lt;a href=&quot;http://www.confluent.io/&quot;&gt;Confluent&lt;/a&gt; wrote a series about its unique features.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://www.confluent.io/blog/elastic-scaling-in-kafka-streams&quot;&gt;Elastic Scaling in Kafka Streams&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.confluent.io/blog/secure-stream-processing-with-kafka-streams&quot;&gt;Secure Streaming Processing with Kafka Streams&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SQL on streaming has divided into two worlds. One is &lt;a href=&quot;https://databricks.com/blog/2016/05/11/apache-spark-2-0-technical-preview-easier-faster-and-smarter.html&quot;&gt;Spark SQL over Structured Streaming&lt;/a&gt;. The other is &lt;a href=&quot;http://www.slideshare.net/julianhyde/streaming-sql&quot;&gt;StreamingSQL&lt;/a&gt; from &lt;a href=&quot;https://calcite.apache.org/&quot;&gt;Apache Calcite&lt;/a&gt;, which is being integrated by Storm, Flink and Samza for their SQL layers.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Another buzzword is IoT Analytics (with streaming). Storm PMC Taylor Goetz shared about this topic in &lt;a href=&quot;https://speakerdeck.com/ptgoetz/beyond-the-tweeting-toaster-iot-analytics-with-apache-storm-kafka-and-arduino&quot;&gt;Beyond the Tweeting Toaster: IoT Analytics with Apache Storm, Kafka and Arduino&lt;/a&gt;. This is, however, one-way flow of &lt;code class='language-text'&gt;sensor -&amp;gt; connector -&amp;gt; kafka -&amp;gt; storm&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Spotify runs its services on Google Compute Engine and adopts Google Cloud Dataflow for data processing work. They've implemented a &lt;a href=&quot;https://github.com/spotify/scio&quot;&gt;Scala DSL&lt;/a&gt; on top of Dataflow SDK which is now &lt;a href=&quot;https://issues.apache.org/jira/browse/BEAM-302&quot;&gt;being moved into Apache Beam&lt;/a&gt;. Check it out at &lt;a href=&quot;https://www.infoq.com/presentations/spotify-streaming-cloud&quot;&gt;Handling Streaming Data in Spotify Using the Cloud&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Machine Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Edo Liberty, head of Yahoo's Independent Research in New York, &lt;a href=&quot;https://www.infoq.com/presentations/data-mining-machine-learning&quot;&gt;introduced basic concepts in online machine learning&lt;/a&gt; and Yahoo's &lt;a href=&quot;http://datasketches.github.io/&quot;&gt;DataSketches library&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Distributed Systems&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Aysylu Greenberg discussed &lt;a href=&quot;https://www.infoq.com/presentations/distributed-systems-patterns&quot;&gt;patterns in distributed systems&lt;/a&gt; from systems she has worked at Google.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Robust &amp;amp; scaleable pipelines&lt;/li&gt;
&lt;li&gt;Leases for sharing &amp;amp; heartbeat&lt;/li&gt;
&lt;li&gt;Trade off inaccuracy for
resilience &amp;amp; performance&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Apache Spark&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html&quot;&gt;Apache Spark Key Terms Explained&lt;/a&gt; covers all the major concepts in Spark from APIs to components.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Speaking of API, have you ever wondered what the relationship is between Spark's three APIs: RDD, DataFrame and Dataset? Now here is &lt;a href=&quot;https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html&quot;&gt;A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Spark Summit 2016 took place from June 6 to June 8 in San Francisco. In &lt;a href=&quot;http://datascienceassn.org/content/spark-summit-2016-review&quot;&gt;Spark Summit 2016 Review&lt;/a&gt; Michael Malak said&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Spark might have been overhyped during the 1.x days, but with Spark 2.0 it's caught up to the hype generated during the 1.0 days.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Scala &amp;amp; Java&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If you've been puzzled by Scala compiler errors and don't know what to do next, &lt;a href=&quot;https://scala-clippy.org/&quot;&gt;Scala Clippy&lt;/a&gt;, which adds helpful messages, might be the right compiler plugin for you.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://blog.takipi.com/stackoverflow-7-of-the-best-java-answers-that-you-havent-seen/&quot;&gt;Stackoverflow: 7 of the Best Java Answers That You Haven’t Seen&lt;/a&gt; is a summary by Takipi on most interesting Java Q&amp;amp;A on Stackoverflow.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Others&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Stackoverflow has an 34 min outage on July 20. From the official &lt;a href=&quot;http://stackstatus.net/post/147710624694/outage-postmortem-july-20-2016&quot;&gt;Outage Postmortem&lt;/a&gt;,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The direct cause was a malformed post that caused one of our regular expressions to consume high CPU on our web servers.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Nathan Marz questioned the practice of recruiting in &lt;a href=&quot;http://nathanmarz.com/blog/the-limited-value-of-a-computer-science-education.html&quot;&gt;The limited value of a computer science education&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Whether someone can or cannot solve some cute algorithm problem in a high-pressure situation tells you nothing about that person's ability to write solid, clean, well-structured programs in normal working conditions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;He said take-home projects is a better alternative. Yes, I agree and I asked the interviewers to give a brief presentation on one of &lt;a href=&quot;https://github.com/onurakpolat/awesome-bigdata&quot;&gt;awesome-bigdata&lt;/a&gt; lists last time.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description><pubDate>Sun, 24 Jul 2016 00:00:00 GMT</pubDate></item><item><title>Weekly Reading 0x7</title><link>https://manuzhang.github.io/posts/2016-06-19-weekly-7/index.html</link><description>&lt;p&gt;So long since last time that the &amp;quot;Weekly Reading&amp;quot; is almost becoming &amp;quot;Monthly Reading&amp;quot;. It is a (bad) sign that I haven't read much these days. One thing I've been up to is the upcoming &lt;a href=&quot;http://www.meetup.com/Shanghai-Big-Data-Streaming-Meetup/events/231831396/&quot;&gt;Shanghai BigData Streaming 3rd Meetup&lt;/a&gt;. Let's chat on Big Data Streaming and European Cups over a cup of beer. Another is the &lt;a href=&quot;https://github.com/apache/incubator-beam/pull/323&quot;&gt;Gearpump Runner&lt;/a&gt; for &lt;a href=&quot;https://beam.apache.org&quot;&gt;Apache Beam&lt;/a&gt;. The reading list is also of good contents on Streaming and Apache Beam / Google Cloud Dataflow.&lt;/p&gt;
&lt;h3&gt;Streaming&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We firstly introduced &lt;a href=&quot;https://beam.apache.org&quot;&gt;Apache Beam&lt;/a&gt; in &lt;a href=&quot;http://manuzhang.github.io/2016/05/08/weekly-4.html&quot;&gt;week 4&lt;/a&gt; and then looked at &lt;a href=&quot;https://cloud.google.com/blog/big-data/2016/05/why-apache-beam-a-google-perspective&quot;&gt;Google's&lt;/a&gt; and &lt;a href=&quot;http://data-artisans.com/why-apache-beam/&quot;&gt;dataArtisans's&lt;/a&gt; perspectives. &lt;a href=&quot;https://www.oreilly.com/ideas/future-proof-and-scale-proof-your-code&quot;&gt;Future-proof and scale-proof your code&lt;/a&gt; gives another two reasons to use Apache Beam.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Future-proofing&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Future-proofing code means that we’ll be able to run it on new technologies as they come out, without having to re-write the code.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Scale-proofing&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As data grows, “scale-proofing” code means that we can start out with small data, and have an API that grows with us.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I think, with Apache Beam, it's cheap to try out another platform and carry out benchmarking. Plus, there will be no difference between writing codes run on single machine and cloud.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This is how wordcount looks like with Beam API. Each &lt;code class='language-text'&gt;apply&lt;/code&gt; function takes a &lt;code class='language-text'&gt;PTransform&lt;/code&gt; parameter, like &lt;code class='language-text'&gt;ParDo&lt;/code&gt; and &lt;code class='language-text'&gt;Count&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;Pipeline p = Pipeline.create(options);
p.apply(ParDo.named(&amp;quot;ExtractWords&amp;quot;).of(new DoFn&amp;lt;String, String&amp;gt;() {
                    @Override
                    public void processElement(ProcessContext c) {
                      for (String word : c.element().split(&amp;quot;[^a-zA-Z']+&amp;quot;)) {
                        if (!word.isEmpty()) {
                          c.output(word);
                        }
                      }
                    }
                  }))
 .apply(Count.&amp;lt;String&amp;gt;perElement())                 
 .apply(TextIO.Write.to(&amp;quot;gs://YOUR_OUTPUT_BUCKET/AND_OUTPUT_PREFIX&amp;quot;));
p.run();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Beam is inspired by &lt;a href=&quot;http://research.google.com/pubs/pub35650.html&quot;&gt;FlumeJava&lt;/a&gt;   but it has replaced FlumeJava's methods on &lt;code class='language-text'&gt;PCollection&lt;/code&gt; with the ubiquitous &lt;code class='language-text'&gt;PTransform&lt;/code&gt;. &lt;a href=&quot;http://beam.incubator.apache.org/blog/2016/05/27/where-is-my-pcollection-dot-map.html&quot;&gt;Where's my &lt;code class='language-text'&gt;PCollection.map()&lt;/code&gt;?&lt;/a&gt; looks at the history and design decisions behind this.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Besides Apache Beam, Google Cloud Dataflow offers two advanced features&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://cloud.google.com/blog/big-data/2016/03/comparing-cloud-dataflow-autoscaling-to-spark-and-hadoop&quot;&gt;Autoscaling&lt;/a&gt; dynamically adjusts the number of workers to the needs of pipeline. It is especially useful for streaming application where input data rate varies over time.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://cloud.google.com/blog/big-data/2016/05/no-shard-left-behind-dynamic-work-rebalancing-in-google-cloud-dataflow&quot;&gt;Liquid Sharding&lt;/a&gt; addresses the problems of stragglers through asking busy workers to give away unprocessed work to free workers.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/akka/reactive-kafka&quot;&gt;Reactive Kafka&lt;/a&gt; is an &lt;a href=&quot;http://doc.akka.io/docs/akka/2.4.6/scala/stream/index.html&quot;&gt;Akka Streams&lt;/a&gt; connector for &lt;a href=&quot;https://kafka.apache.org&quot;&gt;Apache Kafka&lt;/a&gt;. Check out &lt;a href=&quot;http://www.slideshare.net/jimriecken/reducing-microservice-complexity-with-kafka-and-reactive-streams&quot;&gt;Reducing Microservice Complexity with Kafka and Reactive Streams&lt;/a&gt; for examples.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Scala&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://www.lihaoyi.com/post/MicrooptimizingyourScalacode.html&quot;&gt;Li Haoyi demonstrates how to micro-optimize your Scala code&lt;/a&gt;, which is full of useful techniques.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Beyond&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;How to solve hard programing problems ? Julia Evans comes up with &lt;a href=&quot;http://jvns.ca/blog/2016/05/29/three-ways-to-solve-hard-programming-problems/&quot;&gt;three ways&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Realize that there is awesome existing software that you can repurpose&lt;/li&gt;
&lt;li&gt;Steal an idea&lt;/li&gt;
&lt;li&gt;Come up with a new idea&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://boingboing.net/2016/06/15/intel-x86-processors-ship-with.html&quot;&gt;Intel x86s hide another CPU that can take over your machine (you can't audit it)&lt;/a&gt;. Take it with a grain of salt.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That's all for this week. See you in the meetup.&lt;/p&gt;
</description><pubDate>Sun, 19 Jun 2016 00:00:00 GMT</pubDate></item><item><title>Weekly Reading 0x6</title><link>https://manuzhang.github.io/posts/2016-05-29-weekly-6/index.html</link><description>&lt;p&gt;I should apologize (to my self, at least) for that I skipped the weekly reading last week. I gave a &lt;a href=&quot;http://www.slideshare.net/manuzhang/apache-gearpump-lightweight-runtime-streaming-engine&quot;&gt;sharing on Gearpump&lt;/a&gt; at 5th Nanjing Big Data Tech Meetup on Saturday and I'd been busy preparing materials the week before. The only news that caught my eyes was &lt;a href=&quot;https://www.buzzfeed.com/williamalden/inside-palantir-silicon-valleys-most-secretive-company?utm_term=.onev4ekN6#.ihn1Q23KP&quot;&gt;Inside Palantir, Silicon Valley’s Most Secretive Company&lt;/a&gt; on Palantir's grow-up struggles. I was not convinced since losing customers could happen to any startups. Now please read &lt;a href=&quot;http://simplystatistics.org/2016/05/11/palantir-struggles/&quot;&gt;The Real Lesson for Data Science That is Demonstrated by Palantir's Struggles&lt;/a&gt;. Sold?&lt;/p&gt;
&lt;p&gt;Let's get down to this week's readings.&lt;/p&gt;
&lt;h3&gt;Streaming&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://www.confluent.io/blog/announcing-apache-kafka-0.10-and-confluent-platform-3.0&quot;&gt;Apache Kafka 0.10 and Confluent Platform 3.0 is announced&lt;/a&gt;. Highlights of Kafka 0.10&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kafka Streams available&lt;/li&gt;
&lt;li&gt;Rack awareness so that replicas are guaranteed to span multiple racks or available zones&lt;/li&gt;
&lt;li&gt;Timestamps in messages indicates the time message produced&lt;/li&gt;
&lt;li&gt;Kafka Consumer max records&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;More on Kafka Streams. Unlike other distributed streaming engines (e.g. Storm, Spark Streaming), a Kafka Streams instance (program) is simply a Java process which is run on one ore more threads.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Kafka Streams applications can run on YARN, be deployed on Mesos, run in Docker containers, or just embedded into existing Java applications.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.twitter.com/2016/open-sourcing-twitter-heron&quot;&gt;Twitter just open sourced Heron&lt;/a&gt;, the successor to Storm as real-time stream processing engine at Twitter. It provides backward compatibility with Storm's Topology API. Please go to their &lt;a href=&quot;http://heronstreaming.io/&quot;&gt;website&lt;/a&gt; for more information&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Microsoft published a new paper &lt;a href=&quot;https://blog.acolyer.org/2016/05/24/streamscope-continuous-reliable-distributed-processing-of-big-data-streams/&quot;&gt;StreamScope: Continuous Reliable Distributed Processing of Big Data Streams&lt;/a&gt; in NSDI '16. After a quick glance, they didn't go beyond &lt;em&gt;watermark&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Spark&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://databricks.com/blog/2016/05/19/approximate-algorithms-in-apache-spark-hyperloglog-and-quantiles.html&quot;&gt;Spark has implemented approximate Algorithms. HyperLogLog and Quantiles in 2.0&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Others&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Julia Evans &lt;a href=&quot;http://jvns.ca/blog/2016/05/22/how-do-you-write-blog-posts/&quot;&gt;shared some advice on writing blog posts&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I really like writing short blog posts because I have a short attention span and I find short blog posts easier to digest when other people write them.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Yes, blogging should never burden you or your readers. That's it.&lt;/p&gt;
</description><pubDate>Sun, 29 May 2016 00:00:00 GMT</pubDate></item><item><title>Weekly Reading 0x5</title><link>https://manuzhang.github.io/posts/2016-05-15-weekly-5/index.html</link><description>&lt;p&gt;The first Kafka Summit conference took place on April 26, 2016. &lt;a href=&quot;http://www.confluent.io/blog/log-compaction-kafka-summit-edition-may-2016&quot;&gt;Confluent blogs summarized the highlights&lt;/a&gt;. Here are the talks I like most&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://www.slideshare.net/helenaedelson/leveraging-kafka-for-big-data-in-real-time-bidding-analytics-ml-campaign-management-for-globally-distributed-data-flows&quot;&gt;Leveraging Kafka for Big Data in Real Time Bidding, Analytics, ML &amp;amp; Campaign Management for Globally Distributed Data Flows&lt;/a&gt; by Helena Edelson.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.slideshare.net/julianhyde/streaming-sql-61426712&quot;&gt;Streaming SQL&lt;/a&gt; by Julian Hyde.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.slideshare.net/ConfluentInc/securing-kafka&quot;&gt;Securing Kafka&lt;/a&gt; by Jun Rao.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.slideshare.net/ConfluentInc/introducing-kafka-streams-largescale-stream-processing-with-kafka-neha-narkhede&quot;&gt;Introducing Kafka Streams: Large-scale Stream Processing with Kafka&lt;/a&gt; by Neha Narkhede.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now let's start our reading.&lt;/p&gt;
&lt;h3&gt;Streaming&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Last week, we saw &lt;a href=&quot;https://cloud.google.com/blog/big-data/2016/05/why-apache-beam-a-google-perspective&quot;&gt;Why Apache Beam? A Google Perspective&lt;/a&gt;. This week let's look at &lt;a href=&quot;http://data-artisans.com/why-apache-beam/&quot;&gt;data Artisans perspective on their involvement in Apache Beam&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In &lt;a href=&quot;http://manuzhang.github.io/2016/03/15/spark-summit.html&quot;&gt;Streaming at Spark Summit East 2016&lt;/a&gt;, we talked about the upcoming Structured Streaming API in Spark 2.0.  Now &lt;a href=&quot;https://databricks.com/blog/2016/05/11/spark-2-0-technical-preview-easier-faster-and-smarter.html&quot;&gt;Spark 2.0 Technical Preview&lt;/a&gt; is available. They find a number of problems in adopting a single programming model integrating both batch and streaming data and&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The vision of Structured Streaming is to utilize the Catalyst optimizer to discover when it is possible to transparently turn a static program into an incremental execution that works on dynamic, infinite data (aka a stream).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Scala&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Li Haoyi explores how to make use of the type-safety of Scala to catch mistakes at compile time in &lt;a href=&quot;http://www.lihaoyi.com/post/StrategicScalaStylePracticalTypeSafety.html&quot;&gt;Strategic Scala Style: Practical Type Safety&lt;/a&gt;. His previous posts in the series are highly recommended&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://www.lihaoyi.com/post/StrategicScalaStyleConcisenessNames.html&quot;&gt;Strategic Scala Style: Conciseness &amp;amp; Names
&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.lihaoyi.com/post/StrategicScalaStylePrincipleofLeastPower.html&quot;&gt;Strategic Scala Style: Principle of Least Power
&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Daniel Westheide introduces his observation that&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;People are using Option too often where their business logic clearly indicates they should use their own, custom ADT.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;in &lt;a href=&quot;http://danielwestheide.com/blog/2016/04/26/when-option-is-not-good-enough.html&quot;&gt;When Option Is Not Good Enough
&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Machine Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Google &lt;a href=&quot;http://googleresearch.blogspot.jp/2016/05/announcing-syntaxnet-worlds-most.html&quot;&gt;announced SyntaxNet, open sourcing the world’s most accurate parser&lt;/a&gt;. SyntaxNet is an open-source neural network framework implemented in TensorFlow that provides a foundation for Natural Language Understanding (NLU) systems.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;One more thing&lt;/h3&gt;
&lt;p&gt;I also read about two nice articles on non-volatile memory this week&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://blog.acolyer.org/2016/05/06/nova-a-log-structured-file-system-for-hybrid-volatilenon-volatile-main-memories/&quot;&gt;NOVA: A Log-Structured File System for Hybrid Volatile/Non-Volatile Main Memories&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://queue.acm.org/detail.cfm?id=2874238&quot;&gt;Non-volatile Storage&lt;/a&gt; from ACM Queue.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;but I don't think I understand them well enough. Hence I'd like to defer them to a dedicated post. See you there.&lt;/p&gt;
</description><pubDate>Sun, 15 May 2016 00:00:00 GMT</pubDate></item><item><title>Weekly Reading 0x4</title><link>https://manuzhang.github.io/posts/2016-05-08-weekly-4/index.html</link><description>&lt;p&gt;I'd like to start this week with something other than code but &lt;a href=&quot;http://www.slideshare.net/LookAtMySlides/codeware&quot;&gt;how to present code&lt;/a&gt;. There are 5 basic rules to follow,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;use monospaced fonts&lt;/li&gt;
&lt;li&gt;use big fonts&lt;/li&gt;
&lt;li&gt;use syntax highlighting only where needed&lt;/li&gt;
&lt;li&gt;use ellipsis&lt;/li&gt;
&lt;li&gt;use screen annotation&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;Remember your slides are not your IDE&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Now let's get down to code.&lt;/p&gt;
&lt;h3&gt;Streaming&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Unlike the batch world where Spark &amp;quot;rules&amp;quot; (Tez, Flink anyone?), the streaming world has entered into a war era. There are so many &lt;a href=&quot;https://github.com/manuzhang/awesome-streaming&quot;&gt;streaming solutions&lt;/a&gt;, each of which has its own pros and cons. Need an apple-to-apple comparison ? &lt;a href=&quot;https://databaseline.wordpress.com/2016/03/12/an-overview-of-apache-streaming-technologies/&quot;&gt;An Overview of Apache Streaming Technologies&lt;/a&gt; is for you.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Although listed in the previous comparison, &lt;a href=&quot;http://beam.incubator.apache.org/&quot;&gt;Apache Beam&lt;/a&gt; is not yet another streaming technology but&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;provide the world with an easy-to-use, but powerful model for data-parallel processing, both streaming and batch, portable across a variety of runtime platforms&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It's formerly &lt;a href=&quot;https://cloud.google.com/dataflow/&quot;&gt;Google Cloud Dataflow SDK&lt;/a&gt; and requires a runner (e.g. Google Cloud Dataflow, Flink, Spark) to work. &lt;a href=&quot;https://cloud.google.com/blog/big-data/2016/05/why-apache-beam-a-google-perspective&quot;&gt;Why Apache Beam? A Google Perspective&lt;/a&gt; explains why (open sourcing) the project makes sense for Google, and from the business perspective too.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;That motivation hinges primarily on the desire to get as many Apache Beam pipelines as possible running on Cloud Dataflow.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Beam has such nice feature as auto-scaling. &lt;a href=&quot;http://www.infoq.com/presentations/google-cloud-dataflow&quot;&gt;Streaming Auto-scaling in Google Cloud Dataflow&lt;/a&gt; has more details.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Apache Kafka has a data structure called purgatory, which &lt;em&gt;holds any request that hasn't yet met its criteria to succeed but also hasn't yet resulted in an error&lt;/em&gt;. &lt;a href=&quot;http://www.confluent.io/blog/apache-kafka-purgatory-hierarchical-timing-wheels&quot;&gt;Apache Kafka, Purgatory, and Hierarchical Timing Wheels&lt;/a&gt; talks about how Kafka efficiently keep track of tens of thousands of requests that are being asynchronously satisfied by other activity in the cluster. &lt;a href=&quot;http://www.cs.columbia.edu/~nahum/w6998/papers/ton97-timing-wheels.pdf&quot;&gt;Hierarchical Timing Wheels&lt;/a&gt; is really a great data structure to know.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://storm.apache.org/2016/05/06/storm101-released.html&quot;&gt;Storm 1.0.1 is released&lt;/a&gt; as a maintenance release that includes a number of important bug fixes that improve Storm's performance, stability and fault tolerance.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Database&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;“Does the Database Community Have an Identity Crisis?’’&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;The answer is yes, according to Peter Bailis. He looks at yesterday and today of database research and offers advice for tomorrow to &lt;a href=&quot;http://www.bailis.org/blog/how-to-make-fossils-productive-again&quot;&gt;make fossils productive again&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Machine Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://googleresearch.blogspot.jp/2016/04/deepmind-moves-to-tensorflow.html&quot;&gt;DeepMind moves to TensorFlow&lt;/a&gt; after &lt;a href=&quot;http://torch.ch/&quot;&gt;Torch7&lt;/a&gt; has served as their primary research platform for nearly four years.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Others&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Julia Evans is bothered at work by that people who knows amazing things often get knowledge stuck in their head and others don't end up learning it. He looks into this in &lt;a href=&quot;http://jvns.ca/blog/2016/04/30/building-expertise-at-work/&quot;&gt;How does knowledge get locked up in people's heads?&lt;/a&gt;. I've been a fan of sharing at work, and as he Julia says, being asked questions will make you an expert.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Think about it. I'll leave you here.&lt;/p&gt;
</description><pubDate>Sun, 8 May 2016 00:00:00 GMT</pubDate></item><item><title>Weekly Reading 0x3</title><link>https://manuzhang.github.io/posts/2016-04-30-weekly-3/index.html</link><description>&lt;p&gt;Some while ago I read from &lt;a href=&quot;http://db.cs.berkeley.edu/jmh/bio.html&quot;&gt;Joe Hellerstein&lt;/a&gt; introducing great work from his two former students, &lt;a href=&quot;https://databeta.wordpress.com/2016/02/01/the-professors-peter-a-and-b/&quot;&gt;The Professors Peter, A and B&lt;/a&gt;. This week we have some good stuff from both Peter, A and B.&lt;/p&gt;
&lt;p&gt;In this &lt;a href=&quot;http://www.infoq.com/interviews/alvaro-distributed-programming&quot;&gt;interview with Peter Alvaro on distributed computing&lt;/a&gt;, he shared about why distributed programming is hard.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;distributed systems are fundamentally hard, as we have always known because of the presence and interaction of two different forms of uncertainty.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The two forms of uncertainty are&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;asynchrony, which is uncertainty about the ordering and the timing at which messages will be delivered to different nodes.&lt;/li&gt;
&lt;li&gt;partial failure, which means that some of your compute components may fail to run, while others keep running and your program nevertheless gives an outcome, which may be incomplete or incorrect.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;On the other side, Peter Bailis &lt;a href=&quot;http://www.bailis.org/blog/you-can-do-research-too/&quot;&gt;encouraged everyone to get started in research&lt;/a&gt; and gave an example of &lt;em&gt;no one is born a researcher&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;One of my closest colleagues started off doing technical support during the first dot-com boom with only an undergraduate degree in literature and no background in Computer Science. Today, my colleague is a tenure-track professor doing work I deeply respect and admire.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Guess who is that colleague ? Peter A !!!&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.bailis.org/blog/&quot;&gt;Peter B's blog&lt;/a&gt; is the best place to know why distributed programming is hard.&lt;/p&gt;
&lt;p&gt;This is the end. I mean, the end of beginning.&lt;/p&gt;
&lt;h3&gt;Streaming&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://www.mapr.com/blog/real-time-event-streaming-what-are-your-options&quot;&gt;Real-Time Event Streaming: What Are Your Options?&lt;/a&gt; interestingly decompose a typical streaming architecture into three major components&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Producers&lt;/strong&gt; publish event data into a streaming system after collecting it from the data source, transforming it into the desired format, and optionally filtering, aggregating, and enriching it. (e.g. Apache Flume, Streamsets Data Collector)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Streaming system&lt;/strong&gt; takes the data published by the producers, persists it, and reliably delivers it to consumers. (e.g. Apache Kafka, MapR Streams)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Consumers&lt;/strong&gt; are typically stream processing engines that subscribe to data from streams and manipulate or analyze that data to look for alerts and insights. (e.g. Spark Streaming, Apache Storm, Apache Flink, Apache Apex)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Machine Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Learned &lt;a href=&quot;http://machinelearningmastery.com/boosting-and-adaboost-for-machine-learning/&quot;&gt;Boosting and AdaBoost for Machine Learning&lt;/a&gt; from Machine Learning Mastery this week.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;JVM / GC&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Julia Evans firstly shared his recent experience in &lt;a href=&quot;http://jvns.ca/blog/2016/04/22/java-garbage-collection-can-be-really-slow/&quot;&gt;Java garbage collection can be really know&lt;/a&gt; and then gave &lt;a href=&quot;http://jvns.ca/blog/2016/04/23/some-links-on-java-garbage-collection/&quot;&gt;some helpful links on Java garbage collection&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Daniel Spiewak &lt;a href=&quot;https://gist.github.com/djspiewak/7a81a395c461fd3a09a6941d4cd040f2&quot;&gt;explains Miles Sabin's magic&lt;/a&gt; in fixing &lt;a href=&quot;https://issues.scala-lang.org/browse/SI-2712&quot;&gt;SI-2172&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;def foo[F[_], A](fa: F[A]): String = fa.toString

type IntConsumer[A] = Int =&amp;gt; A

val f: IntConsumer[Int] = { x: Int =&amp;gt; x * 2 }
foo(f)           // nope!         
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above should compile now in Scala 2.12.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://mishadoff.com/blog/clojure-design-patterns/&quot;&gt;Clojure Design Patterns&lt;/a&gt; from the conversation of two modest programmers Pedro Veel and Eve Dopler, who are solving common software engineering problems and applying design patterns.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Container&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Docker &lt;a href=&quot;https://blog.docker.com/2016/03/docker-networking-design-philosophy/&quot;&gt;explains the guiding principles behind its networking design&lt;/a&gt; which allows for separation for two difference users, application developer and network IT.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Papers&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The morning paper focused on &lt;a href=&quot;http://eurosys16.doc.ic.ac.uk/&quot;&gt;EuroSys 2016&lt;/a&gt; this week, where we get to know the cost and performance implications of non-volatile memory in &lt;a href=&quot;https://blog.acolyer.org/2016/04/28/data-tiering-in-heterogeneous-memory-systems/&quot;&gt;Data Tiering in Heterogeneous Memory Systems&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Syslog also has a &lt;a href=&quot;http://www.syslog.cl.cam.ac.uk/2016/04/23/eurosys-2016/&quot;&gt;highlight&lt;/a&gt; of EuroSys 2016.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;News / History&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://www.acm.org/awards/2015-technical-awards&quot;&gt;ACM RECOGNIZES MAJOR TECHNICAL CONTRIBUTIONS THAT HAVE ADVANCED THE COMPUTING FIELD&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The 2015 winners,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Richard Stallman&lt;/strong&gt;, for the development and leadership of GCC (GNU Compiler Collection), which has enabled extensive software and hardware innovation, and has been a lynchpin of the free software movement&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Brent Waters&lt;/strong&gt;, for the introduction and development of the concepts of attribute-based encryption and functional encryption&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Michael Luby&lt;/strong&gt;, for groundbreaking contributions to erasure correcting codes, which are essential for improving the quality of video transmission over the Internet.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Eric Horvitz&lt;/strong&gt;, for contributions to artificial intelligence and human-computer interaction spanning the computing and decision sciences through developing principles and models of sensing, reflection, and rational action.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now the real end, and end of April.&lt;/p&gt;
</description><pubDate>Sat, 30 Apr 2016 00:00:00 GMT</pubDate></item><item><title>Weekly Reading 0x2</title><link>https://manuzhang.github.io/posts/2016-04-24-weekly-2/index.html</link><description>&lt;blockquote&gt;
&lt;p&gt;“Winter, spring, summer or fall&lt;br /&gt;
All you’ve got to do is call&lt;br /&gt;
And I’ll be there, ye, ye, ye&lt;br /&gt;
You’ve got a friend”&lt;br /&gt;
-James Taylor&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/William_Campbell_(business_executive)&quot;&gt;Bill Campbell&lt;/a&gt; passed away last week. Honestly, I've never heard of this name before but the &amp;quot;Coach&amp;quot; has deeply influenced the Silicon Valley, tutoring Steve Jobs, Jeff Beros, Google founders, Ben Horowitz and countless others. Ben Horowitz &lt;a href=&quot;https://medium.com/@bhorowitz/bill-d9151e6f7538#.fwusr99qy&quot;&gt;expressed his sorrows&lt;/a&gt; starting with the above poem.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Let's start this week.&lt;/p&gt;
&lt;h3&gt;Streaming&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://www.datatorrent.com/blog/end-to-end-exactly-once-with-apache-apex/&quot;&gt;End-to-end “Exactly-Once” with Apache Apex&lt;/a&gt; explains how Apex delivers exactly-once semantics through Kafka connector (with window id to offset tracking), state checkpointing at window boundary and JDBC transaction to make atomic write.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://community.hortonworks.com/articles/14171/windowing-and-state-checkpointing-in-apache-storm.html&quot;&gt;Windowing and State checkpointing in Apache Storm&lt;/a&gt; explores support for windowing operations and stateful processing in Storm 1.0.0. Storm now tracks event time and deals with out-of-order tuples with watermark, and state snapshot with checkpoint stream.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Machine Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;New tutorials from Machine Learning Mastery&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://machinelearningmastery.com/support-vector-machines-for-machine-learning/&quot;&gt;Support Vector Machines for Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://machinelearningmastery.com/bagging-and-random-forest-ensemble-algorithms-for-machine-learning/&quot;&gt;Bagging and Random Forest Ensemble Algorithms for Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://www.inference.vc/deep-learning-is-easy/&quot;&gt;Deep Learning is Easy - Learn Something Harder&lt;/a&gt; reminds deep learning newcomers of the complexity beyond the surface if they are going to take the field seriously.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/&quot;&gt;The amazing power of word vectors&lt;/a&gt; introduces &amp;quot;word2vec&amp;quot; work of Mikolov et al. at Google. Word vector are very good at answering analogy questions like &amp;quot;man is to woman as uncle is to ? (aunt)&amp;quot;. This post is from one of my favorite blogs, &lt;a href=&quot;https://blog.acolyer.org/&quot;&gt;the morning paper&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;an interesting/influential/important paper from the world of CS every weekday morning, as selected by Adrian Colyer&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;JVM&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Chris Newland discusses performance-boosting techniques used by the JVM’s JIT and introduces &lt;a href=&quot;https://github.com/AdoptOpenJDK/jitwatch&quot;&gt;JITWatch&lt;/a&gt;, a tool helping to get the best JVM performance for a code.
&lt;a href=&quot;http://www.infoq.com/presentations/jitwatch&quot;&gt;Understanding HotSpot JVM Performance with JITWatch
&lt;/a&gt;. Yet Another.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Scala&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://speakerdeck.com/heathermiller/academese-to-english-a-practical-tour-of-scalas-type-system&quot;&gt;Academese to English: A Practical Tour of Scala’s Type System&lt;/a&gt; takes an tour of Scala's type system with rich examples. For those from Java world, you will find it make much sense since finally a list of dogs can be a list of animals. This talk doesn't, however, touch the complex stuff like type-level programming, higher-kinded types and path-dependent types.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Industry&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Another nice write-up from &lt;a href=&quot;https://blog.acolyer.org/&quot;&gt;the morning paper&lt;/a&gt;, &lt;a href=&quot;https://blog.acolyer.org/2016/03/04/googles-hybrid-approach-to-research/&quot;&gt;Google’s Hybrid Approach to Research
&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Because of the time-frame and effort involved, Google’s approach to research is iterative and usually involves writing production, or near-production, code from day one&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://highscalability.com/blog/2016/4/20/how-twitter-handles-3000-images-per-second.html&quot;&gt;How Twitter Handles 3,000 Images Per Second&lt;/a&gt; from &lt;a href=&quot;http://highscalability.com/&quot;&gt;High Scalability&lt;/a&gt;. This blog is full of good stuff on distributed systems, highly recommended.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Others&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://www.pulitzer.org/winners/associated-press&quot;&gt;The 2016 Pulitzer Prize Winner in Public Service&lt;/a&gt; is awarded to Associated Press.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;For an investigation of severe labor abuses tied to the supply of seafood to American supermarkets and restaurants, reporting that freed 2,000 slaves, brought perpetrators to justice and inspired reforms.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr /&gt;
&lt;p&gt;I'd like to end this week with Horowitz's&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The worst thing about today is that I can’t call Bill. I miss him so much.&lt;/p&gt;
&lt;/blockquote&gt;
</description><pubDate>Sun, 24 Apr 2016 00:00:00 GMT</pubDate></item><item><title>Weekly Reading 0x1</title><link>https://manuzhang.github.io/posts/2016-04-17-weekly-1/index.html</link><description>&lt;p&gt;I love reading articles and projects on the Internet from all fields of programming, computer science, engineering and technology.
The problem is that so much information easily wears me out. I usually put the posts in Pocket to read later but my Pocket items just keep accumulating (Yes, I'm a procrastinator). The information is not digested before going out so the good contents on the Web are not made good use of. That has troubled me for long and finally I've made my mind to write a weekly summary of what I've read over the previous week, also in the spirit of others, e.g. &lt;a href=&quot;http://www.cakesolutions.net/teamblogs&quot;&gt;Cake Solutions Team Blogs&lt;/a&gt;, &lt;a href=&quot;https://zx31415.wordpress.com/&quot;&gt;Fight with Infinity&lt;/a&gt;, &lt;a href=&quot;https://wdrl.info/&quot;&gt;Web Development Reading List&lt;/a&gt;, &lt;a href=&quot;https://www.hadoopweekly.com/&quot;&gt;Hadoop Weekly&lt;/a&gt;. Note that it's the time when I read those articles not when they are published. As a analogy, it's processing time rather than event time in a &lt;a href=&quot;https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101&quot;&gt;streaming&lt;/a&gt; system and lateness is inherent.&lt;/p&gt;
&lt;p&gt;Ok, here comes the first one.&lt;/p&gt;
&lt;h3&gt;Distributed Systems&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Werner Vogels, CTO of Amazon.com, looked back over the past 10 years of AWS and shared with us &lt;a href=&quot;http://www.allthingsdistributed.com/2016/03/10-lessons-from-10-years-of-aws.html&quot;&gt;10 Lessons from 10 Years of Amazon Web Services&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Streaming&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Apache Flink introduced first version of &lt;a href=&quot;https://en.wikipedia.org/wiki/Complex_event_processing&quot;&gt;Complex Event Processing&lt;/a&gt; library with the example of &lt;a href=&quot;http://flink.apache.org/news/2016/04/06/cep-monitoring.html&quot;&gt;monitoring and alert generation for a data center&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://storm.apache.org/2016/04/12/storm100-released.html&quot;&gt;Storm 1.0.0 released&lt;/a&gt; with a great many features.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;up to 16 times faster with latency reduced up to 60%&lt;/li&gt;
&lt;li&gt;high volume of heartbeat writes to Zookeeper from workers has become a bottleneck for scalability. Now Storm uses &lt;a href=&quot;http://clusterlabs.org/&quot;&gt;Pacemaker&lt;/a&gt;, an in-memory key/value store to process heartbeats from workers&lt;/li&gt;
&lt;li&gt;distributed cache API to allow for sharing of files among topologies with backends of local file system and HDFS&lt;/li&gt;
&lt;li&gt;Nimbus HA leveraging distributed cache API for replication&lt;/li&gt;
&lt;li&gt;native Streaming window API&lt;/li&gt;
&lt;li&gt;stateful Bolt API with automatic checkpointing&lt;/li&gt;
&lt;li&gt;automatic backpressure&lt;/li&gt;
&lt;li&gt;resource aware scheduler&lt;/li&gt;
&lt;li&gt;dynamically change log level for a running topology&lt;/li&gt;
&lt;li&gt;tuple sampling, distributed log search and worker profiling on Storm UI&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kafka Streams is a Java library for building distributed stream processing apps using Apache Kafka. To get a sense of it, &lt;a href=&quot;http://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple&quot;&gt;Introducing Kafka Streams: Stream Processing Made Simple&lt;/a&gt; is a nice read from Jay Kreps (author of &lt;a href=&quot;kafka.apache.org&quot;&gt;Kafka&lt;/a&gt;, &lt;a href=&quot;samza.apache.org&quot;&gt;Samza&lt;/a&gt;, &lt;a href=&quot;confluent.io&quot;&gt;Confluent&lt;/a&gt; co-founder and CEO). I urge you to read his masterpieces if you haven't.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying&quot;&gt;The Log: What every software engineer should know about real-time data's unifying abstraction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.oreilly.com/ideas/questioning-the-lambda-architecture&quot;&gt;Questioning the Lambda Architecture
&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;From the industry, Uber has developed a lambda-like architecture for &lt;a href=&quot;http://www.infoq.com/presentations/real-time-streaming-uber&quot;&gt;Real-time Stream Computing &amp;amp; Analytics&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Machine Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;I've been studying the basics of machine learning from &lt;a href=&quot;http://machinelearningmastery.com/&quot;&gt;Machine Learning Mastery&lt;/a&gt;. For last week,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://machinelearningmastery.com/popular-deep-learning-libraries/&quot;&gt;Popular Deep Learning Libraries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://machinelearningmastery.com/k-nearest-neighbors-for-machine-learning/&quot;&gt;K-Nearest Neighbors for Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://machinelearningmastery.com/naive-bayes-for-machine-learning/&quot;&gt;Naive Bayes for Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Another blog &lt;a href=&quot;https://thebeautyofml.wordpress.com&quot;&gt;The Beauty of Machine Learning&lt;/a&gt; tries to explain machine learning / deep learning in short words.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://thebeautyofml.wordpress.com/2016/04/03/in-a-nutshell-learning/&quot;&gt;In a Nutshell: Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://thebeautyofml.wordpress.com/2016/03/25/in-nutshell-neural-networks/&quot;&gt;In a Nutshell: Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you've often heard of &amp;quot;Backpropagation&amp;quot; but don't know what it is like me, &lt;a href=&quot;http://colah.github.io/posts/2015-08-Backprop/&quot;&gt;Calculus on Computational Graphs: Backpropagation&lt;/a&gt; is a must read.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Have you ever wondered how Google search by image works ? Here is the answer, &lt;a href=&quot;http://www.nanalyze.com/2016/01/deep-learning-and-machine-learning-simply-explained/&quot;&gt;Deep Learning And Machine Learning Simply Explained - Nanalyze&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It's interesting to look at neural networks as functions in functional programming ? &lt;a href=&quot;http://colah.github.io/posts/2015-09-NN-Types-FP/&quot;&gt;Neural Networks, Types, and Functional Programming&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;And, &lt;a href=&quot;http://jvns.ca/blog/2016/04/10/why-i-dont-like-black-boxes/&quot;&gt;Looking inside machine learning black boxes&lt;/a&gt; by Julia Evans, whose blog is of great contents.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Further, explain the predictions of any classifier with &lt;a href=&quot;http://homes.cs.washington.edu/~marcotcr/blog/lime/&quot;&gt;LIME - Local Interpretable Model-Agnostic Explanations&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finally, an interview with DeepMind founder Demis Hassabis, &lt;a href=&quot;http://www.theverge.com/2016/3/10/11192774/demis-hassabis-interview-alphago-google-deepmind-ai&quot;&gt;DeepMind founder Demis Hassabis on how AI will shape the future&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;JVM / GC&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Gearpump recently hit &lt;a href=&quot;https://github.com/gearpump/gearpump/issues/1816&quot;&gt;abnormal gc behavior of Executor&lt;/a&gt;, where a lot of &lt;code class='language-text'&gt;akka.dispatch.AbstractNodeQueue$Node&lt;/code&gt; objects are generated. It turns out to be an &lt;a href=&quot;https://github.com/akka/akka/issues/17547&quot;&gt;Akka issue&lt;/a&gt; and &lt;a href=&quot;http://psy-lob-saw.blogspot.jp/2016/03/gc-nepotism-and-linked-queues.html&quot;&gt;GC 'Nepotism' And Linked Queues
&lt;/a&gt; has given a thorough explanation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Network / Web&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://accu.org/index.php/journals/2180&quot;&gt;Once Again on TCP vs UDP&lt;/a&gt; weighs up their pros and cons but there is always a trade off.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In a sense, replacing TCP with UDP is trading off reliability for interactivity. The most critical factor in selection of one over another one is usually related to acceptable delays.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://queue.acm.org/detail.cfm?id=2673311&quot;&gt;Security Collapse in the HTTPS Market&lt;/a&gt; from ACM Queue assesses legal and technique solutions to secure HTTPS. It's essential to understand the economic incentives of the stakeholders, web-site owners, certificate authorities, web browsers, and end users.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Google Cloud experienced a connectivity issue in all regions last week. Check &lt;a href=&quot;https://status.cloud.google.com/incident/compute/16007?post-mortem&quot;&gt;Google Cloud Status&lt;/a&gt; for detailed description and root cause. TL;DR, it's software bugs.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;History&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Judah Levine originally built servers and programmed software to send time over the Internet for  U.S. National Institute of Standards and Technology (NIST) back in 1993. &lt;a href=&quot;http://spectrum.ieee.org/tech-talk/computing/networks/meet-the-guy-whose-software-keeps-the-nations-clocks-in-sync&quot;&gt;Meet the Guy Whose Software Keeps the World’s Clocks in Sync&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ok, This is the end of first week.&lt;/p&gt;
</description><pubDate>Sun, 17 Apr 2016 00:00:00 GMT</pubDate></item><item><title>Streaming at Spark Summit East 2016</title><link>https://manuzhang.github.io/posts/2016-03-15-spark-summit/index.html</link><description>&lt;p&gt;Spark Summit East 2016 is held in NYC last month.
Databricks already had a &lt;a href=&quot;https://databricks.com/blog/2016/02/18/a-look-back-at-spark-summit-east-2016-thank-you-nyc.html&quot;&gt;look back&lt;/a&gt;, and I'm going to focus on the (Spark) streaming part here.&lt;/p&gt;
&lt;h2&gt;Highlights&lt;/h2&gt;
&lt;p&gt;The most interesting ones are&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;https://spark-summit.org/east-2016/events/spark-streaming-and-iot/&quot;&gt;Spark Streaming and Iot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://spark-summit.org/east-2016/events/online-security-analytics-on-large-scale-video-surveillance-system/&quot;&gt;Online Security Analytics on Large Scale Video Survellance System&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://spark-summit.org/east-2016/events/clickstream-analysis-with-spark-understanding-visitors-in-realtime/&quot;&gt;Clickstream Analysis with Spark—Understanding Visitors in Realtime
&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Spark Streaming and Iot&lt;/h3&gt;
&lt;p&gt;Mike Freedman, CEO and Co-Founder of &lt;a href=&quot;http://www.iobeam.com/&quot;&gt;iobeam&lt;/a&gt;, mainly talked about the challenges in applying Spark to IoT.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://image.slidesharecdn.com/1gmikefreedman-160224030556/95/spark-streaming-and-iot-by-mike-freedman-4-638.jpg?cb=1456283190&quot; alt=&quot;challenges_applying_spark_iot&quot; /&gt;&lt;/p&gt;
&lt;p&gt;I like this talk because these challenges are quite general. It's unclear how iobeam solved them with Spark Streaming which only supports data arrival time. iobeam is a data analysis platform designed for IoT. I really enjoy their websites which put codes side-by-side with use cases.&lt;/p&gt;
&lt;h3&gt;Online Security Analytics on Large Scale Video Survellance System&lt;/h3&gt;
&lt;p&gt;This is from EMC Video Analytics Data Lake, where Spark Streaming is used for online video processing and detection.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://image.slidesharecdn.com/3mhyucao-160224032524/95/online-security-analytics-on-large-scale-video-surveillance-system-by-yu-cao-and-xiaoyan-guo-11-638.jpg?cb=1456284399&quot; alt=&quot;online_video_processing&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Streaming application serves to feed offline model training which is in turn used to realtime detection.&lt;/p&gt;
&lt;h3&gt;Clickstream Analysis with Spark—Understanding Visitors in Realtime&lt;/h3&gt;
&lt;p&gt;The talk is really about the architecture evolution from &amp;quot;Larry &amp;amp; Friends&amp;quot; (Oracle) to &amp;quot;Hadoop &amp;amp; Friends&amp;quot; (HDFS, Hive), from Kappa-Architecture to Lambda-Architecture, and finally Mu-Architecture all based on Spark.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://image.slidesharecdn.com/9scjadersberger-160224034337/95/clickstream-analysis-with-sparkunderstanding-visitors-in-realtime-by-josef-adersberger-19-638.jpg?cb=1456285460&quot; alt=&quot;connection_streaming_batch&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Note that realtime here means 15 mins so a low latency streaming engine like Storm is overengineered. It's, however, a sweet spot for Spark Streaming given the other components in the system are also based on Spark.&lt;/p&gt;
&lt;h2&gt;Core&lt;/h2&gt;
&lt;p&gt;Spark 2.0 will add an infinite Dataframes API for Spark Streaming, unified with the existing Dataframes API for batch processing.  Event-time aggregations will finally arrive in Spark Streaming.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;https://spark-summit.org/east-2016/events/keynote-day-3/&quot;&gt;The Future of Real-Time in Spark&lt;/a&gt; by Reynold Xin, Databricks&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://spark-summit.org/east-2016/events/structuring-spark-dataframes-datasets-and-streaming/&quot;&gt;Structuring Spark Dataframes, Datasets and Streaming&lt;/a&gt; by Michael Amburst, Databricks.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Meanwhile, Back pressure and Elastic Scaling are two important features under development.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;https://spark-summit.org/east-2016/events/building-robust-scalable-and-adaptive-applications-on-spark-streaming/&quot;&gt;Reactive Streams, linking Reactive Application to Spark Streaming&lt;/a&gt; by Luc Bourlier, Lightbend.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://spark-summit.org/east-2016/events/reactive-streams-linking-reactive-application-to-spark-streaming/&quot;&gt;Building Robust, Scalable and Adaptive Applications on Spark Streaming&lt;/a&gt; by Tathagata Das, Databricks.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Connectors&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;https://spark-summit.org/east-2016/events/realtime-risk-management-using-kafka-python-and-spark-streaming/&quot;&gt;Realtime Risk Management Using Kafka, Python, and Spark Streaming&lt;/a&gt; at Shopify.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://spark-summit.org/east-2016/events/building-realtime-data-pipelines-with-kafka-connect-and-spark-streaming/&quot;&gt;Building Realtime Data Pipelines with Kafka Connect and Spark Streaming&lt;/a&gt; by Confluent. This is more about &lt;a href=&quot;http://docs.confluent.io/2.0.0/connect/&quot;&gt;Kafka connnect&lt;/a&gt; than Spark Streaming.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Use cases&lt;/h2&gt;
&lt;p&gt;Other less interesting use cases&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;https://spark-summit.org/east-2016/events/interactive-visualization-of-streaming-data-powered-by-spark/&quot;&gt;Interactive Visualization of Streaming Data Powered by Spark&lt;/a&gt; introduces streaming data visualization at &lt;a href=&quot;http://www.zoomdata.com/&quot;&gt;Zoomdata&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://spark-summit.org/east-2016/events/online-predictive-modeling-of-fraud-schemes-from-mulitple-live-streams/&quot;&gt;Online Predictive Modeling of Fraud Schemes from Mulitple Live Streams&lt;/a&gt; at Atigeo.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://spark-summit.org/east-2016/events/using-spark-to-analyze-activity-and-performance-in-high-speed-trading-environments/&quot;&gt;Using Spark to Analyze Activity and Performance in High Speed Trading Environments&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;https://spark-summit.org/east-2016/schedule/&quot;&gt;https://spark-summit.org/east-2016/schedule/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description><pubDate>Tue, 15 Mar 2016 00:00:00 GMT</pubDate></item><item><title>Unikernel </title><link>https://manuzhang.github.io/posts/2016-01-28-unikernel/index.html</link><description>&lt;p&gt;The first time I learned about Unikernel was from &lt;a href=&quot;https://ma.ttias.be/what-is-a-unikernel/&quot;&gt;What is a ‘unikernel’?&lt;/a&gt; when glancing through posts from Pocket Recommended on my shuttle bus the other day. It's totally out of curiosity and &amp;quot;Recommended&amp;quot; usually means a lot of people are watching it. After a quick read, I didn't get the point. Is it something &lt;a href=&quot;http://roadtounikernels.myriabit.com/&quot;&gt;Docker&lt;/a&gt; ? I forgot about it as soon as I got off the shuttle bus.&lt;/p&gt;
&lt;p&gt;Then came the news that &lt;a href=&quot;http://blog.docker.com/2016/01/unikernel/&quot;&gt;Unikernel Systems Joins Docker&lt;/a&gt;. Interesting ! Meanwhile, I was looking for materials for weekly internal sharing at work. Why didn't I share about Unikernels ? I went on to google and found a bunch of resources from &lt;a href=&quot;https://en.wikipedia.org/wiki/Unikernel&quot;&gt;Wikipedia&lt;/a&gt;, &lt;a href=&quot;https://wiki.xenproject.org/wiki/Unikernels&quot;&gt;Xen Wiki&lt;/a&gt; and &lt;a href=&quot;http://unikernel.org/&quot;&gt;unikernel.org&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Why do we need Unikernel&lt;/h2&gt;
&lt;p&gt;At the age of cloud computing, programmers Tom and Jerry rent two Linux virtual machines (VMs) from &lt;a href=&quot;https://aws.amazon.com/ec2&quot;&gt;Amazon EC2&lt;/a&gt; to host their web servers, and the two VMs end up on the same physical machine. The enabing technique is called virtualization.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://ma.ttias.be/wp-content/uploads/2015/11/road_to_unikernels.png&quot; alt=&quot;road_to_unikernels&quot; /&gt;&lt;/p&gt;
&lt;p&gt;(Source: &lt;a href=&quot;http://roadtounikernels.myriabit.com/&quot;&gt;road to unikernels&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;As in the above image, hardware resources are virtualized by a &lt;a href=&quot;https://en.wikipedia.org/wiki/Hypervisor&quot;&gt;hypervisor&lt;/a&gt; and shared among multiple VMs. One of the most well known hypervisors, &lt;a href=&quot;http://www.xenproject.org/&quot;&gt;Xen&lt;/a&gt; (used by EC2, Rackspace, etc), is developed by the &lt;a href=&quot;https://en.wikipedia.org/wiki/University_of_Cambridge_Computer_Laboratory&quot;&gt;University of Cambridge Computer Laboratory&lt;/a&gt;, &lt;strong&gt;who conceived Unikernels&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Despite this shift from applications running on multi-user operating
systems to provisioning many instances of single-purpose
VMs, there is little actual specialisation that occurs in the image
that is deployed to the cloud. We take an extreme position on specialisation,
treating the final VM image as a single-purpose appliance
rather than a general-purpose system by stripping away functionality
at compile-time&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;(Source: &lt;a href=&quot;http://anil.recoil.org/papers/2013-asplos-mirage.pdf&quot;&gt;Unikernels: Library Operating Systems for the Cloud&lt;/a&gt; )&lt;/p&gt;
&lt;p&gt;If Tom and Jerry only use the VMs to host their web servers, they probably don't need a general-purpose system for a desktop computer. With a single-purpose Unikernels system, Tom and Jerry could link only the system functionality required by their web servers.&lt;/p&gt;
&lt;p&gt;The benefits are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;improved security with reduced deployed codes and attack surface&lt;/li&gt;
&lt;li&gt;small footprints and fast boot time&lt;/li&gt;
&lt;li&gt;whole-system optimisation across device drivers and application logic&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Now, what is a Unikernel&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Unikernels are specialised, single-address-space machine images constructed by using library operating systems.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;(Source: &lt;a href=&quot;https://en.wikipedia.org/wiki/Unikernel&quot;&gt;Unikernel&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Unikernel is built on library operating system (libOS).&lt;/p&gt;
&lt;h3&gt;Library Operating System&lt;/h3&gt;
&lt;p&gt;In a library operating system,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;there is only a single address space&lt;/li&gt;
&lt;li&gt;device drivers are implemented as a set of libraries&lt;/li&gt;
&lt;li&gt;access control and isolation are enforced as policies in the application layer&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Without the separation of user space and kernel space, there is no need for repeated privilege transitions to move data between them. On the other hand, the single address space makes it difficult to isolate mulitple applications running side by side on a libOS.&lt;/p&gt;
&lt;p&gt;The drawback has been overcame by modern hypervisor which provides virtual machines with CPU time and strongly isolated virtual devices.&lt;/p&gt;
&lt;p&gt;Another drawback is protocol libraries (e.g. device drivers) have to been rewritten to replace that of a tradtional operating system.&lt;/p&gt;
&lt;p&gt;Fortunately, they have been implemented in &lt;a href=&quot;https://mirage.io/&quot;&gt;MirageOS&lt;/a&gt;, the first prototype of a Unkernel.&lt;/p&gt;
&lt;h3&gt;MirageOS&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/b/b3/Unikernel_mirage_example.png&quot; alt=&quot;unikernel architecture&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The traditional software stacks are reduced to application code and mirage runtime. OS functionalities are imported as libraries provided by MirageOS. Every application is compiled into its own specialised OS.&lt;/p&gt;
&lt;p&gt;Users could target their codebase towards different platforms, Unix for test and Xen on x86 or Xen on ARM for deployement, which are run on the cloud or embedded systems.&lt;/p&gt;
&lt;p&gt;Besides MirageOS, there is an &lt;a href=&quot;http://unikernel.org/projects/&quot;&gt;expanding Unikernel ecosystem&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Some systems (like Rumprun) are language-agnostic, and provide a platform for any application code based on the requests it makes of the operating system... Unikernels can run in containers, on hypervisors, and on a wide array of bare-metal hardware.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Is Unikernel really needed&lt;/h2&gt;
&lt;p&gt;My colleagues are uncovinced. On the internal sharing, they don't think it's worthwhile to throw all the existing software stacks in the Linux ecosystem. Meanwhile, the benefits brought by Unikernel are doubtable. For example, tailored Linux systems could be also small. &lt;a href=&quot;https://github.com/gliderlabs/docker-alpine&quot;&gt;docker-alpine&lt;/a&gt; image is only 5 MB.&lt;/p&gt;
&lt;p&gt;At the time of writing, another post popped up from Pocket Recommended, &lt;a href=&quot;https://www.joyent.com/blog/unikernels-are-unfit-for-production&quot;&gt;Unikernels are unfit for production
&lt;/a&gt;. The author questions the main reaons (performance, security, footprint) to adopt a Unikernel system. He thinks the most important reason that Unikernel is unfit for production is that &lt;strong&gt;Unikernels are entirely undebuggable&lt;/strong&gt; (please read the original article for thorough explanations).&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this post, we get to know what Unikernel is and why it exists. It's unclear whether Unikernel will become a new revolution for cloud computing like Docker (I also list the discussions on Hacker News and Reddit in the reference below).&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;http://queue.acm.org/detail.cfm?id=2566628&quot;&gt;Unikernels: Rise of the Virtual Library Operating System&lt;/a&gt; by Anil Madhavapeddy and David J. Scott.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://speakerdeck.com/amirmc/unikernels&quot;&gt;Unikernels at PolyConf 2015&lt;/a&gt; by Amir Chaudhry.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://news.ycombinator.com/item?id=10945219&quot;&gt;Discussions on Hacker News&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.reddit.com/r/programming/comments/4206cv/unikernel_systems_joins_docker/&quot;&gt;Discussions on Reddit&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description><pubDate>Thu, 28 Jan 2016 00:00:00 GMT</pubDate></item><item><title>Storm Basics</title><link>https://manuzhang.github.io/posts/2015-10-30-storm/index.html</link><description>&lt;p&gt;&lt;a href=&quot;https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0CB0QFjAAahUKEwjV-qG-5OnIAhVG6GMKHZ44Bb0&amp;amp;url=http%3A%2F%2Fstorm.apache.org%2F&amp;amp;usg=AFQjCNFLXvS1O0EBpftPpR_uwlmw3yCnSQ&amp;amp;sig2=GHn1tEiPZVAJQObUBXjrow&quot;&gt;Apache Storm&lt;/a&gt; is a distributed stream processing system (streaming system), which provides high performance unbounded data processing. I believe Storm is the first &lt;a href=&quot;http://storm.apache.org/documentation/Powered-By.html&quot;&gt;widely used&lt;/a&gt; streaming system and is to streaming what Hadoop is to batch. Emerging streaming systems make themselves known by comparing with Storm and claim they have better performance than Storm.  I highly recommend you read &lt;a href=&quot;http://nathanmarz.com/blog/history-of-apache-storm-and-lessons-learned.html&quot;&gt;history of apache storm and lessons learned&lt;/a&gt; by its creator Nathan Marz.&lt;/p&gt;
&lt;p&gt;Storm is evolving fast. The current stable release is 0.9.5, and 0.9.6 and 0.10.0 will soon be released. 0.10.0 will include such big features as Nimbus HA and Kerberos support. Other nice features like &lt;a href=&quot;https://issues.apache.org/jira/browse/STORM-886&quot;&gt;Automatic Back Pressure&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/STORM-855&quot;&gt;Tuple Batching&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/STORM-893&quot;&gt;Resource Aware Scheduling&lt;/a&gt; are on the way. I've been following Storm for quite a while, from the &lt;a href=&quot;https://github.com/intel-hadoop/storm-benchmark&quot;&gt;storm-benchmark&lt;/a&gt; project to more recently &lt;a href=&quot;https://github.com/gearpump/gearpump/tree/master/experiments/storm&quot;&gt;storm on gearpump&lt;/a&gt;. Each time I went to the implementation and knew a bit more but there are still many blind spots. Meanwhile, I don't like the structure of the official documentation. Hence, it's a good time to look back and summarize (esp. for myself) the basic building blocks of Storm. By the way, thanks to Storm, I learned Clojure and entered into the secret garden of Lisp.&lt;/p&gt;
&lt;p&gt;This will be a series of &lt;strong&gt;Storm Basics&lt;/strong&gt; articles based on the &lt;code class='language-text'&gt;0.9.x&lt;/code&gt; branch and leave more recent advances for future. I won't cover Trident either since it's a totally add-up layer built on Storm core and too complex for basics.&lt;/p&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;Storm is deeply influenced by &lt;a href=&quot;https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=8&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0CFkQFjAHahUKEwjrtNP0itDIAhVQVIgKHU-SCac&amp;amp;url=http%3A%2F%2Fresearch.google.com%2Farchive%2Fmapreduce-osdi04.pdf&amp;amp;usg=AFQjCNEL7nTxrQ6fiMUtt4AZh6gK5og2IQ&amp;amp;sig2=76hkm1YtxIZYLDdQQPg5_w&quot;&gt;MapReduce&lt;/a&gt; (Nathan is also the author of &lt;a href=&quot;http://cascalog.org/&quot;&gt;Cascalog&lt;/a&gt;, the Clojure API over Hadoop, the open source implementation of MapReduce). and brings the MapReduce pattern to the streaming world. Some background knowledge of MapReduce and Streaming here will help us understand the philosophy of Storm.&lt;/p&gt;
&lt;h3&gt;MapReduce&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;What is MapReduce ?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The problem is how to process large data sets on commondity machines (please refer to the original paper for some background).&lt;/p&gt;
&lt;p&gt;Large data sets don't fit in a single machine so we need a distributed system that is able to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;utilize hardware resources&lt;/li&gt;
&lt;li&gt;schedule computations across machines&lt;/li&gt;
&lt;li&gt;transfer data between computations&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Commondity machines are likely to fail and hence the system should provide fault tolerance such as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;restart computations on the same or other machines&lt;/li&gt;
&lt;li&gt;re-send data to restarted computations&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further, users with little experience for distributed systems and concurrency programing could easily use the system.&lt;/p&gt;
&lt;p&gt;MapReduce is such a system. It abstracts processing logic into two simple functions, &lt;code class='language-text'&gt;map&lt;/code&gt; and &lt;code class='language-text'&gt;reduce&lt;/code&gt;. Users only need to specify the functions locally and the system will automatically parallelize them across machines. It handles all the resource management, computation scheduling, data transfer and fault tolerance for users.&lt;/p&gt;
&lt;p&gt;In summary, MapReduce is a system to utilize distributed system to process large data sets.&lt;/p&gt;
&lt;h3&gt;Streaming&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;What is Streaming ?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In &lt;a href=&quot;http://radar.oreilly.com/2015/08/the-world-beyond-batch-streaming-101.html&quot;&gt;The world beyond batch: Streaming 101&lt;/a&gt;, Tyler Akidau defined &lt;strong&gt;streaming&lt;/strong&gt; as&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;a type of data processing engine that is designed with infinite data sets in mind.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In contrast, batch systems like MapReduce are designed to process finite data sets usually partitioned in batches and pre-loaded into the system.&lt;/p&gt;
&lt;p&gt;He further clarifies that such terms as &amp;quot;Unbounded data&amp;quot;, &amp;quot;Unbounded data processing&amp;quot; and &amp;quot;Low-latency, approximate, and/or speculative results&amp;quot; should not be taken as streaming although they are typical characteristics.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;repeated runs of batch engines have been used to process unbounded data since batch systems were first conceived (and conversely, well-designed streaming systems are more than capable of handling “batch” workloads over bounded data).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&quot;http://spark.apache.org/&quot;&gt;Spark&lt;/a&gt; is a batch stream while Spark Streaming is a streaming system built on Spark. &lt;a href=&quot;https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0CB0QFjAAahUKEwjQuLO0x-nIAhUG_mMKHfE2AsI&amp;amp;url=https%3A%2F%2Fflink.apache.org%2F&amp;amp;usg=AFQjCNF8bleCeH3021R16FSXT1_4FvpLkw&amp;amp;sig2=pUfmgPZJIrnODUZ8O1BcaA&quot;&gt;Flink&lt;/a&gt; supports batch mode with an underlying streaming system.&lt;/p&gt;
&lt;p&gt;That said, a batch system has an upper bound on latency it can achieve while a well designed streaming system provides accurate results. Hence, streaming is a superset of batch.&lt;/p&gt;
&lt;h3&gt;Storm&lt;/h3&gt;
&lt;p&gt;As a streaming system, Storm is designed to process infinite data sets (messages). It processes each message as soon as its arrival, emits result per message, and goes to next message and loops forever. Storm inherits the MapReduce model that users write local functions and the system handles the parallelization, resource management, fault tolerance, etc. It extends the computation logic from &lt;code class='language-text'&gt;map&lt;/code&gt; and &lt;code class='language-text'&gt;reduce&lt;/code&gt; to a directed graph called Topology. Storm provides low latency, high throughput and at-least-once message guarantee. These topics will be covered in the following posts.&lt;/p&gt;
&lt;p&gt;I'd like to conclude this overview with a quick start guide to try out Storm.&lt;/p&gt;
&lt;h2&gt;Quick Start&lt;/h2&gt;
&lt;p&gt;You should have Java, Zookeeper and Storm installed locally.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;# 1. launch zookeeper 
bin/zkServer.sh start            

# 2. start nimbus 
bin/storm nimbus

# 3. start supervisor 
bin/storm supervisor

# 4. submit topology
bin/storm jar examples/storm-starter/storm-starter-topologies-${version}.jar storm.starter.ExclamationTopology exclamation
   
# 5. start web ui 
bin/storm ui
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Open &lt;code class='language-text'&gt;localhost:8080&lt;/code&gt; on your browser and the topology should already be running.&lt;/p&gt;
</description><pubDate>Fri, 30 Oct 2015 00:00:00 GMT</pubDate></item><item><title>Photon : Fault-tolerant and Scalable Joining of Continuous Data Streams</title><link>https://manuzhang.github.io/posts/2015-07-15-photon/index.html</link><description>&lt;p&gt;&lt;a href=&quot;http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/41318.pdf&quot;&gt;Photon: Fault-tolerant and Scalable Joining of Continuous Data Streams&lt;/a&gt; - Ananthanarayanan et al. 2013&lt;/p&gt;
&lt;p&gt;We are familar with how join is performed in a relational databases (RDBMS). A typical inner-join SQL could be&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;select * from t_primary, t_foreign,
where t_primary.foreign_key = t_foreign.primary_key
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A naive implementation of such a query is nested-loops-joins, where outer loop consumes table t_primary row by row and inner loop executed for each outer row searchs for matching rows in table t_foreign.&lt;/p&gt;
&lt;p&gt;Joining two continuous data streams are like joining two tables in RDBMS but with far more challenges.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;In RDBMS tables are pre-loaded while data streams are flowing into the system continuously and endlessly. We cannot wait for all the inputs being loaded.&lt;/li&gt;
&lt;li&gt;Another reason we cannot wait is the latency requirement. Data streams are usually from real-time applications where a query must be anwsered within a few seconds. Otherwise, the anwser would be valueless. Think about the case I asked about the traffic condition when driving home. If the system reported an hour later I would either already get home or be trapped in traffic jam !&lt;/li&gt;
&lt;li&gt;It should also have high throughput to serve a large number of requests.&lt;/li&gt;
&lt;li&gt;It's possible that streams data are delayed or unordered. The arrival order of inputs with the same key from two data streams are uncertain. In the extreme case, it may take hours or days for one input finally has its match. Then it already fails the latency requirement.&lt;/li&gt;
&lt;li&gt;Inputs could be lost or resent, which is ok for the Facebook favorite counts but untolerable for Amazon's purchase system. Amazon doesn't want to lose money and users don't want to be charged twice. Hence, it's critical to ensure join is performed exactly-once.&lt;/li&gt;
&lt;li&gt;When the data volume cannot fit into a single machine we have to scale out and shard data on many commodity machines. For one, high scalability is required as data is ever growing. For another, network partitions and hardware failures become the norm. The system should have good fault-tolerance and strike a balance between availability and consistency.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It's interesting to see how Google approaches these challenges in its joinning system, Photon, which is driving Google's Advertising System. Photon joins data streams such as web search queries and user clicks on advertisements and the joined log derives key business metrics includign billing for advertisers. It's geographically distributed and processes millions of events per minute at peak with an average end-to-end latency for less than 10 seconds.&lt;/p&gt;
&lt;p&gt;Here's how Photon joins a search query with subsequent clicks on ads.
&lt;img src=&quot;https://lh3.googleusercontent.com/QOPFJROSqiVDRosE62TAFjjA_MGDMmkyEr8yD-eJLJ8=w879-h680-no&quot; alt=&quot;photon_join&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Photon formalizes the problem as&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Formally, given two continuously growing log streams such that each event in the primary log stream contains a unique identifier, and each event in the foreign log stream contains the identifier referring to an event in the primary log stream, we want to join each foreign log event with the corresponding primary log event and produce the joined event.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As a system joinning continuous data streams, Photon face the challenges as described above. Additionally, at Google's scale, Photon is required to automatically handle datacenter-level outage with no manual operations and no impact on system availability. Hence, there are at least two copies of Photon pipeline in differenct datacenters each of which continues processing independent of the other.&lt;/p&gt;
&lt;p&gt;While datacenter-level replication ensures availability, it becomes very difficult to guarantee consistency, i.e. one Photon pipeline should be aware whether one input event is already joined by another Photon pipeline. That means an input envent is joined at-most-once. Photon relies on its IdRegistry, a &lt;a href=&quot;http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf&quot;&gt;Paxos&lt;/a&gt; based in-memory key-value store, to coordinate between pipeline workers. Let's see how it works through an illustration of a single Photon pipeline.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/d7ryPlWanPm-34Ok9JCs9xyUVyAowc5TiI7OnbbXkFg=w961-h606-no&quot; alt=&quot;photon_pipeline&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The dispatcher consumes the click events from the logs as they come in, and issues a lookup in the IdRegistry. If the click id already exists in the IdRegistry, the dispatcher assumes that the click has already been joined and skips processing the click.&lt;/li&gt;
&lt;li&gt;If the click id does not exist in the IdRegistry, the dispatcher sends the click to the joiner asynchronously and waits for the response. If the joiner fails to join the click (say, due to a network problem, or because of a missing query event), the dispatcher will keep retrying by sending the click to another joiner instance after some backoff period. This guarantees at-least-once semantics with minimum losses.&lt;/li&gt;
&lt;li&gt;The joiner extracts query id from the click and does a lookup in the EventStore to find the corresponding query.&lt;/li&gt;
&lt;li&gt;If the query is not found, the joiner sends a failure response to the dispatcher so that it can retry. If the query is found, the joiner tries to register the click id into the IdRegistry.&lt;/li&gt;
&lt;li&gt;If the click id already exists in the IdRegistry, the joiner assumes that the click has already been joined. If the joiner is able to register click id into the IdRegistry, the joiner stores information from the query in the click and writes the event to the joined click logs.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The retrying logic ensures an input event is joined at-least-once. Combined with the at-most-once semantics guaranteed by IdRegistry, an input event is joined exactly-once in Photon. The system now looks reaonable and functional but it still needs to be performant, which highly depends on the performance of IdRegistry.&lt;/p&gt;
&lt;p&gt;To be fault-tolerant, IdRegistry is itself replicated in different geographical regions.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Based on typical network statistics, the round-trip- time between different geographical regions (such as east and west coasts of the United States) can be over 100 milliseconds. This would limit the throughput of Paxos to less than 10 transactions per second, which is orders of magnitude fewer than our requirements—we need to process (both read and write) tens of thousands of events (i.e., key commits) per second.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To improve IdRegistry's throughput,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;only meta-data is stored in IdRegistry.&lt;/li&gt;
&lt;li&gt;batches client-side requests into one .&lt;/li&gt;
&lt;li&gt;batches server-side operations into one Paxos transaction.&lt;/li&gt;
&lt;li&gt;dynamically shards IdRegistry such that operations on different shards are performed concurrently.&lt;/li&gt;
&lt;li&gt;deletes old keys&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Events delayed by more than N days are discarded where N is determined by evaluating the trade-off between the costage of storage of the cost of dropping such events.&lt;/p&gt;
&lt;p&gt;This post is inspired by &lt;a href=&quot;http://blog.acolyer.org/&quot;&gt;The Morning Paper&lt;/a&gt;.&lt;/p&gt;
</description><pubDate>Wed, 15 Jul 2015 00:00:00 GMT</pubDate></item></channel></rss>