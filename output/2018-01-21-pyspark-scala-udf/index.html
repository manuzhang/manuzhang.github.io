<html><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="description" content="A layout example that shows off a blog page with a list of posts." /><link rel="stylesheet" href="https://unpkg.com/purecss@2.0.3/build/pure-min.css" /><link rel="stylesheet" href="https://unpkg.com/purecss@2.0.3/build/grids-responsive-min.css" /><link rel="stylesheet" href="../../themes/styles.css" /><link rel="stylesheet" href="../../themes/prism.css" /><script src="../../themes/prism.js"></script></head><body><div id="layout" class="pure-g"><div id="layout" class="pure-g"><div class="sidebar pure-u-1 pure-u-md-1-4"><div class="header"><h1 class="brand-title">My Blog</h1><h2 class="brand-tagline">Static Blog generated in Scala</h2><nav class="nav"><ul class="nav-list"><li class="nav-item"><a class="pure-button" href="https://github.com/manuzhang">GitHub</a></li><li class="nav-item"><a class="pure-button" href="https://twitter.com/manuzhang">Twitter</a></li></ul></nav></div></div></div><div class="content pure-u-1 pure-u-md-3-4"><h1>How to use Java UDF in PySpark</h1><p>It's been half a year since my last post. I've switched to a new job, where I've been working on a machine learning platform. My job is to build a data preprocessing tool around Spark SQL (Python / Scala). I'd like to share the findings and learnings along the way, which can barely be found through Spark documentation or Google. The APIs are based on Spark 2.2.0.</p>
<p>The reason to use Spark SQL or DataFrame API rather than RDD is to make use of its optimized execution engine.</p>
<p>I saw people would fall back to RDD when an operation could not be done in DataFrame. That would result in the extra serialization / deserialization overhead between Python and JVM. Instead, we can plug in a Java UDF to achieve it.</p>
<ol>
<li>Implement <code class='language-text'>org.apache.spark.sql.api.java.UDF[1-22]</code> interface in Java</li>
<li>Register the Java function with <code class='language-text'>spark.udf.sqlContext.registerJavaFunction</code> in Python</li>
<li>Register the <code class='language-text'>DataFrame</code> as temporary table and apply a SQL</li>
</ol>
<pre><code class="language-python">spark.udf.sqlContext.registerJavaFunction(&quot;strlen&quot;, &quot;io.github.manuzhang.JavaStringLength&quot;)

// create dataframe
df =

df.createTemporaryView(&quot;t1&quot;);
df = spark.sql(&quot;select strlen() as from t1&quot;)
</code></pre>
</div><div></div></div></body></html>