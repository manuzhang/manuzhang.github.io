<html><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="description" content="Static Blog generated in Scala" /><title>ManuZhang's Blog</title><link rel="shortcut icon" href="favicon.png" /><link rel="stylesheet" href="https://unpkg.com/purecss@2.0.3/build/pure-min.css" /><link rel="stylesheet" href="https://unpkg.com/purecss@2.0.3/build/grids-responsive-min.css" /><link rel="stylesheet" href="../../themes/styles.css" /><link rel="stylesheet" href="../../themes/prism.css" /><script src="../../themes/prism.js"></script></head><body><div id="layout" class="pure-g"><div class="sidebar pure-u-1 pure-u-md-1-4"><div class="header"><h1 class="brand-title">ManuZhang's Blog</h1><h2 class="brand-tagline">Static Blog generated in Scala</h2><nav class="nav"><ul class="nav-list"><li class="nav-item"><a class="pure-button" href="https://github.com/manuzhang">GitHub</a></li><li class="nav-item"><a class="pure-button" href="https://twitter.com/manuzhang">Twitter</a></li><li class="nav-item"><a class="pure-button" href="rss/index.xml">RSS</a></li></ul></nav></div></div><div class="content pure-u-1 pure-u-md-3-4"><div><div><h1>Upgrading to Spark 2.3.1</h1><p>2018-08-11<a class="home" href="../../">Home</a></p></div><p>Recently we upgraded our Spark cluster from 2.2.0 to 2.3.1. The transition has been smooth except that the Spark driver address has changed from IP to hostname.</p>
<p>One thing I found immediately was that submitted application didn't run and I also failed to open application's Web UI page. I looked into the log and it said the driver couldn't be connected by its hostname, which is expected in our cluster environment. Back in Spark 2.2.0, executors found driver by its IP. So what has changed ? Google led me to <a href="https://issues.apache.org/jira/browse/SPARK-21642">SPARK-21642</a> whose change set driver address to FQDN from IP for SSL cases. This would break in environments where hostname could not be resolved. Luckily, a nice guy provided a solution in the comment</p>
<blockquote>
<p>I ran into the same problem. I simply put <code class='language-text'>export SPARK_LOCAL_HOSTNAME=$(ip route get 1 | awk '{print $NF;exit}')</code>
into my launch script to solve the issue. This forces spark to use your IP as hostname. It should work on any debian based system.</p>
</blockquote>
<p>We set <code class='language-text'>SPARK_LOCAL_HOSTNAME</code> to IP in <code class='language-text'>spark-env.sh</code> and it worked like a charm.</p>
</div><div class="footer pure-u-1"><span>Generated with <a href="https://github.com/manuzhang/sbg">SBG.</a> Written in Scala</span></div></div></div></body></html>