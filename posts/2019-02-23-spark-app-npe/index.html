<html><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="description" content="Static Blog generated in Scala" /><title>ManuZhang's Blog</title><link rel="shortcut icon" href="favicon.png" /><link rel="stylesheet" href="https://unpkg.com/purecss@2.0.3/build/pure-min.css" /><link rel="stylesheet" href="https://unpkg.com/purecss@2.0.3/build/grids-responsive-min.css" /><link rel="stylesheet" href="../../themes/styles.css" /><link rel="stylesheet" href="../../themes/prism.css" /><script src="../../themes/prism.js"></script></head><body><div id="layout" class="pure-g"><div class="sidebar pure-u-1 pure-u-md-1-4"><div class="header"><h1 class="brand-title">ManuZhang's Blog</h1><h2 class="brand-tagline">Static Blog generated in Scala</h2><nav class="nav"><ul class="nav-list"><li class="nav-item"><a class="pure-button" href="https://github.com/manuzhang">GitHub</a></li><li class="nav-item"><a class="pure-button" href="https://twitter.com/manuzhang">Twitter</a></li><li class="nav-item"><a class="pure-button" href="rss/index.xml">RSS</a></li></ul></nav></div></div><div class="content pure-u-1 pure-u-md-3-4"><div><div><h1>NPE from Spark App that extends scala.App</h1><p>2019-02-23<a class="home" href="../../">Home</a></p></div><p>This is the log of a bizarre <code class='language-text'>NullPointerException(NPE)</code> from a Spark application that extends <code class='language-text'>scala.App</code> and a minor bug of Spark hidden for years.</p>
<p>Last week, I was developing the following Spark (2.3.1) application that pushes data from <code class='language-text'>Hive</code> to a database.</p>
<pre><code class="language-scala">object Test extends App {
  val foo = args(0)
  val bar = args(1)
  
  val spark = SparkSession.builder
    .getOrCreate()
    
  spark.sql(&quot;select * from test&quot;)
    .foreachPartition { rowIter: Internal[Row] =&gt;
       val client = new DbClient
       client.setFoo(foo) // throws NullPointerException
       client.setBar(bar)
       client.init
      
       client.put(rowIter.map { row =&gt;
         row.getLong(0) -&gt; row.getLong(1)
       }.toMap)
    }
}
</code></pre>
<p>The local variables <code class='language-text'>foo</code> and <code class='language-text'>bar</code> are used to configure the <code class='language-text'>DbClient</code> on remote executors. I was expecting them to be shipped through <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#understanding-closures-">closures</a> but somehow they weren't.</p>
<p>My colleague Vincent pointed me to <code class='language-text'>ClosureCleaner</code> which <code class='language-text'>logDebug</code> <a href="https://github.com/apache/spark/blob/v2.3.1/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala#L221">all fields, methods and classes</a> in closures. I enabled driver's debug log and the absence of <code class='language-text'>foo</code> and <code class='language-text'>bar</code> assured me that they were not captured by closure.</p>
<p>Then I looked into what happens in <code class='language-text'>foreachPartition</code>. The byte code reminded me that <code class='language-text'>foo</code> and <code class='language-text'>bar</code> were static members of <code class='language-text'>Test$</code> rather than local variables.</p>
<pre><code class="language-bash">  9: getstatic      #26                 // Field Test$.MODULE$:LTest$;
  12: invokevirtual #30                 // Method Test$.foo:()Ljava/lang/String;
  15: invokevirtual #34                 // Method DbClient.setFoo:(Ljava/lang/String;)V
</code></pre>
<p><strong>It suddenly struck me that the initialization of the class (<code class='language-text'>Test$</code>) that extends <code class='language-text'>scala.App</code> is actually delayed to its <code class='language-text'>main()</code> method</strong>. That's why <code class='language-text'>foo</code> and <code class='language-text'>bar</code> were uninitialized as confirmed by <a href="https://scala-lang.org/files/archive/api/2.11.12/#scala.App">scala.App's doc</a>,</p>
<blockquote>
<p>It should be noted that this trait is implemented using the <a href="https://scala-lang.org/files/archive/api/2.11.12/scala/DelayedInit.html">DelayedInit</a> functionality, which means that fields of the object will not have been initialized before the main method has been executed.</p>
</blockquote>
<p>Meanwhile, I googled &quot;Spark closure problems&quot; which led me to an ancient Spark jira <a href="https://issues.apache.org/jira/browse/SPARK-4170">Closure problems when running Scala app that &quot;extends App&quot;</a>. There was a fix that would print a warning when an application extends <code class='language-text'>App</code>.</p>
<pre><code class="language-scala">if (classOf[scala.App].isAssignableFrom(mainClass)) {
  printWarning(&quot;Subclasses of scala.App may not work correctly. Use a main() method instead.&quot;)
}
</code></pre>
<p>Why have I never been warned ? After some digging, I realized the above &quot;if clause&quot; would never be true. This is because Scala compiler generate two Java classes <code class='language-text'>Test</code> and <code class='language-text'>Test$</code> from <code class='language-text'>object Test</code> and the <code class='language-text'>mainClass</code> (<code class='language-text'>Test</code>) I passed in is not that (<code class='language-text'>Test$</code>) extends <code class='language-text'>scala.App</code>. It's recorded in <a href="https://issues.apache.org/jira/browse/SPARK-26977">SPARK-26977</a>.</p>
<p>The solution is not (<strong>never for Spark</strong>) to extend <code class='language-text'>scala.App</code> and put everything in Test's own main method. Then <code class='language-text'>foo</code> and <code class='language-text'>bar</code> are now initialized local variables and shipped remotely through closure.</p>
<h3>Appendix</h3>
<ol>
<li>
<p>How to decompile the anonymous function passed to <code class='language-text'>foreachPartition</code> ?</p>
<pre><code class="language-text">javap -c Test\$\$anonfun\$1 
</code></pre>
</li>
<li>
<p>How to enable driver's debug log ?</p>
<p>Prepare a <code class='language-text'>log4j.properties</code> file with</p>
<pre><code class="language-text">log4j.rootLogger=DEBUG
</code></pre>
<p>and submit a Spark application with</p>
<pre><code class="language-text">--driver-java-options &quot;-Dlog4j.configuration=file:/path/to/log4j.properties&quot;
</code></pre>
</li>
</ol>
</div><div class="footer pure-u-1"><span>Generated with <a href="https://github.com/manuzhang/sbg">SBG.</a> Written in Scala</span></div></div></div></body></html>