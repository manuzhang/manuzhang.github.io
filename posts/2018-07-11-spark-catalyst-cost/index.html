<html><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="description" content="Static Blog generated in Scala" /><title>ManuZhang's Blog</title><link rel="shortcut icon" href="favicon.png" /><link rel="stylesheet" href="https://unpkg.com/purecss@2.0.3/build/pure-min.css" /><link rel="stylesheet" href="https://unpkg.com/purecss@2.0.3/build/grids-responsive-min.css" /><link rel="stylesheet" href="../../themes/styles.css" /><link rel="stylesheet" href="../../themes/prism.css" /><script src="../../themes/prism.js"></script></head><body><div id="layout" class="pure-g"><div class="sidebar pure-u-1 pure-u-md-1-4"><div class="header"><h1 class="brand-title">ManuZhang's Blog</h1><h2 class="brand-tagline">Static Blog generated in Scala</h2><nav class="nav"><ul class="nav-list"><li class="nav-item"><a class="pure-button" href="https://github.com/manuzhang">GitHub</a></li><li class="nav-item"><a class="pure-button" href="https://twitter.com/manuzhang">Twitter</a></li><li class="nav-item"><a class="pure-button" href="rss/index.xml">RSS</a></li></ul></nav></div></div><div class="content pure-u-1 pure-u-md-3-4"><div><div><h1>The hidden cost of Spark withColumn</h1><p>2018-07-11<a class="home" href="../../">Home</a></p></div><p>Recently, we've been working on machine learning pipeline with Spark, where <a href="https://spark.apache.org/sql/">Spark SQL &amp; DataFrame</a> is used for data preprocessing and <a href="https://spark.apache.org/mllib/">MLlib</a> for training. In one use case, the data source is a very wide Hive table of ~1000 columns. The columns are stored in String so we need to cast them to Integer before they can be fed into model training.</p>
<p>This was what I got initially with DataFrame Scala API (2.2.0).</p>
<pre><code class="language-scala">df.columns.foldLeft(df) { case (df, col) =&gt;
  df.withColumn(col, df(col).cast(IntegerType))
}
</code></pre>
<p>Since DataFrame is immutable, I created a new <code class='language-text'>DataFrame</code> for each <code class='language-text'>Column</code> casted using <code class='language-text'>withColumn</code>. I didn't think that was be a big deal since at run time all columns would be casted in one shot thanks to <a href="https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html">Spark's Catalyst optimizer</a>.</p>
<blockquote>
<p>At its core, Catalyst contains a general library for representing trees and applying rules to manipulate them. On top of this framework, we have built libraries specific to relational query processing (e.g., expressions, logical query plans), and several sets of rules that handle different phases of query execution: analysis, logical optimization, physical planning, and code generation to compile parts of queries to Java bytecode</p>
</blockquote>
<p>To my surprise, the job stuck in submission for minutes without outputting anything. Luckily, I have a nice colleague Vincent who saved my day with the following fix.</p>
<pre><code class="language-scala">df.select(df.columns.map { col =&gt;
  df(col).cast(IntegerType)
}: _*)
</code></pre>
<p>He suspected that it's expensive to call <code class='language-text'>withColumn</code> for a thousand times. Hence, he dived into its implementation and found out the above in the private method <code class='language-text'>withColumns</code> called by <code class='language-text'>withColumn</code>. In his fast version, only one new <code class='language-text'>DataFrame</code> was created.</p>
<p>I wondered why there was a significant cost difference and looked further into it. After turning on the debug log, I saw a lot of <code class='language-text'>=== Result of Batch Resolution ===</code>s in my slow version. It suddenly struck me that Catalyst's analysis might not be free. A thousand <code class='language-text'>withColumn</code>s were actually a thousand times of analysis, which held true for all APIs on <code class='language-text'>DataFrame</code>. On the other hand, analysis of transform on <code class='language-text'>Column</code> was actually lazy.</p>
<p>The log led me to <code class='language-text'>org.apache.spark.sql.catalyst.rules.RuleExecutor</code> where I  spotted a <code class='language-text'>timeMap</code> tracking time running specific rules. What's more exciting, the statistics was exposed through <code class='language-text'>RuleExecutor.dumpTimeSpent</code> which I could add to compare the costs in two versions.</p>
<pre><code class="language-scala">df.columns.foldLeft(df) { case (df, col) =&gt;
  println(RuleExecutor.dumpTimeSpent())
  df.withColumn(col, df(col).cast(IntegerType))
}
println(RuleExecutor.dumpTimeSpent())

df.select(df.columns.map { col =&gt;
  println(RuleExecutor.dumpTimeSpent())
  df(col).cast(IntegerType)
}: _*)
println(RuleExecutor.dumpTimeSpent())

</code></pre>
<p>As expected, the time spent increased for each <code class='language-text'>DataFrame#withColumn</code> while that stayed the same for <code class='language-text'>Column#cast</code>. It would take about 100ms for one round of analysis. (I have a table of time spent for all 56 rules in 100 <code class='language-text'>withColumn</code> calls in appendix if you are curious).</p>
<h3>Summary</h3>
<ol>
<li>The hidden cost of <code class='language-text'>withColumn</code> is Spark Catalyst's analysis time.</li>
<li>The time spent in Catalyst analysis is usually negligible but it will become an issue when there is a large number of transforms on <code class='language-text'>DataFrame</code>. It's not unusual for a model to have 1000 features which may require preprocessing.</li>
<li>Don't create a new <code class='language-text'>DataFrame</code> for each transform on <code class='language-text'>Column</code>. Create one at last with <code class='language-text'>DataFrame#select</code></li>
</ol>
<h3>Appendix</h3>
<table>
<thead>
<tr><th>Rule</th><th>Nano Time</th></tr>
</thead>
<tbody>
<tr><td>org.apache.spark.sql.catalyst.analysis.Analyzer$FixNullability</td><td>489262230</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveGroupingAnalytics</td><td>243030776</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.TypeCoercion$PropagateTypes</td><td>143141555</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer</td><td>97690381</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.ResolveCreateNamedStruct</td><td>87845664</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveWindowFrame</td><td>85098172</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveWindowOrder</td><td>83967566</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.Analyzer$ExtractWindowExpressions</td><td>63928074</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences</td><td>56549170</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions</td><td>52411767</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.Analyzer$ExtractGenerator</td><td>24759815</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.ResolveTimeZone</td><td>24078761</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.Analyzer$ResolvePivot</td><td>23264984</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.ResolveInlineTables</td><td>22864548</td></tr>
<tr><td>org.apache.spark.sql.execution.datasources.FindDataSourceTable</td><td>22127481</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.DecimalPrecision</td><td>20855512</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.TypeCoercion$ImplicitTypeCasts</td><td>19908820</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.TimeWindowing</td><td>17289560</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.TypeCoercion$DateTimeOperations</td><td>16691649</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.TypeCoercion$FunctionArgumentConversion</td><td>16645812</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.ResolveHints$ResolveBroadcastHints</td><td>16391773</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions</td><td>16094905</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.TypeCoercion$InConversion</td><td>15937875</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.TypeCoercion$PromoteStrings</td><td>15659420</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.TypeCoercion$IfCoercion</td><td>15131194</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.TypeCoercion$BooleanEquality</td><td>15120505</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.TypeCoercion$Division</td><td>14657587</td></tr>
<tr><td>org.apache.spark.sql.execution.datasources.PreprocessTableCreation</td><td>12421808</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery</td><td>12330915</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.UpdateOuterReferences</td><td>11919954</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.TypeCoercion$CaseWhenCoercion</td><td>11807169</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.EliminateUnions</td><td>11761260</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.SubstituteUnresolvedOrdinals</td><td>11683297</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.ResolveHints$RemoveAllHints</td><td>11363987</td></tr>
<tr><td>org.apache.spark.sql.execution.datasources.DataSourceAnalysis</td><td>11253060</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.Analyzer$HandleNullInputsForUDF</td><td>11075682</td></tr>
<tr><td>org.apache.spark.sql.execution.datasources.PreprocessTableInsertion</td><td>11061610</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.Analyzer$GlobalAggregates</td><td>10708386</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.CleanupAliases</td><td>9447785</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAliases</td><td>4725210</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNewInstance</td><td>3634067</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.Analyzer$CTESubstitution</td><td>2359406</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveOrdinalInOrderByAndGroupBy</td><td>2191643</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.ResolveTableValuedFunctions</td><td>2160003</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations</td><td>2095181</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions</td><td>2029468</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.TypeCoercion$WidenSetOperationTypes</td><td>1999994</td></tr>
<tr><td>org.apache.spark.sql.execution.datasources.ResolveSQLOnFile</td><td>1891759</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveMissingReferences</td><td>1864083</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNaturalAndUsingJoin</td><td>1856631</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggAliasInGroupBy</td><td>1740242</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveGenerate</td><td>1714332</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveUpCast</td><td>1686660</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.Analyzer$PullOutNondeterministic</td><td>1602061</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.Analyzer$WindowsSubstitution</td><td>1406648</td></tr>
<tr><td>org.apache.spark.sql.catalyst.analysis.AliasViewChild</td><td>1184166</td></tr>
</tbody>
</table>
</div><div class="footer pure-u-1"><span>Generated with <a href="https://github.com/manuzhang/sbg">SBG.</a> Written in Scala</span></div></div></div></body></html>