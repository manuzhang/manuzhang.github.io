<html><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="description" content="Static Blog generated in Scala" /><title>ManuZhang's Blog</title><link rel="shortcut icon" href="favicon.png" /><link rel="stylesheet" href="https://unpkg.com/purecss@2.0.3/build/pure-min.css" /><link rel="stylesheet" href="https://unpkg.com/purecss@2.0.3/build/grids-responsive-min.css" /><link rel="stylesheet" href="../../themes/styles.css" /><link rel="stylesheet" href="../../themes/prism.css" /><script src="../../themes/prism.js"></script></head><body><div id="layout" class="pure-g"><div class="sidebar pure-u-1 pure-u-md-1-4"><div class="header"><h1 class="brand-title">ManuZhang's Blog</h1><h2 class="brand-tagline">Static Blog generated in Scala</h2><nav class="nav"><ul class="nav-list"><li class="nav-item"><a class="pure-button" href="https://github.com/manuzhang">GitHub</a></li><li class="nav-item"><a class="pure-button" href="https://twitter.com/manuzhang">Twitter</a></li><li class="nav-item"><a class="pure-button" href="rss/index.xml">RSS</a></li></ul></nav></div></div><div class="content pure-u-1 pure-u-md-3-4"><div><div><h1>Regression in taking LIMIT rows from a bucket table in Spark 3.1.1</h1><p>2022-03-15<a class="home" href="../../">Home</a></p></div><p>It's been almost three years since my <a href="https://manuzhang.github.io/posts/2019-04-18-spark-python-udf/index.html">last post on Spark Python UDF</a> and I've dug deeper and learned more about Spark. I've also kept a practice of writing down how I approached to issues at work. However, I usually find they are either too specific to our own environment or have been well documented or I was occupied with some reading. Eventually, there can be no more &quot;execuse&quot; and here we go again!</p>
<h3>Regression</h3>
<p>During migration of Spark applications from 2.3.1 to 3.1.1, a regression was reported for the following query, whose running time increased from 33 seconds to over 3 minutes.</p>
<pre><code class="language-sql">SELECT i, k
FROM t
WHERE j='z'
LIMIT 10;
</code></pre>
<p><code class='language-text'>t</code> is a bucket table, with 10000 buckets, bucket column <code class='language-text'>i</code> and sort column <code class='language-text'>j</code>, and each bucket file is around 3 GB.</p>
<h3>Diagnostics</h3>
<p>Firstly, looking from the application UI, the query was executed quite differently. In 2.3.1, the result was returned early after 1 task reading 487.7 MB data, which is an expected behavior for <code class='language-text'>LIMIT</code> clause. In 3.1.1, however, more than 3 TB data need be read with over 200k tasks from 9 stages. It's almost a full table scan!</p>
<p><img src="https://user-images.githubusercontent.com/1191767/158113221-64676211-a869-4949-97ef-498cd34ceb22.png" alt="2.3.1 stages" style="width:100%" /></p>
<p><img src="https://user-images.githubusercontent.com/1191767/158113505-7aae47b0-13d5-42c7-a8bb-55872a15d5e1.png" alt="3.1.1 stages" style="width:100%" /></p>
<p>Meanwhile, this driver log in 2.3.1 indicted it's a bucketed scan which I didn't find in 3.1.1.</p>
<pre><code class="language-text">INFO FileSourceScanExec: Planning with 10000 buckets
</code></pre>
<p>Okay, I remember that in bucketed scan, when dividing source data into partitions, a bucket file is a partition regardless of its size. Otherwise, the partition size is decided by <code class='language-text'>spark.sql.files.maxPartitionBytes</code> which defaults to 128 MB. Hence, for 10000 bucket files, no more than 10000 tasks are needed to find 10 rows satisifying <code class='language-text'>j='z'</code> with bucketed scan. In contrast, there could be as many as 240,000(<code class='language-text'>3 * 1024 * 10000 / 128</code>) tasks with non-bucketed scan. Nonetheless, in this case Spark looked no further than the first file with bucketed scan in 2.3.1, which corresponded to at most 24 tasks with non-bucketed scan.</p>
<h4>Why were over 200k tasks launched in 3.1.1?</h4>
<blockquote>
<p>the devil is in the details</p>
</blockquote>
<p>After checking the codes, it turns out partitions are <a href="https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala#L609">sorted and read in reverse size order</a> in non-bucketed scan. The tail of the first file, whose size is smaller than 128 MB, is left until the non-tail parts of all files have been scanned. And when the answer happened to lie in the tail of the first file here, all files need be scanned!</p>
<h4>And why was the bucketed scan disabled in 3.1.1?</h4>
<p>The behavior change was brought in by <a href="https://github.com/apache/spark/pull/29804">[SPARK-32859][SQL] Introduce physical rule to decide bucketing dynamically</a> with a new config   <code class='language-text'>spark.sql.sources.bucketing.autoBucketedScan.enabled</code>. The config was later <a href="https://github.com/apache/spark/pull/30138">enabled by default except for cache query</a>. After switching off the config and always enabling bucketed scan in 3.1.1, the query is executed as fast as 2.3.1.</p>
<p>The rational behind the change is to disable bucketed scan when there are no benefits, e.g. no join or group by on bucket column. It looks, however, not all cases have been considered when deciding whether bucketed scan is benefitting.</p>
<h4>How were the tasks scheduled in 3.1.1?</h4>
<p>One more puzzle for me is the special incremental way the tasks were scheduled in 3.1.1. Searching for keyword &quot;limit&quot; in configs has led me to <code class='language-text'>spark.sql.limit.scaleUpFactor</code> whose doc says</p>
<blockquote>
<p>Minimal increase rate in number of partitions between attempts when executing a take on a query. Higher values lead to more partitions read. Lower values might lead to longer execution times as more jobs will be run.</p>
</blockquote>
<p>I tried increasing the config to 300k and the performance was a bit better with 239999 tasks immediately being scheduled in the second stage. I suppose it would much better if I could got more resources to run those tasks in parallel.</p>
<p><img src="https://user-images.githubusercontent.com/1191767/158311306-26e955d5-4eef-45e6-b017-b7dd36c52f18.png" alt="3.1.1 stages with large scaleUpFactor" style="width:100%" /></p>
<h3>Summary</h3>
<p>These configs are worth checking when taking <code class='language-text'>LIMIT</code> rows with filter from a bucket table.</p>
<table class="pure-table pure-table-striped">
<thead>
<tr><th>Name</th><th>Default</th><th>Meaning</th><th>Since Version</th></tr>
</thead>
<tbody>
<tr><td>spark.sql.files.maxPartitionBytes</td><td>128 MB</td><td>The maximum number of bytes to pack into a single partition when reading files</td><td>2.0.0</td></tr>
<tr><td>spark.sql.sources.bucketing.enabled</td><td>true</td><td>When false, we will treat bucketed table as normal table</td><td>2.0.0</td></tr>
<tr><td>spark.sql.sources.bucketing.autoBucketedScan.enabled</td><td>true</td><td>Whe true, decide whether to do bucketed scan on input tables based on query plan automatically</td><td>3.1.1</td></tr>
<tr><td>spark.sql.limit.scaleUpFactor</td><td>4</td><td>Minimal increase rate in number of partitions between attempts when executing a take on a query</td><td>2.1.1</td></tr>
</tbody>
</table>
<p>I also <a href="https://github.com/apache/spark/pull/35514">raised to the community that such behavior change need be put in migration guide but they didn't agree</a>.</p>
<p>An important lesson I've learned is to resolve all the doubts before turning to other suspicious points. In this case, I got distracted by the differences in execution plans and spent a lot of time in comparing how <code class='language-text'>LIMIT</code> clause was compiled into physical plans from 2.3.1 to 3.1.1.</p>
<h3>References</h3>
<ol>
<li><a href="https://spark.apache.org/docs/3.1.1/configuration.html">Spark 3.1.1 Configuration</a></li>
</ol>
</div><div class="footer pure-u-1"><span>Generated with <a href="https://github.com/manuzhang/sbg">SBG.</a> Written in Scala</span></div></div></div></body></html>
